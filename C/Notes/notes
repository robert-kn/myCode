Algorithms have 3 characteristics:
1. effective computability means each step can be carried out by a computer.
2. finiteness means that a procedure terminates.
3. definiteness means that each step is precisely stated.

ISA specifies the interface between a computer program and the computer hardware that will be 
responsible for executing the instructions. Opcode refers to instructions e.g mul and operand
refers to a data value that is operated on.

There are many ISA's in existence. Each specifies a different number of opcodes, operands and 
addressing modes from the rest.

The automobile ISA describes what a driver needs to know as he/she sits inside an automobile 
in order for it to carry out the driver's wishes. All automobiles have the same ISA therefore
drivers do not need to acquire different driving licenses for different automobiles.

The microarchitecture (or implementation) of an automobile's ISA is about what goes on underneath the hood. 
Here all automobiles brands and models can be different depending on the cost/performance tradeoffs the 
designer made before the car was manufactured.

Each microarchitecture is an opportunity for computer designers to make different tradeoffs between the cost 
of the microprocessor, the performance that it will provide, and the energy that it will consume.

The following are the transformations that need to happen in order for computers to be used to 
solve a problem when electron move across a voltage potential:

Problem expressed in natural language -> algorithm -> program -> ISA -> microarchitecture ->
logic circuits -> the devices  (cmos circuits, nmos circuits and gallium arsenide circuits)

At each level of transformation there are choices on how to proceed which determine the resulting cost and 
performance of the computer.

Data type: a particular representation of information is termed as a data type if there are operations inside 
of a computer that can operate on the information that is encoded in that representation.

signed magnitude and 1'complement data types require unnecessarily cumbersome hardware to do addition. on the 
other hand, the circuitry required to add 2 integers using 2's complement data type is much simpler

manual conversion of decimal fraction (e.g. 0.421) to binary requires the following steps:
see decimalFractionToBinary1.png and decimalFractionToBinary2.png

manual conversion of binary fraction to decimal requires the following steps: see binaryFractionToDecimal.png

manual conversion of decimal to 2's complement can be achieved by following these steps see 
decimalTo2scomplement.png

The value of a positive number does not change if we extend the sign bit 0 as many bit positions to the left 
as desired. Similarly, the value of a negative number does not change by extending the sign bit 1 as many bit 
positions to the left as desired. Since in both cases it is the sign bit that is extended, we refer to the 
operation as Sign-EXTension, often abbreviated SEXT. Sign-extension is performed in order to be able to 
operate on representations of diﬀerent lengths. It does not aﬀect the values of the numbers being represented.

overflow in unsigned arithmetic is relatively straightforward because it results in overflow of the msb 
meaning that the result is less than the value of one of the numbers that was added.

overflow in signed negative numbers during arithmetic is easy to test for because the msb overflows and becomes 0  
which means that the number becomes positive. In contrast, when adding two positive signed numbers, overflow occurs 
when the msb is turned on meaning that the result becomes negative.

Suppose we wish to know if two patterns are identical. Since the XOR function pro-
duces a 0 only if the corresponding pair of bits is identical, two patterns are identical
if the output of the XOR is all 0s.

Floating Point Data Type (Greater Range, Less Precision): Most ISAs today specify more than one ﬂoating point 
data type. One of them, usually called ﬂoat, consists of 32 bits, allocated as follows:

1 bit for the sign (positive or negative)
8 bits for the range (the exponent ﬁeld)
23 bits for precision (the fraction ﬁeld)

Normalised Form; the ﬂoating point data type represents numbers expressed in scientiﬁc notation, and mostly in 
normalized form see 32bitFloatingPoint.png: 

N = (−1)^S × 1.fraction × 2^(exponent−127), 1 ≤ exponent ≤ 254

where S, fraction, and exponent are binary numbers.

The computer’s 32-bit ﬂoating point data type consists of (a) a sign bit (positive or negative), (b) 24 binary
digits in normalised form (one non-zero binary digit to the left of the binary point) times (c) the radix 2 
raised to an exponent expressed in eight bits.

The sign bit S is just a single binary digit, 0 for positive numbers, 1 for negative numbers. The 23 fraction 
bits form the 24-bit quantity 1.fraction, where normalized form demands exactly one non-zero binary digit to 
the left of the binary point. Since there exists only one non-zero binary digit (i.e., the value 1), it is 
unnecessary to explicitly store that bit in our 32-bit ﬂoating point format. In fact that is how we get 24 
bits of precision, the 1 to the left of the binary point that is always present in normalised numbers and so 
is unnecessary to store, and the 23 bits of fraction that are actually part of the 32-bit data type.

We say mostly in normalised form because (as noted in the equation) the data type represents a ﬂoating point 
number in normalised form only if the eight-bit exponent is restricted to the 254 unsigned integer values, 
1 (00000001) through 254 (11111110).

As you know, with eight bits, one can represent 256 values uniquely. For the other two integer values 0 
(00000000) and 255 (11111111), the ﬂoating point data type does not represent normalized numbers.

A normalised number is a representation of a real number in scientific notation where:
1. One non-zero digit precedes the decimal point (mantissa).
2. The exponent is adjusted to ensure the mantissa has a specific range

The eight exponent bits are encoded in what we call an excess code, named for the notion that one can get the 
*real* exponent by treating the code as an unsigned integer and subtracting the excess (sometimes called the 
bias). In the case of the IEEE Floating Point that almost everyone uses, that excess (or bias) is 127 for 
32-bit ﬂoating point numbers.

Encoding is the process of converting data into a format required for various information processing needs, 
including transmission, storage, and retrieval. It involves the use of a code to change original data into a 
form that can be used by an external process. 

The exponent ﬁeld gives us numbers as large as 2^(+127) for an 
exponent ﬁeld containing 254 (11111110) and as small as 2^(−126)  for an exponent ﬁeld containing 1 (00000001).

What does the ﬂoating point data type 00111101100000000000000000000000 represent? see floatingPoint1.png 

How is the number −6 5/8  represented in the ﬂoating point data type? 
floatingPoint2.png 


The following three examples provide further illustrations of the interpretation of the 32-bit ﬂoating point 
data type according to the rules of the IEEE standard.

0  10000011  00101000000000000000000 is 1.00101 ⋅ 2^4  = 18.5

The exponent ﬁeld contains the unsigned number 131. Since 131 − 127 is 4, the exponent is +4. Combining a 1 to
the left of the binary point with the fraction ﬁeld to the right of the binary point yields 1.00101. If we 
move the binary point four positions to the right, we get 10010.1, which is 18.5.

1  10000010  00101000000000000000000 is −1 ⋅ 1.00101 ⋅ 2^3  =−9.25

The sign bit is 1, signifying a negative number. The exponent is 130, signifying an exponent of 130 − 127, or 
+3. Combining a 1 to the left of the binary point with the fraction ﬁeld to the right of the binary point 
yields 1.00101. Moving the binary point three positions to the right, we get 1001.01, which is −9.25.

0  11111110  11111111111111111111111

The sign is +. The exponent is 254 − 127, or +127. Combining a 1 to the left of the binary point with the 
fraction ﬁeld to the right of the binary point yields 1.11111111 … 1, which is approximately 2. Therefore, 
the result is approximately 2^128.

We noted before that the ﬂoating point data type represented numbers expressed in scientiﬁc notation in 
normalised form provided the exponent ﬁeld does not contain 00000000 or 11111111. If the exponent ﬁeld 
contains 11111111, we use the ﬂoating point data type to represent various things, among them the notion of 
inﬁnity. Inﬁnity is represented by the exponent ﬁeld containing all 1s and the fraction ﬁeld containing all 0s.
We represent positive inﬁnity if the sign bit is 0 and negative inﬁnity if the sign bit is 1.

Subnormal Numbers

The smallest number that can be represented in normalised form is:

N = 1.00000000000000000000000 × 2^(−126)

What about numbers smaller than 2^(−126) but larger than 0? We call such numbers subnormal numbers because 
they cannot be represented in normalised form. The largest subnormal number is

N = 0.11111111111111111111111 × 2^(−126)

The smallest subnormal number is

N = 0.00000000000000000000001 × 2^(−126) i.e., 2^−23 × 2^−126 which is 2^−149 

Note that the largest subnormal number is 2^−126 minus 2^−149 . Do you see why that is the case? nope

Subnormal numbers are numbers of the form

N = (−1)^s × 0.fraction × 2^−126

We represent them with an exponent ﬁeld of 00000000. The fraction ﬁeld is represented in the same way as with 
normalized numbers. That is, if the exponent ﬁeld contains 00000000, the exponent is −126, and the signiﬁcant 
digits are obtained by starting with a leading 0, followed by a binary point, followed by the 23 bits of the 
fraction ﬁeld.

What number corresponds to the following ﬂoating point representation?

0  00000000  00001000000000000000000

Answer: The leading 0 means the number is positive. The next eight bits, a zero exponent, means the exponent 
is −126, and the bit to the left of the binary point is 0. The last 23 bits form the number 
0.00001000000000000000000, which equals 2^−5 . Thus, the number represented is 2^−5 ⋅ 2^−126 ,which is 2^−131.

Including subnormal numbers allows very, very tiny numbers to be represented.

ASCII Codes: Another representation of information is the standard code that almost all computer equipment 
manufacturers have agreed to use for transferring characters between the main computer processing unit and the
input and output devices. It (ASCII) greatly simpliﬁes the interface between a keyboard manufactured by one 
company, a computer made by another company, and a monitor made by a third company.

Each key on the keyboard is identiﬁed by its unique ASCII code. So, for example, the digit 3 is represented as
00110011, the digit 2 is 00110010, the lowercase e is 01100101, and the ENTER key is 00001101. When you type a
key on the keyboard, the corresponding eight-bit code is stored and made available to the computer.

Most keys are associated with more than one code. For example, the ASCII code for the letter E is 01000101, 
and the ASCII code for the letter e is 01100101. Both are associated with the same key, although in one case 
the Shift key is also depressed while in the other case, it is not.

In order to display a particular character on the monitor, the computer must transfer the ASCII code for that 
character to the electronics associated with the monitor. 

Most processors these days are manufactured out of MOS transistors (metal oxide semiconductors). There are two
types: p type and n type

When the N-type transistor is supplied with 1.2 volts, the connection from source to drain acts like a piece of
wire. We say (in the language of electricity) that we have a short circuit between the source and drain. If the 
gate of the N-type transistor is supplied with 0 volts, the connection between the source and drain is broken. We 
say that between the source and drain we have an open circuit.

The P-type transistor works in exactly the opposite fashion from the N-type transistor. When the gate is supplied 
with 0 volts, the P-type transistor acts (more or less) like a piece of wire, closing the circuit. When the gate 
is supplied with 1.2 volts, the P-type transistor acts like an open circuit. Because the P-type and N-type
transistors act in this complementary way, we refer to circuits that contain both P-type and N-type transistors as 
CMOS circuits, for complementary metal-oxide semiconductor.

One step up from the transistor is the logic gate. That is, we construct basic logic
structures out of individual MOS transistors.

The NOT Gate (Inverter): It is constructed from two MOS transistors, one P-type and one N-type. See Notgate.png 
shows the behavior of the circuit if the input is supplied with 0 volts. Note that the P-type transistor acts
like a short circuit and the N-type transistor acts like an open circuit. The output is, therefore, connected to 
1.2 volts. On the other hand, if the input is supplied with 1.2 volts, the P-type transistor acts like an open 
circuit, but the N-type transistor acts like a short circuit. The output in this case is connected to ground (i.e.,
0 volts). The complete behavior of the circuit can be described by means of a table, as shown in Figure 3.4c. If we 
replace 0 volts with the symbol 0 and 1.2 volts with the symbol 1, we have the truth table see notgate.png

NOR Gates: a NOR gate contains two P-type and two N-type transistors see NORGate.png
Figure 3.5b shows the behavior of the circuit if A is supplied with 0 volts and B is supplied 
with 1.2 volts. In this case, the lower of the two P-type transistors produces an open circuit, 
and the output C is disconnected from the 1.2-volt power supply. However, the leftmost N-type 
transistor acts like a piece of wire, connecting the output C to 0 volts.

Note that if both A and B are supplied with 0 volts, the two P-type transistors
conduct, and the output C is connected to 1.2 volts. Note further that there is no
ambiguity here, since both N-type transistors act as open circuits, and so C is
disconnected from ground.

If either A or B is supplied with 1.2 volts, the corresponding P-type transistor
results in an open circuit. That is suﬃcient to break the connection from C to
the 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type 
transistors is suﬃcient to cause that transistor to conduct, resulting in C being
connected to ground (i.e., 0 volts).

If we replace the voltages with their logical equivalents, we have the truth
table of Figure 3.5d. 

OR Gate: If we augment the circuit of Figure 3.5a by adding an inverter at 
its output, as shown in Figure 3.6a see OR gate, we have at the output D the logical 
function OR. Figure 3.6a is the circuit for an OR gate. Figure 3.6b describes the behavior 
of this circuit if the input variable A is set to 0 and the input variable B is set to 1. 
Figure 3.6c shows the circuit’s truth table.

AND and NAND Gates: see AND.png if either A or B is supplied with 0 volts,
there is a direct connection from C to the 1.2-volt power supply. The fact that C
is at 1.2 volts means the N-type transistor whose gate is connected to C provides
a path from D to ground. Therefore, if either A or B is supplied with 0 volts, the
output D of the circuit of Figure 3.8 is 0 volts.

Again, we note that there is no ambiguity. The fact that at least one of the two
inputs A or B is supplied with 0 volts means that at least one of the two N-type
transistors whose gates are connected to A or B is open, and that consequently, C
is disconnected from ground. Furthermore, the fact that C is at 1.2 volts means
the P-type transistor whose gate is connected to C is open-circuited. Therefore,
D is not connected to 1.2 volts.

On the other hand, if both A and B are supplied with 1.2 volts, then both
of their corresponding P-type transistors are open. However, their corresponding
N-type transistors act like pieces of wire, providing a direct connection from C
to ground. Because C is at ground, the rightmost P-type transistor acts like a
closed circuit, forcing D to 1.2 volts.

Figure 3.8b is a truth table that summarizes the behavior of the circuit of
Figure 3.8a. Note that the circuit is an AND gate. The circuit shown within the
dashed lines (i.e., having output C) is a NOT-AND gate, which we generally
abbreviate as NAND.

The gates just discussed are very common in digital logic circuits and in
digital computers. There are billions of inverters (NOT gates) in Intel’s Skylake
microprocessor. As a convenience, we can represent each of these gates by standard symbols see symbols.png

The bubble shown in the inverter, NAND, and NOR gates signiﬁes the complement (i.e., NOT) 
function.

Now that we understand the workings of the basic logic gates, the next step
is to build some of the logic structures that are important components of the
microarchitecture of a computer.

There are fundamentally two kinds of logic structures, those that include the
storage of information and those that do not.

Here we will deal with structures that do not store information. These structures are sometimes referred to as 
decision elements. Usually, they are referred to as combinational logic structures because their outputs are strictly
dependent on the combination of input values that are being applied to the structure right now. Their outputs are not
at all dependent on any past history of information that is stored internally, since no information can be stored 
internally in a combinational logic circuit.

Decoder: Figure 3.11 shows a logic gate implementation of a two-input decoder. A decoder
has the property that exactly one of its outputs is 1 and all the rest are 0s. The one
output that is logically 1 is the output corresponding to the input pattern that it is
expected to detect. In general, decoders have n inputs and 2^n outputs. We say the
output line that detects the input pattern is asserted. That is, that output line has
the value 1, rather than 0 as is the case for all the other output lines. In Figure 3.11,
note that for each of the four possible combinations of inputs A and B, exactly one
output has the value 1 at any one time. In Figure 3.11b, the input to the decoder
is 10, resulting in the third output line being asserted.

Mux: The function of a mux is to select one of the inputs (A or B) and connect it to the 
output. The select signal (S in Figure 3.12) determines which input is connected to the output.
Figure 3.12a shows a logic gate implementation of a two-input multiplexer, more commonly 
referred to as a mux.

The mux of Figure 3.12 works as follows: Suppose S = 0, as shown in
Figure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out-
put of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is
whatever the input A is. That is, if A = 0, then the output of the leftmost AND gate
is 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output
of the rightmost AND gate is 0, it has no eﬀect on the OR gate. Consequently,
the output at C is exactly the same as the output of the leftmost AND gate. The
net result of all this is that if S = 0, the output C is identical to the input A.
On the other hand, if S = 1, it is B that is ANDed with 1, resulting in the
output of the OR gate having the value of B.

In summary, the output C is always connected to either the input A or the
input B — which one depends on the value of the select line S. We say S selects the
source of the mux (either A or B) to be routed through to the output C. Figure 3.12c
shows the standard representation for a mux.

In general, a mux consists of 2^n inputs and n select lines. Figure 3.13a
shows a gate-level description of a four-input mux. It requires two select lines.
Figure 3.13b shows the standard representation for a four-input mux.
Question: Can you construct the gate-level representation for an eight-input
mux? yes. How many select lines must you have? 3

A One-Bit Adder (a.k.a. a Full Adder):

A simple algorithm for binary addition is to proceed as you have always done in the case of 
decimal addition, from right to left, one column at a time, adding the two digits from the two 
values plus the carry in, and generating a sum digit and a carry to the next column. The
only diﬀerence here (with binary addition) is you get a carry after 1, rather than
after 9.

Figure 3.14 is a truth table that describes the result of binary addition on one
column of bits within two n-bit operands. At each column, there are three values
that must be added: one bit from each of the two operands A and B and the carry
from the previous column. We designate these three bits as Ai , Bi , and Ci . There
are two results, the sum bit (Si) and the carry over to the next column, Ci+1 . Note
that if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e.,
Ci+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If
all three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and
a carry of 1.

Figure 3.15 shows a logic gate implementation of a one-bit adder. Note that each AND gate in Figure 3.15 produces 
an output 1 for exactly one of the eight input combinations of Ai, Bi, and Ci. The output of the OR gate for Ci+1 
must be 1 in exactly those cases where the corresponding input combinations in Figure 3.14 produce an output 1. 
Therefore, the inputs to the OR gate that generates Ci+1 are the outputs of the AND gates corresponding to those 
input combinations. Similarly, the inputs to the OR gate that generates Si are the outputs of the AND gates 
corresponding to the input combinations that require an output 1 for Si in the truth table of Figure 3.14.

Note that since the input combination 000 does not result in an output 1 for either Ci+1 or S, its corresponding 
AND gate is not an input to either of the two OR gates.

Figure 3.16 shows a circuit for adding two 4-bit binary numbers, using four of the one-bit adder circuits of Figure 
3.15. Note that the carry out of column i is an input to the addition performed in column i + 1.

If we wish to implement a logic circuit that adds two 16-bit numbers, we can do so with a circuit of 16 one-bit 
adders.

We should point out that historically the logic circuit of Figure 3.15 that provides three inputs (Ai, Bi, and Ci) 
and two outputs (the sum bit Si and the carry over to the next column Ci+1) has generally been referred to as a 
full adder to diﬀerentiate it from another structure, which is called a half adder. The distinction between the two 
is the carry bit. Note that the carry into the rightmost column in Figure 3.16 is 0. That is, in the rightmost 
circuit, S0 and C1 depend only on two inputs, A0 and B0. Since that circuit depends on only two inputs, it has been
referred to as a half adder. Since the other circuits depend on all three inputs, they are referred to as full 
adders. We prefer the term one-bit adder as a simpler term for describing what is happening in each column.

The Programmable Logic Array (PLA)

Figure 3.17 illustrates a very common building block for implementing any collection of logic functions one wishes 
to implement. The building block is called a programmable logic array (PLA). It consists of an array of AND gates 
(called an AND array) followed by an array of OR gates (called an OR array). The number of AND gates corresponds 
to the number of input combinations (rows) in the truth table. For n-input logic functions, we need a PLA with 2^n
n-input AND gates. In Figure 3.17, we have 2^3 three-input AND gates, corresponding to three logical input variables.
The number of OR gates corresponds to the number of logic functions we wish to implement, that is, the number of 
output columns in the truth table. The implementation algorithm is simply to connect the output of an AND gate to 
the input of an OR gate if the corresponding row of the truth table produces an output 1 for that output column. 
Hence the notion of programmable. That is, we say we program the connections from AND gate outputs to OR gate 
inputs to implement our desired logic functions

Figure 3.15 shows seven AND gates connected to two OR gates since our requirement was to implement two functions 
(sum and carry) of three input variables. Figure 3.17 shows a PLA that can implement any four functions of three 
variables by appropriately connecting AND gate outputs to OR gate inputs. That is, any function of three variables 
can be implemented by connecting the outputs of all AND gates corresponding to input combinations for which the 
output is 1 to inputs of one of the OR gates. Thus, we could implement the one-bit adder by programming the two OR 
gates in Figure 3.17 whose outputs are W and X by connecting or not connecting the outputs of the AND gates to the 
inputs of those two OR gates as speciﬁed by the two output columns of Figure 3.14.

Logical Completeness

We saw that any logic function we wished to implement could be accomplished with a PLA. We saw that the PLA 
consists of only AND gates, OR gates, and inverters. That means that any logic function can be implemented, 
provided that enough AND, OR, and NOT gates are available. We say that the set of gates {AND, OR, NOT} is logically 
complete because we can build a circuit to carry out the speciﬁcation of any truth table we wish without using any 
other kind of gate. That is, the set of gates {AND, OR, and NOT} is logically complete because a barrel of AND 
gates, a barrel of OR gates, and a barrel of NOT gates are suﬃcient to build a logic circuit that carries out the 
speciﬁcation of any desired truth table. The barrels may have to be big, but the point is, we do not need any other 
kind of gate to do the job.

Question: Is there any single two-input logic gate that is logically complete? For example, is the NAND gate 
logically complete? Hint: Can I implement a NOT gate with a NAND gate? If yes, can I then implement an AND gate 
using a NAND gate followed by a NOT gate? If yes, can I implement an OR gate using just AND gates and NOT gates?

If all of the above is true, then the NAND gate is logically complete, and I can implement any desired logic 
function as described by its truth table with a barrel of NAND gates.

Now we are ready to discuss logic structures that do include the storage of information.


The R-S Latch

A simple example of a storage element is the R-S latch. It can store one bit of information, a 0 or a 1. The R-S 
latch can be implemented in many ways, the simplest being the one shown in Figure 3.18. Two 2-input NAND gates are 
connected such that the output of each is connected to one of the inputs of the other. The remaining inputs S and R 
are normally held at a logic level 1. 

The R-S latch gets its name from the old designations for setting the latch to store a 1 and setting the latch to 
store a 0. Setting the latch to store a 1 was referred to as setting the latch, and setting the latch to store a 0 
was referred to as resetting the latch. Ergo, R-S.

The Quiescent State: We describe the quiescent (or quiet) state of a latch as the state when the latch is storing a 
value, either 0 or 1, and nothing is trying to change that value. This is the case when inputs S and R both have the
logic value 1. In Figure 3.18 the letter a designates the value that is currently stored in the latch, which we also
refer to as the output of the latch.

Consider ﬁrst the case where the value stored and therefore the output a is 1. Since that means the value A is 1 
(and since we know the input R is 1 because we are in the quiescent state), the NAND gate’s output b must be 0. 
That, in turn, means B must be 0, which results in the output a equal to 1. As long as the inputs S and R remain 1, 
the state of the circuit will not change. That is, the R-S latch will continue to store the value 1 (the value of 
the output a).

If, on the other hand, we assume the output a is 0, then A must be 0, and the output b must be 1. That, in turn, 
results in B equal to 1, and combined with the input S equal to 1 (again due to quiescence), results in the output 
a equal to 0. Again, as long as the inputs S and R remain 1, the state of the circuit will not change. In this 
case, we say the R-S latch stores the value 0.

Setting the Latch to a 1 or a 0 The latch can be set to 1 by momentarily setting S to 0, provided we keep the value 
of R at 1. Similarly, the latch can be set to 0 by momentarily setting R to 0, provided we keep the value of S at 1. 
In order for the R-S latch to work properly, both S and R must never be allowed to be set to 0 at the same time.

We use the term set to denote setting a variable to 0 or 1, as in “set to 0” or “set to 1.” In addition, we often 
use the term clear  to denote the act of setting a variable to 0.

If we set S to 0 for a very brief period of time, this causes a to equal 1, which in turn causes A to equal 1. Since
R is also 1, the output at b must be 0. This causes B to be 0, which in turn makes a equal to 1. If, after that very
brief period of time, we now return S to 1, it does not aﬀect a. Why? Answer: Since B is also 0, and since only one 
input 0 to a NAND gate is enough to guarantee that the output of the NAND gate is 1, the latch will continue to 
store a 1 long after S returns to 1.

In the same way, we can clear the latch (set the latch to 0) by setting R to 0 for a very short period of time. 

We should point out that if both S and R were allowed to be set to 0 at the same time, the outputs a and b would 
both be 1, and the ﬁnal state of the latch would depend on the electrical properties of the transistors making up 
the gates and not on the logic being performed. How the electrical properties of the transistors would determine the
ﬁnal state in this case is a subject we will have to leave for a later semester. :-(

Finally, we should note that when a digital circuit is powered on, the latch can be in either of its two states, 0 
or 1. It does not matter which state since we never use that information until after we have set it to 1 or 0.

