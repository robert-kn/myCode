1. A Tour of Computer Systems

All information in a system—including disk files, programs stored in memory, user data stored in memory, and data 
transferred across a network—is represented as a bunch of bits. The only thing that distinguishes different data 
objects is the context in which we view them.

On a Unix system, the translation from source file to object file is performed by a compiler driver. see helloc 
and figure1.3

The translation is performed in the sequence of four phases shown in Figure 1.3    

1. Preprocessing phase: The preprocessor (cpp) modifies the original C program according to directives that begin 
with the ` #' character. For example, the #include <stdio.h> command in line hello.c stdio.h 1 of tells the 
preprocessor to read the contents of the system header file and insert it directly into the program text. The 
result is another C program, typically with the .i suffix.

2. Compilation phase. The compiler (cc1) translates the text file hello.i into the text file hello.s, which 
contains an assembly-language program. Assembly language is useful because it provides a common output language 
for different compilers for different high-level languages.

3. Assembly phase. Next, the assembler (as) translates hello.s into machine-language instructions, packages them 
in a form known as a relocatable object program, and stores the result in the object file hello.o. This file is a 
binary file containing 17 bytes to encode the instructions for function main. If we were to view hello.o with a 
text editor, it would appear to be gibberish.

4. Linking phase. Notice that our hello program calls the printf function, which is part of the standard C library 
provided by every C compiler. The printf function resides in a separate precompiled object file called printf.o, 
which must somehow be merged with our hello.o program. The linker (ld) handles this merging. The result is the 
hello file, which is an executable object file (or simply executable) that is ready to be loaded into memory and 
executed by the system.

why it pays to understand how compilation systems work; there are some important reasons why programmers need to 
understand how compilation systems work:

1. Optimizing program performance. Modern compilers are sophisticated tools that usually produce good code. As 
programmers, we do not need to know the inner workings of the compiler in order to write efficient code. However, 
in order to make good coding decisions in our C programs, we do need a basic understanding of machine-level code 
and how the compiler translates different C statements into machine code. For example, is a switch statement 
always more efficient than a sequence of if-else statements? How much overhead is incurred by a function call? Is 
a while loop more efficient than a for loop? Are pointer references more efficient than array indexes? Why does 
our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? 
How can a function run faster when we simply rearrange the parentheses in an arithmetic expression?

2. Understanding link-time errors. In our experience, some of the most perplexing programming errors are related 
to the operation of the linker, especially when you are trying to build large software systems. For example, what 
does it mean when the linker reports that it cannot resolve a reference? What is the difference between a static 
variable and a global variable? What happens if you define two global variables in different C files with the same 
name? What is the difference between a static library and a dynamic library? Why does it matter what order we list 
libraries on the command line? And scariest of all, why do some linker-related errors not appear until run time? 

3. Avoiding security holes. For many years, buffer overflow vulnerabilities have accounted for many of the 
security holes in network and Internet servers. These vulnerabilities exist because too few programmers understand 
the need to carefully restrict the quantity and forms of data they accept from untrusted sources. A first step in 
learning secure programming is to understand the consequences of the way data and control information are stored 
on the program stack. 

The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then 
performs the command. If the first word of the command line does not correspond to a built-in shell command, then 
the shell assumes that it is the name of an executable file that it should load and run. So in this case, the 
shell loads and runs the program and then waits for it to terminate. 

Hardware Organization of a System

figure1.4 is modeled on recent systems.

1. Buses: carry bytes of information back and forth between the components. Buses are typically designed to 
transfer fixed-size chunks of bytes known as words. The number of bytes in a word (the word size) is a fundamental 
system parameter that varies across systems. Most machines today have word sizes of either 4 bytes (32 bits) or 8 
bytes (64 bits).

2. I/O Devices: are the system's connection to the external world. Our example system has four I/O devices: a 
keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term 
storage of data and programs. Each I/O device is connected to the I/O bus by either a controller or an adapter. 

3. Main Memory: is a temporary storage device that holds both a program and the data it manipulates while the 
processor is executing the program. Physically, main memory consists of a collection of dynamic random access 
memory(DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address 
(array index) starting at zero. In general, each of the machine instructions that constitute a program can consist 
of a variable number of bytes. The sizes of data items that correspond to C program variables vary according to 
type. 

4. Processor: is the engine that interprets (or executes) instructions stored in main memory. At its core is a 
word-size storage device (or register) called the program counter (PC). At any point in time, the PC points at 
(contains the address of) the next machine-language instruction in main memory to be executed. From the time that 
power is applied to the system until the time that the power is shut off, a processor repeatedly executes the 
instruction pointed at by the program counter and updates the program counter to point to the next instruction. A 
processor appears to operate according to a very simple instruction execution model, defined by its instruction 
set architecture. In this model, instructions execute in strict sequence, and executing a single instruction 
involves performing a series of steps. The processor reads the instruction from memory pointed at by the program 
counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, 
and then updates the PC to point to the next instruction, which may or may not be contiguous in memory to the 
instruction that was just executed.

Running the hello Program: As we type the characters ./hello at the keyboard, the shell program reads each one 
into a register and then stores it in memory, as shown in Figure 1.5. When we hit the enter key on the keyboard, 
the shell knows that we have finished typing the command. The shell then loads the executable hello file by 
executing a sequence of instructions that copies the code and data in the hello object file from disk to main 
memory. The data includes the string of characters hello, world\n that will eventually be printed out. Using a 
technique known as direct memory access (DMA, discussed in Chapter 6), the data travel directly from disk to main 
memory, without passing through the processor. This is shown in figure1.6

Once the code and data in the object file are loaded into memory, the processor begins executing the 
machine-language instructions in the hello program's main routine. These instructions copy the bytes in the hello, 
world\n string from memory to the register file, and from there to the display device, where they are displayed on
the screen. This step is shown in Figure 1.7.

Caches Matter: a system spends a lot of time moving information from one place to another. From a programmer's 
perspective, much of this copying is overhead that slows down the "real work" of the program. Thus, a major goal 
for system designers is to make these copy operations run as fast as possible. Because of physical laws, larger 
storage devices are slower than smaller storage devices. And faster devices are more expensive to build than their 
slower counterparts.

It is easier and cheaper to make processors run faster than it is to make main memory run faster. why?

To deal with the processor-memory gap, system designers include smaller, faster storage devices called cache 
memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to 
need in the near future. Figure 1.8 shows the cache memories in a typical system. An L1 cache on the processor 
chip holds tens of thousands of bytes and can be accessed nearly as fast as the register file. A larger L2 cache 
with hundreds of thousands to millions of bytes is connected to the processor by a special bus. It might take 5 
times longer for the processor to access the L2 cache than the L1 cache, but this is still 5 to 10 times faster 
than accessing the main memory. The L1 and L2 caches are implemented with a hardware technology known as static 
random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. The 
idea behind caching is that a system can get the effect of both a very large memory and a very fast one by 
exploiting locality, the tendency for programs to access data and code in localized regions. By setting up caches 
to hold data that are likely to be accessed often, we can perform most memory operations using the fast caches.

Storage Devices Form a Hierarchy








