1. A Tour of Computer Systems

All information in a system—including disk files, programs stored in memory, user data stored in memory, and data 
transferred across a network—is represented as a bunch of bits. The only thing that distinguishes different data 
objects is the context in which we view them.

On a Unix system, the translation from source file to object file is performed by a compiler driver. see helloc 
and figure1.3

The translation is performed in the sequence of four phases shown in Figure 1.3    

1. Preprocessing phase: The preprocessor (cpp) modifies the original C program according to directives that begin 
with the ` #' character. For example, the #include <stdio.h> command in line hello.c stdio.h 1 of tells the 
preprocessor to read the contents of the system header file and insert it directly into the program text. The 
result is another C program, typically with the .i suffix.

2. Compilation phase. The compiler (cc1) translates the text file hello.i into the text file hello.s, which 
contains an assembly-language program. Assembly language is useful because it provides a common output language 
for different compilers for different high-level languages.

3. Assembly phase. Next, the assembler (as) translates hello.s into machine-language instructions, packages them 
in a form known as a relocatable object program, and stores the result in the object file hello.o. This file is a 
binary file containing 17 bytes to encode the instructions for function main. If we were to view hello.o with a 
text editor, it would appear to be gibberish.

4. Linking phase. Notice that our hello program calls the printf function, which is part of the standard C library 
provided by every C compiler. The printf function resides in a separate precompiled object file called printf.o, 
which must somehow be merged with our hello.o program. The linker (ld) handles this merging. The result is the 
hello file, which is an executable object file (or simply executable) that is ready to be loaded into memory and 
executed by the system.

why it pays to understand how compilation systems work; there are some important reasons why programmers need to 
understand how compilation systems work:

1. Optimizing program performance. Modern compilers are sophisticated tools that usually produce good code. As 
programmers, we do not need to know the inner workings of the compiler in order to write efficient code. However, 
in order to make good coding decisions in our C programs, we do need a basic understanding of machine-level code 
and how the compiler translates different C statements into machine code. For example, is a switch statement 
always more efficient than a sequence of if-else statements? How much overhead is incurred by a function call? Is 
a while loop more efficient than a for loop? Are pointer references more efficient than array indexes? Why does 
our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? 
How can a function run faster when we simply rearrange the parentheses in an arithmetic expression?

2. Understanding link-time errors. In our experience, some of the most perplexing programming errors are related 
to the operation of the linker, especially when you are trying to build large software systems. For example, what 
does it mean when the linker reports that it cannot resolve a reference? What is the difference between a static 
variable and a global variable? What happens if you define two global variables in different C files with the same 
name? What is the difference between a static library and a dynamic library? Why does it matter what order we list 
libraries on the command line? And scariest of all, why do some linker-related errors not appear until run time? 

3. Avoiding security holes. For many years, buffer overflow vulnerabilities have accounted for many of the 
security holes in network and Internet servers. These vulnerabilities exist because too few programmers understand 
the need to carefully restrict the quantity and forms of data they accept from untrusted sources. A first step in 
learning secure programming is to understand the consequences of the way data and control information are stored 
on the program stack. 

The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then 
performs the command. If the first word of the command line does not correspond to a built-in shell command, then 
the shell assumes that it is the name of an executable file that it should load and run. So in this case, the 
shell loads and runs the program and then waits for it to terminate. 

Hardware Organization of a System

figure1.4 is modeled on recent systems.

1. Buses: carry bytes of information back and forth between the components. Buses are typically designed to 
transfer fixed-size chunks of bytes known as words. The number of bytes in a word (the word size) is a fundamental 
system parameter that varies across systems. Most machines today have word sizes of either 4 bytes (32 bits) or 8 
bytes (64 bits).

2. I/O Devices: are the system's connection to the external world. Our example system has four I/O devices: a 
keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term 
storage of data and programs. Each I/O device is connected to the I/O bus by either a controller or an adapter. 

3. Main Memory: is a temporary storage device that holds both a program and the data it manipulates while the 
processor is executing the program. Physically, main memory consists of a collection of dynamic random access 
memory(DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address 
(array index) starting at zero. In general, each of the machine instructions that constitute a program can consist 
of a variable number of bytes. The sizes of data items that correspond to C program variables vary according to 
type. 

4. Processor: is the engine that interprets (or executes) instructions stored in main memory. At its core is a 
word-size storage device (or register) called the program counter (PC). At any point in time, the PC points at 
(contains the address of) the next machine-language instruction in main memory to be executed. From the time that 
power is applied to the system until the time that the power is shut off, a processor repeatedly executes the 
instruction pointed at by the program counter and updates the program counter to point to the next instruction. A 
processor appears to operate according to a very simple instruction execution model, defined by its instruction 
set architecture. In this model, instructions execute in strict sequence, and executing a single instruction 
involves performing a series of steps. The processor reads the instruction from memory pointed at by the program 
counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, 
and then updates the PC to point to the next instruction, which may or may not be contiguous in memory to the 
instruction that was just executed.

Running the hello Program: As we type the characters ./hello at the keyboard, the shell program reads each one 
into a register and then stores it in memory, as shown in Figure 1.5. When we hit the enter key on the keyboard, 
the shell knows that we have finished typing the command. The shell then loads the executable hello file by 
executing a sequence of instructions that copies the code and data in the hello object file from disk to main 
memory. The data includes the string of characters hello, world\n that will eventually be printed out. Using a 
technique known as direct memory access (DMA, discussed in Chapter 6), the data travel directly from disk to main 
memory, without passing through the processor. This is shown in figure1.6

Once the code and data in the object file are loaded into memory, the processor begins executing the 
machine-language instructions in the hello program's main routine. These instructions copy the bytes in the hello, 
world\n string from memory to the register file, and from there to the display device, where they are displayed on
the screen. This step is shown in Figure 1.7.

Caches Matter: a system spends a lot of time moving information from one place to another. From a programmer's 
perspective, much of this copying is overhead that slows down the "real work" of the program. Thus, a major goal 
for system designers is to make these copy operations run as fast as possible. Because of physical laws, larger 
storage devices are slower than smaller storage devices. And faster devices are more expensive to build than their 
slower counterparts.

It is easier and cheaper to make processors run faster than it is to make main memory run faster. why?

To deal with the processor-memory gap, system designers include smaller, faster storage devices called cache 
memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to 
need in the near future. Figure 1.8 shows the cache memories in a typical system. An L1 cache on the processor 
chip holds tens of thousands of bytes and can be accessed nearly as fast as the register file. A larger L2 cache 
with hundreds of thousands to millions of bytes is connected to the processor by a special bus. It might take 5 
times longer for the processor to access the L2 cache than the L1 cache, but this is still 5 to 10 times faster 
than accessing the main memory. The L1 and L2 caches are implemented with a hardware technology known as static 
random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. The 
idea behind caching is that a system can get the effect of both a very large memory and a very fast one by 
exploiting locality, the tendency for programs to access data and code in localized regions. By setting up caches 
to hold data that are likely to be accessed often, we can perform most memory operations using the fast caches.

Storage Devices Form a Hierarchy

The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower 
level. Thus, the register file is a cache for the L1 cache. Caches L1 and L2 are caches for L2 and L3, respectively. 
The L3 cache is a cache for the main memory, which is a cache for the disk. On some networked systems with 
distributed file systems, the local disk serves as a cache for data stored on the disks of other systems.

The Operating System Manages the Hardware

We can think of the operating system as a layer of software interposed between the application program and the 
hardware, as shown in Figure 1.10. All attempts by an application program to manipulate the hardware must go 
through the operating system. The operating system has two primary purposes: (1) to protect the hardware from 
misuse by runaway applications and (2) to provide applications with simple and uniform mechanisms for manipulating 
complicated and often wildly different low-level hardware devices. The operating system achieves both goals via the 
fundamental abstractions shown in Figure 1.11: processes, virtual memory, and files. As this figure suggests, files 
are abstractions for I/O devices, virtual memory is an abstraction for both the main memory and disk I/O devices, 
and processes are abstractions for the processor, main memory, and I/O devices.

Processes

When a program such as hello runs on a modern system, the operating system provides the illusion that the program 
is the only one running on the system. The program appears to have exclusive use of both the processor, main 
memory, and I/O devices. The processor appears to execute the instructions in the program, one after the other, 
without interruption. And the code and data of the program appear to be the only objects in the system's memory. 
These illusions are provided by the notion of a process.

A process is the operating system's abstraction for a running program. Multiple processes can run concurrently on 
the same system, and each process appears to have exclusive use of the hardware. By concurrently, we mean that the 
instructions of one process are interleaved with the instructions of another process. In most systems, there are 
more processes to run than there are CPUs to run them.

Traditional systems could only execute one program at a time, while newer multi-core processors can execute several 
programs simultaneously. In either case, a single CPU can appear to execute multiple processes concurrently by 
having the processor switch among them. The operating system performs this interleaving with a mechanism known as 
context switching. 

The operating system keeps track of all the state information that the process needs in order to run. This state, 
which is known as the context, includes information such as the current values of the PC, the register file, and 
the contents of main memory. At any point in time, a uniprocessor system can only execute the code for a single 
process. When the operating system decides to transfer control from the current process to some new process, it 
performs a context switch by saving the context of the current process, restoring the context of the new process, 
and then passing control to the new process. Figure 1.12 shows the basic idea for our example scenario.

There are two concurrent processes in our example scenario: the shell process and the process. Initially, the 
shell process is running alone, waiting for input on the command line. When we ask it to run the program, the shell 
carries out our request by invoking a special function known as a system call that passes control to the operating 
system. The operating system saves the shell's context, creates a new process and its context, and then passes 
control to the new process. After terminates, the operating system restores the context of the shell process and 
passes control back to it, where it waits for the next command-line input.

As Figure 1.12 indicates, the transition from one process to another is managed by the operating system kernel. 
The kernel is the portion of the operating system code that is always resident in memory. When an application 
program requires some action by the operating system, such as to read or write a file, it executes a special 
system call instruction, transferring control to the kernel. The kernel then performs the requested operation and 
returns back to the application program. Note that the kernel is not a separate process. Instead, it is a 
collection of code and data structures that the system uses to manage all the processes.

Threads

Although we normally think of a process as having a single control flow, in modern systems a process can actually 
consist of multiple execution units, called threads, each running in the context of the process and sharing the 
same code and global data. Threads are an increasingly important programming model because of the requirement for 
concurrency in network servers, because it is easier to share data between multiple threads than between multiple 
processes, and because threads are typically more efficient than processes. Multi-threading is also one way to make 
programs run faster when multiple processors are available.

Virtual Memory

Virtual memory is an abstraction that provides each process with the illusion that it has exclusive use of the 
main memory. Each process has the same uniform view of memory, which is known as its virtual address space. The 
virtual address space for Linux processes is shown in figure 1.13. In Linux, the topmost region of the address 
space is reserved for code and data in the operating system that is common to all processes. The lower region of 
the address space holds the code and data defined by the user's process. Note that addresses in the figure 
increase from the bottom to the top.

The virtual address space seen by each process consists of a number of well-defined areas, each with a specific 
purpose.

Program code and data: Code begins at the same fixed address for all processes, followed by data locations that 
correspond to global C variables. The code and data areas are initialized directly from the contents of an 
executable object file—in our case, the hello executable.

Heap: The code and data areas are followed immediately by the run-time heap. Unlike the code and data areas, which 
are fixed in size once the process begins running, the heap expands and contracts dynamically at run time as a 
result of calls to C standard library routines such as malloc and heap.

Shared libraries: Near the middle of the address space is an area that holds the code and data for shared libraries 
such as the C standard library and the math library. The notion of a shared library is a powerful but somewhat 
difficult concept. 

Stack: At the top of the user's virtual address space is the user stack that the compiler uses to implement 
function calls. Like the heap, the user stack expands and contracts dynamically during the execution of the 
program. In particular, each time we call a function, the stack grows. Each time we return from a function, it 
contracts. 

Kernel virtual memory. The top region of the address space is reserved for the kernel. Application programs are 
not allowed to read or write the contents of this area or to directly call functions defined in the kernel code. 
Instead, they must invoke the kernel to perform these operations.

For virtual memory to work, a sophisticated interaction is required between the hardware and the operating system 
software, including a hardware translation of every address generated by the processor. The basic idea is to store 
the contents of a process's virtual memory on disk and then use the main memory as a cache for the disk.

Files

A file is a sequence of bytes, nothing more and nothing less. Every I/O device, including disks, keyboards, 
displays, and even networks, is modeled as a file. All input and output in the system is performed by reading and 
writing files, using a small set of system calls known as Unix I/O.

This simple and elegant notion of a file is nonetheless very powerful because it provides applications with a 
uniform view of all the varied I/O devices that might be contained in the system. For example, application 
programmers who manipulate the contents of a disk file are blissfully unaware of the specific disk technology. 
Further, the same program will run on different systems that use different disk technologies. 

Systems Communicate with Other Systems Using Networks

Up to this point in our tour of systems, we have treated a system as an isolated collection of hardware and 
software. In practice, modern systems are often linked to other systems by networks. From the point of view of an 
individual system, the network can be viewed as just another I/O device, as shown in Figure 1.14. When the system
copies a sequence of bytes from main memory to the network adapter, the data flow across the network to another 
machine, instead of, say, to a local disk drive. Similarly, the system can read data sent from other machines and 
copy these data to its main memory.

With the advent of global networks such as the Internet, copying information from one machine to another has become 
one of the most important uses of computer systems. For example, applications such as email, instant messaging, the 
World Wide Web, FTP, and telnet are all based on the ability to copy information over a network.

Concurrency and Parallelism

Throughout the history of digital computers, two demands have been constant forces in driving improvements: we want 
them to do more, and we want them to run faster. Both of these factors improve when the processor does more things 
at once. We use the term concurrency to refer to the general concept of a system with multiple, simultaneous 
activities, and the term parallelism to refer to the use of concurrency to make a system run faster. Parallelism 
can be exploited at multiple levels of abstraction in a computer system. We highlight three levels here, working 
from the highest to the lowest level in the system hierarchy.

Thread-Level Concurrency

Building on the process abstraction, we are able to devise systems where multiple programs execute at the same 
time, leading to concurrency. With threads, we can even have multiple control flows executing within a single 
process. Support for concurrent execution has been found in computer systems since the advent of time-sharing in 
the early 1960s. Traditionally, this concurrent execution was only simulated, by having a single computer rapidly 
switch among its executing processes, much as a juggler keeps multiple balls flying through the air. This form of 
concurrency allows multiple users to interact with a system at the same time, such as when many people want to get 
pages from a single Web server. It also allows a single user to engage in multiple tasks concurrently, such as 
having a Web browser in one window, a word processor in another, and streaming music playing at the same time. 
Until recently, most actual computing was done by a single processor, even if that processor had to switch among 
multiple tasks. This configuration is known as a uniprocessor system.

When we construct a system consisting of multiple processors all under the control of a single operating system 
kernel, we have a multiprocessor system. Such systems have been available for large- scale computing since the 
1980s, but they have more recently become commonplace with the advent of multi-core processors and hyperthreading.
Figure 1.16 shows a taxonomy of these different processor types.

Multi-core processors have several CPUs (referred to as "cores") integrated onto a single integrated-circuit chip. 
Figure 1.17 illustrates the organization of a typical multi-core processor, where the chip has four CPU cores, each 
with its own L1 and L2 caches, and with each L1 cache split into two parts—one to hold recently fetched 
instructions and one to hold data. The cores share higher levels of cache as well as the interface to main memory. 
Industry experts predict that they will be able to have dozens, and ultimately hundreds, of cores on a single chip.

Hyperthreading, sometimes called simultaneous multi-threading, is a technique that allows a single CPU to execute 
multiple flows of control. It involves having multiple copies of some of the CPU hardware, such as program counters 
and register files, while having only single copies of other parts of the hardware, such as the units that perform 
floating-point arithmetic. Whereas a conventional processor requires around 20,000 clock cycles to shift between
different threads, a hyper threaded processor decides which of its threads to execute on a cycle-by-cycle basis. It 
enables the CPU to take better advantage of its processing resources. For example, if one thread must wait for some 
data to be loaded into a cache, the CPU can proceed with the execution of a different thread. As an example, the 
Intel Core i7 processor can have each core executing two threads, and so a four-core system can actually execute 
eight threads in parallel.

The use of multiprocessing can improve system performance in two ways. First, it reduces the need to simulate 
concurrency when performing multiple tasks. As mentioned, even a personal computer being used by a single person 
is expected to perform many activities concurrently. Second, it can run a single application program faster, but 
only if that program is expressed in terms of multiple threads that can effectively execute in parallel. Thus, 
although the principles of concurrency have been formulated and studied for over 50 years, the advent of multi-core 
and hyperthreaded systems has greatly increased the desire to find ways to write application programs that can 
exploit the thread-level parallelism available with the hardware. 

Instruction-Level Parallelism

At a much lower level of abstraction, modern processors can execute multiple instructions at one time, a property 
known as instruction-level parallelism. For example, early microprocessors, such as the 1978- vintage Intel 8086, 
required multiple (typically 3-10) clock cycles to execute a single instruction. More recent processors can sustain 
execution rates of 2-4 instructions per clock cycle. Any given instruction requires much longer from start to 
finish, perhaps 20 cycles or more, but the processor uses a number of clever tricks to process as many as 100 
instructions at a time. 

Single-Instruction, Multiple-Data (SIMD) Parallelism

At the lowest level, many modern processors have special hardware that allows a single instruction to cause 
multiple operations to be performed in parallel, a mode known as single-instruction, multiple- data(SIMD) 
parallelism. These SIMD instructions are provided mostly to speed up applications that process image, sound, and 
video data. Although some compilers attempt to automatically extract SIMD parallelism from C programs, a more 
reliable method is to write programs using special vector data types supported in compilers such as GCC. 

Chapter 2 Representing and Manipulating Information

Computer representations use a limited number of bits to encode a number, and hence some operations can overflow 
when the results are too large to be represented. The different mathematical properties of integer versus. 
floating-point arithmetic stem from the difference in how they handle the finiteness of their 
representations—integer representations can encode a comparatively small range of values, but do so precisely, 
while floating-point representations can encode a wide range of values, but only approximately.

By studying the actual number representations, we can understand the ranges of values that can be represented and 
the properties of the different arithmetic operations. This understanding is critical to writing programs that work 
correctly over the full range of numeric values and that are portable across different combinations of machine, 
operating system, and compiler. 

The C++ programming language is built upon C, using the exact same numeric representations and operations. 
Everything said in this chapter about C also holds for C++. The Java language definition, on the other hand, 
created a new set of standards for numeric representations and operations. Whereas the C standards are designed to 
allow a wide range of implementations, the Java standard is quite specific on the formats and encodings of data.

2.1 Information Storage

Rather than accessing individual bits in memory, most computers use blocks of 8 bits, or bytes, as the smallest 
addressable unit of memory. A machine-level program views memory as a very large array of bytes, referred to as 
virtual memory. Every byte of memory is identified by a unique number, known as its address, and the set of all 
possible addresses is known as the virtual address space. As indicated by its name, this virtual address space is 
just a conceptual image presented to the machine-level program. The actual implementation uses a combination of 
dynamic random access memory (DRAM), flash memory, disk storage, special hardware, and operating system software to 
provide the program with what appears to be a monolithic byte array.

2.1.2 Data Sizes

Every computer has a word size, indicating the nominal size of pointer data. Since a virtual address is encoded by 
such a word, the most important system parameter determined by the word size is the maximum size of the virtual 
address space. That is, for a machine with a w-bit word size, the virtual addresses can range from 0 to 
2^(w) — 1, giving the program access to at most 2^(w) bytes.

A 32-bit word size limits the virtual address space to 4 gigabytes (written 4 GB). Scaling up to a 64-bit word 
size leads to a virtual address space of 16 exabytes, or around 1.84 × 10^(19) bytes. Most 64-bit machines can also 
run programs compiled for use on 32- bit machines, a form of backward compatibility. So, for example, when a 
program is compiled with the directive gcc –m32 prog.c then this program will run correctly on either a 32-bit or 
a 64-bit machine. On the other hand, a program compiled with the directive gcc –m64 prog.c will only run on a 
64-bit machine. We will therefore refer to programs as being either “32-bit programs” or “64-bit programs,” since 
the distinction lies in how a program is compiled, rather than the type of machine on which it runs.

The C language supports multiple data formats for both integer and floating-point data. The exact numbers of bytes 
for some data types depends on how the program is compiled. To avoid the vagaries of relying on “typical” sizes and 
different compiler settings, ISO C99 introduced a class of data types where the data sizes are fixed regardless of 
compiler and machine settings. Among these are data types int32_t and int64_t, having exactly 4 and 8 bytes, 
respectively. Using fixed-size integer types is the best way for programmers to have close control over data 
representations.

Most of the data types encode signed values, unless prefixed by the keyword unsigned or using the specific unsigned 
declaration for fixed-size data types. The exception to this is data type. Although most compilers and machines 
treat these as signed data, the C standard does not guarantee this. Instead, as indicated by the square
char brackets, the programmer should use the declaration signed char to guarantee a 1-byte signed value. 

Programmers should strive to make their programs portable across different machines and compilers. One aspect of 
portability is to make the program insensitive to the exact sizes of the different data types. The C standards set 
lower bounds on the numeric ranges of the different data types but there are no upper bounds (except with the 
fixed-size types). With 32-bit machines and 32-bit programs being the dominant combination from around 1980 until 
around 2010, many programs have been written assuming the allocations for 32-bit programs. 

With the transition to 64-bit machines, many hidden word size dependencies have arisen as bugs in migrating these 
programs to new machines. For example, many programmers historically assumed that an object declared as type int 
could be used to store a pointer. This works fine for most 32-bit programs, but it leads to problems for 64-bit 
programs.

Addressing and Byte Ordering

For program objects that span multiple bytes, we must establish two conventions: what the address of the object 
will be, and how we will order the bytes in memory. In virtually all machines, a multi-byte object is stored as a 
contiguous sequence of bytes, with the address of the object given by the smallest address of the bytes used. For 
example, suppose a variable x of type int has address 0x100; that is, the value of the address expression &x is           . 
0x100. Then (assuming data type int has a 32-bit representation) the 4 bytes of x would be stored in memory 
locations 0x100, 0x101, 0x102, and 0x103.

For ordering the bytes representing an object, there are two common conventions. Consider a w-bit integer having a 
bit representation [xw−1,xw−2,⋯,x1,x0], where xw–1 is the most significant bit and x0 is the least. Assuming w is 
a multiple of 8, these bits can be grouped as bytes, with the most significant byte having bits [xw−1,xw−2,⋯,xw−8], 
the least significant byte having bits [x7,x6,…,x0], and the other bytes having bits from the middle. Some machines 
choose to store the object in memory ordered from least significant byte to most, while other machines store them 
from most to least. The former convention — where the least significant byte comes first—is referred to as little 
endian. The latter convention—where the most significant byte comes first—is referred to as big endian.

Suppose the variable x of type int and at address 0x100 has a hexadecimal value of 0x01234567. The ordering of the 
bytes within the address range 0x100 through 0x103 depends on the type of machine: see little-big-endian.png

Note that in the word 0x01234567 the high-order byte has hexadecimal value 0x01, while the low-order byte has value
0x67.

Most Intel-compatible machines operate exclusively in little-endian mode. On the other hand, most machines from IBM 
and Oracle (arising from their acquisition of Sun Microsystems in 2010) operate in big-endian mode. Many recent 
microprocessor chips are bi-endian, meaning that they can be configured to operate as either little- or big-endian 
machines. In practice, however, byte ordering becomes fixed once a particular operating system is chosen. For 
example, ARM microprocessors, used in many cell phones, have hardware that can operate in either little- or 
big-endian mode, but the two most common operating systems for these chips—Android (from Google) and IOS (from 
Apple) —operate only in little-endian mode.

For most application programmers, the byte orderings used by their machines are totally invisible; programs 
compiled for either class of machine give identical results. At times, however, byte ordering becomes an issue. 
The first is when binary data are communicated over a network between different machines. A common problem is for 
data produced by a little-endian machine to be sent to a big-endian machine, or vice versa, leading to the bytes 
within the words being in reverse order for the receiving program. To avoid such problems, code written for 
networking applications must follow established conventions for byte ordering to make sure the sending machine 
converts its internal representation to the network standard, while the receiving machine converts the network 
standard to its internal representation.