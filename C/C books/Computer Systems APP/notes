1. A Tour of Computer Systems

All information in a system—including disk files, programs stored in memory, user data stored in memory, and data 
transferred across a network—is represented as a bunch of bits. The only thing that distinguishes different data 
objects is the context in which we view them.

On a Unix system, the translation from source file to object file is performed by a compiler driver. see helloc 
and figure1.3

The translation is performed in the sequence of four phases shown in Figure 1.3    

1. Preprocessing phase: The preprocessor (cpp) modifies the original C program according to directives that begin 
with the ` #' character. For example, the #include <stdio.h> command in line hello.c stdio.h 1 of tells the 
preprocessor to read the contents of the system header file and insert it directly into the program text. The 
result is another C program, typically with the .i suffix.

2. Compilation phase. The compiler (cc1) translates the text file hello.i into the text file hello.s, which 
contains an assembly-language program. Assembly language is useful because it provides a common output language 
for different compilers for different high-level languages.

3. Assembly phase. Next, the assembler (as) translates hello.s into machine-language instructions, packages them 
in a form known as a relocatable object program, and stores the result in the object file hello.o. This file is a 
binary file containing 17 bytes to encode the instructions for function main. If we were to view hello.o with a 
text editor, it would appear to be gibberish.

4. Linking phase. Notice that our hello program calls the printf function, which is part of the standard C library 
provided by every C compiler. The printf function resides in a separate precompiled object file called printf.o, 
which must somehow be merged with our hello.o program. The linker (ld) handles this merging. The result is the 
hello file, which is an executable object file (or simply executable) that is ready to be loaded into memory and 
executed by the system.

why it pays to understand how compilation systems work; there are some important reasons why programmers need to 
understand how compilation systems work:

1. Optimizing program performance. Modern compilers are sophisticated tools that usually produce good code. As 
programmers, we do not need to know the inner workings of the compiler in order to write efficient code. However, 
in order to make good coding decisions in our C programs, we do need a basic understanding of machine-level code 
and how the compiler translates different C statements into machine code. For example, is a switch statement 
always more efficient than a sequence of if-else statements? How much overhead is incurred by a function call? Is 
a while loop more efficient than a for loop? Are pointer references more efficient than array indexes? Why does 
our loop run so much faster if we sum into a local variable instead of an argument that is passed by reference? 
How can a function run faster when we simply rearrange the parentheses in an arithmetic expression?

2. Understanding link-time errors. In our experience, some of the most perplexing programming errors are related 
to the operation of the linker, especially when you are trying to build large software systems. For example, what 
does it mean when the linker reports that it cannot resolve a reference? What is the difference between a static 
variable and a global variable? What happens if you define two global variables in different C files with the same 
name? What is the difference between a static library and a dynamic library? Why does it matter what order we list 
libraries on the command line? And scariest of all, why do some linker-related errors not appear until run time? 

3. Avoiding security holes. For many years, buffer overflow vulnerabilities have accounted for many of the 
security holes in network and Internet servers. These vulnerabilities exist because too few programmers understand 
the need to carefully restrict the quantity and forms of data they accept from untrusted sources. A first step in 
learning secure programming is to understand the consequences of the way data and control information are stored 
on the program stack. 

The shell is a command-line interpreter that prints a prompt, waits for you to type a command line, and then 
performs the command. If the first word of the command line does not correspond to a built-in shell command, then 
the shell assumes that it is the name of an executable file that it should load and run. So in this case, the 
shell loads and runs the program and then waits for it to terminate. 

Hardware Organization of a System

figure1.4 is modeled on recent systems.

1. Buses: carry bytes of information back and forth between the components. Buses are typically designed to 
transfer fixed-size chunks of bytes known as words. The number of bytes in a word (the word size) is a fundamental 
system parameter that varies across systems. Most machines today have word sizes of either 4 bytes (32 bits) or 8 
bytes (64 bits).

2. I/O Devices: are the system's connection to the external world. Our example system has four I/O devices: a 
keyboard and mouse for user input, a display for user output, and a disk drive (or simply disk) for long-term 
storage of data and programs. Each I/O device is connected to the I/O bus by either a controller or an adapter. 

3. Main Memory: is a temporary storage device that holds both a program and the data it manipulates while the 
processor is executing the program. Physically, main memory consists of a collection of dynamic random access 
memory(DRAM) chips. Logically, memory is organized as a linear array of bytes, each with its own unique address 
(array index) starting at zero. In general, each of the machine instructions that constitute a program can consist 
of a variable number of bytes. The sizes of data items that correspond to C program variables vary according to 
type. 

4. Processor: is the engine that interprets (or executes) instructions stored in main memory. At its core is a 
word-size storage device (or register) called the program counter (PC). At any point in time, the PC points at 
(contains the address of) the next machine-language instruction in main memory to be executed. From the time that 
power is applied to the system until the time that the power is shut off, a processor repeatedly executes the 
instruction pointed at by the program counter and updates the program counter to point to the next instruction. A 
processor appears to operate according to a very simple instruction execution model, defined by its instruction 
set architecture. In this model, instructions execute in strict sequence, and executing a single instruction 
involves performing a series of steps. The processor reads the instruction from memory pointed at by the program 
counter (PC), interprets the bits in the instruction, performs some simple operation dictated by the instruction, 
and then updates the PC to point to the next instruction, which may or may not be contiguous in memory to the 
instruction that was just executed.

Running the hello Program: As we type the characters ./hello at the keyboard, the shell program reads each one 
into a register and then stores it in memory, as shown in Figure 1.5. When we hit the enter key on the keyboard, 
the shell knows that we have finished typing the command. The shell then loads the executable hello file by 
executing a sequence of instructions that copies the code and data in the hello object file from disk to main 
memory. The data includes the string of characters hello, world\n that will eventually be printed out. Using a 
technique known as direct memory access (DMA, discussed in Chapter 6), the data travel directly from disk to main 
memory, without passing through the processor. This is shown in figure1.6

Once the code and data in the object file are loaded into memory, the processor begins executing the 
machine-language instructions in the hello program's main routine. These instructions copy the bytes in the hello, 
world\n string from memory to the register file, and from there to the display device, where they are displayed on
the screen. This step is shown in Figure 1.7.

Caches Matter: a system spends a lot of time moving information from one place to another. From a programmer's 
perspective, much of this copying is overhead that slows down the "real work" of the program. Thus, a major goal 
for system designers is to make these copy operations run as fast as possible. Because of physical laws, larger 
storage devices are slower than smaller storage devices. And faster devices are more expensive to build than their 
slower counterparts.

It is easier and cheaper to make processors run faster than it is to make main memory run faster. why?

To deal with the processor-memory gap, system designers include smaller, faster storage devices called cache 
memories (or simply caches) that serve as temporary staging areas for information that the processor is likely to 
need in the near future. Figure 1.8 shows the cache memories in a typical system. An L1 cache on the processor 
chip holds tens of thousands of bytes and can be accessed nearly as fast as the register file. A larger L2 cache 
with hundreds of thousands to millions of bytes is connected to the processor by a special bus. It might take 5 
times longer for the processor to access the L2 cache than the L1 cache, but this is still 5 to 10 times faster 
than accessing the main memory. The L1 and L2 caches are implemented with a hardware technology known as static 
random access memory (SRAM). Newer and more powerful systems even have three levels of cache: L1, L2, and L3. The 
idea behind caching is that a system can get the effect of both a very large memory and a very fast one by 
exploiting locality, the tendency for programs to access data and code in localized regions. By setting up caches 
to hold data that are likely to be accessed often, we can perform most memory operations using the fast caches.

Storage Devices Form a Hierarchy

The main idea of a memory hierarchy is that storage at one level serves as a cache for storage at the next lower 
level. Thus, the register file is a cache for the L1 cache. Caches L1 and L2 are caches for L2 and L3, respectively. 
The L3 cache is a cache for the main memory, which is a cache for the disk. On some networked systems with 
distributed file systems, the local disk serves as a cache for data stored on the disks of other systems.

The Operating System Manages the Hardware

We can think of the operating system as a layer of software interposed between the application program and the 
hardware, as shown in Figure 1.10. All attempts by an application program to manipulate the hardware must go 
through the operating system. The operating system has two primary purposes: (1) to protect the hardware from 
misuse by runaway applications and (2) to provide applications with simple and uniform mechanisms for manipulating 
complicated and often wildly different low-level hardware devices. The operating system achieves both goals via the 
fundamental abstractions shown in Figure 1.11: processes, virtual memory, and files. As this figure suggests, files 
are abstractions for I/O devices, virtual memory is an abstraction for both the main memory and disk I/O devices, 
and processes are abstractions for the processor, main memory, and I/O devices.

Processes

When a program such as hello runs on a modern system, the operating system provides the illusion that the program 
is the only one running on the system. The program appears to have exclusive use of both the processor, main 
memory, and I/O devices. The processor appears to execute the instructions in the program, one after the other, 
without interruption. And the code and data of the program appear to be the only objects in the system's memory. 
These illusions are provided by the notion of a process.

A process is the operating system's abstraction for a running program. Multiple processes can run concurrently on 
the same system, and each process appears to have exclusive use of the hardware. By concurrently, we mean that the 
instructions of one process are interleaved with the instructions of another process. In most systems, there are 
more processes to run than there are CPUs to run them.

Traditional systems could only execute one program at a time, while newer multi-core processors can execute several 
programs simultaneously. In either case, a single CPU can appear to execute multiple processes concurrently by 
having the processor switch among them. The operating system performs this interleaving with a mechanism known as 
context switching. 

The operating system keeps track of all the state information that the process needs in order to run. This state, 
which is known as the context, includes information such as the current values of the PC, the register file, and 
the contents of main memory. At any point in time, a uniprocessor system can only execute the code for a single 
process. When the operating system decides to transfer control from the current process to some new process, it 
performs a context switch by saving the context of the current process, restoring the context of the new process, 
and then passing control to the new process. Figure 1.12 shows the basic idea for our example scenario.

There are two concurrent processes in our example scenario: the shell process and the process. Initially, the 
shell process is running alone, waiting for input on the command line. When we ask it to run the program, the shell 
carries out our request by invoking a special function known as a system call that passes control to the operating 
system. The operating system saves the shell's context, creates a new process and its context, and then passes 
control to the new process. After terminates, the operating system restores the context of the shell process and 
passes control back to it, where it waits for the next command-line input.

As Figure 1.12 indicates, the transition from one process to another is managed by the operating system kernel. 
The kernel is the portion of the operating system code that is always resident in memory. When an application 
program requires some action by the operating system, such as to read or write a file, it executes a special 
system call instruction, transferring control to the kernel. The kernel then performs the requested operation and 
returns back to the application program. Note that the kernel is not a separate process. Instead, it is a 
collection of code and data structures that the system uses to manage all the processes.

Threads

Although we normally think of a process as having a single control flow, in modern systems a process can actually 
consist of multiple execution units, called threads, each running in the context of the process and sharing the 
same code and global data. Threads are an increasingly important programming model because of the requirement for 
concurrency in network servers, because it is easier to share data between multiple threads than between multiple 
processes, and because threads are typically more efficient than processes. Multi-threading is also one way to make 
programs run faster when multiple processors are available.

Virtual Memory

Virtual memory is an abstraction that provides each process with the illusion that it has exclusive use of the 
main memory. Each process has the same uniform view of memory, which is known as its virtual address space. The 
virtual address space for Linux processes is shown in figure 1.13. In Linux, the topmost region of the address 
space is reserved for code and data in the operating system that is common to all processes. The lower region of 
the address space holds the code and data defined by the user's process. Note that addresses in the figure 
increase from the bottom to the top.

The virtual address space seen by each process consists of a number of well-defined areas, each with a specific 
purpose.

Program code and data: Code begins at the same fixed address for all processes, followed by data locations that 
correspond to global C variables. The code and data areas are initialized directly from the contents of an 
executable object file—in our case, the hello executable.

Heap: The code and data areas are followed immediately by the run-time heap. Unlike the code and data areas, which 
are fixed in size once the process begins running, the heap expands and contracts dynamically at run time as a 
result of calls to C standard library routines such as malloc and heap.

Shared libraries: Near the middle of the address space is an area that holds the code and data for shared libraries 
such as the C standard library and the math library. The notion of a shared library is a powerful but somewhat 
difficult concept. 

Stack: At the top of the user's virtual address space is the user stack that the compiler uses to implement 
function calls. Like the heap, the user stack expands and contracts dynamically during the execution of the 
program. In particular, each time we call a function, the stack grows. Each time we return from a function, it 
contracts. 

Kernel virtual memory. The top region of the address space is reserved for the kernel. Application programs are 
not allowed to read or write the contents of this area or to directly call functions defined in the kernel code. 
Instead, they must invoke the kernel to perform these operations.

For virtual memory to work, a sophisticated interaction is required between the hardware and the operating system 
software, including a hardware translation of every address generated by the processor. The basic idea is to store 
the contents of a process's virtual memory on disk and then use the main memory as a cache for the disk.




