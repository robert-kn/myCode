what should you know about fundamentals? they do not change very often

what will mastery of the fundamentals yield? there is no limit to how high I will soar provided I continue to put in the
work 

what is this book about? mastering the fundamentals of computing

what is the conventional wisdom when it comes to teaching computer science? start  with a high level programming language
which leads to the memorisation of technical details without an understanding of basic underpinnings

how does the book approach mastering computing fundamentals? a bottom up approach where it continually builds on what is 
covered (scaffolding)

what is technique is widely pervasive when it comes to teaching students any subject? information hiding approach

why is it used? because it is a useful productivity enhancer and allows the student to hit the ground running

what are its drawbacks? information hiding gets in the way of understanding which leads to problems when students have to
think from first principles about for example why a particular error is happening (understanding not memorising)

information hiding is only useful once one has an understanding of the fundamentals. bottom up approach is best for learning
in order to acquire an understanding

does that mean that the top down design approach is flawed?

what was their  concern about including C++ in their instructional approach? many of the languages features are too far 
abstracted from the underlying layers to make for an easy fit to their instructional approach. Additionally C++ is a vast 
language that would have necessiated more pages to be added to the book. 

why do they still use C in the book? serves as the defacto development language for systems aand hardware oriented projects

what are the two major segments that the book can be broken down into? (a) the underlying structure of a computer (b) programming
in a high level language

why does the C programming language fit very nicely with the bottom up approach? its low level nature allows students to see
clearly the connection between software and the underlying hardware 

why lc-3 ISA and not ARM or RISCV? because commercial ISAs have no place in an introductory course but still have to be
understood in order to be used effectively. They wanted an ISA that was clean with no special cases to deal with, with as few
opcodes as necessary so that you the student spends all your time on the fundamental concepts of the course and very little 
time on the nuaances of the instruction set

the lc-3 instruction set with only 15 four-bit opcodes, is small enough that the students can absorb the ISA without struggling
too much

what are the stand out observations of the approach taken in the book?

(a) understanding and not memorising: bottom up learning approach leads to less memorisation of seemingly arbitrary rules
which is prevalent in traditional programming courses. by the time a topic is taught, you will have an understanding of how
the topic is implemented at the levels below it. this approach is good for design courses where understanding of and insights
gained from fundamentals are essential to making the required design trade offs.
(b) you get to debug your own programs
(c) preparation for the future: cutting through protective layers; as a professional, if you are ignorant of what is going
on inside computers you will likely discover the hard way that the effectiveness of your solutions is impacted adversely by
things other than the actual programs you write. serious programmers will write more efficient code if they understand what
is going on beyond the statements in their high level language. high level programming language courses where the compiler 
protects the student from everything ugly underneath does not serve most engineering students well
(d) rippling effects through the curriculum: the material taught in the book has a rippling effect on what can be taught in 
subsequent courses. subsequent programming courses cannot only assume the students know the syntax of C/C++ but also understand
how it relates to the underlying architecture. ergo, the focus can be on problem solving and more sophisticated data structures

how to install the simulator debugger written by the authors for the lc-3 ISA:

download link for the material can be found here: https://highered.mheducation.com/sites/1260150534/student_view0/lc-3_simulator.html

LC3Tools is a cross-platform set of tools to build code for and simulate the LC-3 system described in the book.

The latest version of LC3Tools for all platforms can be found at https://github.com/chiragsakhuja/lc3tools/releases

Windows Users: lc3tools-setup-VERSION.exe
macOS Users: LC3Tools-VERSION.dmg
Linux Users: lc3tools-VERSION-x86_64.AppImage

There are several other resources, including more documentation and instructions on the command line tools, at https://github.com/chiragsakhuja/lc3tools.git 

chapter 1

1. what is the intent of the book? there is no magic to computing as computers are deterministic systems; computers are not 
electronic geniues infact they are electronic idiots that do exactly as they are told 

2. a computer is a complex system made up of? systematically interconnected collection of very simple parts; it is
these simple parts that the book will introduce, explain and tie together to show how they make up a computer

3. what is the main goal of the book? by the time you are finished, you will be able to write programs using a language like C 
and be able to understand what is going on underneath the computer

4. how will we get there? you will see that all information processed by computers is in the form of 0's and 1's. therefors, all 
information processed by computers is therefore encoded as 0's and 1's. you will learn how to process such information

5. what is meant by all information processed by computers is encoded in 0's and 1's? each piece of information has a code point made up
of 1's and 0's

6. what is the lc-3? it is a microprocessor that has all the important characteristics of real microprocessors without being so complicated that it gets in the way of my understanding. so it is a piece of hardware except that it doesn't exist hence when you write code in assembly language or machine language for the lc3 processor, you will need a simulator to see what would happen in the registers and memory of a real lc3 during the execution of a program. all that exists is the ISA and microarchitecture which would implement the ISA

7. what do high level languages enable programmers to do? effectively develop complex sofware by abstracting away the details of the underlying hardware

8. what are the two recurring themes in the book? abstraction and not separating hardware and software layers of a computer

9. define abstraction? technique of establishing a simpler way to interact with a system

10. what is the premise that allows us to use abstraction effectively? allows not getting bogged down in the details of a system when everything is working fine; establishes a simpler way for a person to interect with a system

11. why is abstraction favoured? it is a productivity enhancer as i can deal with a situation at a higher level focusing on the essential aspects whilst keeping component ideas in the background

12. when the detail is not working fine, what must you be able to do? unabstract i.e. go from the abstraction back to its component parts

13. what are modern processors comprised of? transistors

14. what do transistors combine to form? logic gates 

15. what do opportunity do logic gates provide? the ability to think in 1's and 0's instead of varying voltages across the transistor wires

16. what does a combination of logic gates produce? a logic circuit - a further abstraction

17. when putting together logic circuits, should a designer be thinking of the internals of each individual gate or treat each gate as a component? treat each gate as a component as thinking of the internals of each gate would slow down the process of designing the logic circuit

18. how can the above thinking be applied to the design of an application program? each component should be thought of as an abstraction (thinking about the internals of each component would be detrimental to progress)

19. why should you be careful about letting abstractions be the deepest level of my understanding? because you will be at the mercy of the component parts working together; if one of them breaks then you won't be able to intervene to get the system up and running again

20. how will the material in the book be presented in terms of abstraction? the level of abstraction will be continually raised i.e. from individual transistors to gates to logic circuits to even larger abstractions

21. what should you keep in mind during your study and practice of computing? hardware and software are component parts of a computing system that work best when they are designed by people who take into account the capabilities and limitations of both

22. what allows engineers to be able to work on computing systems whilst ignoring the other side i.e. software people ignoring the hardware it runs on, and hardware people ignoring the software that will run on it? abstraction

23. why is being clueless about the underlying layers not advisable? because you will not be able to take advantage of the nuances of underlying layers when it is important to be able to do so

24. what do the authors suggest about the your study and practice of computing? that hardware and software are names for components of two parts of a computing system that work best when they are designed by people who take into account the capabilities and limitations of both

25. what path will the book set you on? the path to mastery of computer hardware and computer software which will make you more capable as a software engineer i.e. you will be able to come up with better solutions to computing problems when you have the capability of both at your finger tips

26. what is the definition of a computer? system consisting of software that directs and specifies the processing of information and the hardware that performs the actual processing of information in response to what the software specifies

27. what piece of hardware actually does the processing in a computer? CPU

28. when did the first computers show their face on planet earth? 1940s

29. what was the name of one the the first computers? ENIAC

30. what is marked about the computing devices of today and those of yesteryear? the weights have decreased tremendously, so has the power consumption all the whilst the computing power has increased by many orders of magnitude

31. If you want to see a picture of the ENIAC see figure1.1 (17,000 vacuum tubes, 2.4 metres high, 30 metres wide, 30 tons weight, 140kw to operate)

32. 40 years later (in the 1980s) and many computing companies later, the burroughs A series was created. you can see a picture of it in figure 1.2 (one of the dozen or so boards 18 inch boards that comprised the machine). each board contained 50 or more integrated circuit packages. weighed about 1 ton, and required approximately 25 kw to operate 

33. fast forward to today and the relative weights and power consumption of computing devices has decreased massively. the speed at which they process information has also increased enormously. they estimate that the computing power in a smartphone today is more than 4 million times the computing power of the ENIAC

34. what has brought about this increase in computational power? integrated circuit packages have seen phenomenal improvement e.g first  intel microprocessor in 1971 (intel 4004) contained 2300 transistors and operated at 106 KHz; one of the latest intel microprocessors Core i9-13900K contains a reported 25.9 billion transistors and and can operate at a frequency of 5.8 GHz. this factor of one million since 1971 in both the number of transistors and the frequency of microprocessors has had important implications

35. what are these implications? the fact that each operation can be performed in one millionth of the time it took in 1971 means that the processor can do one million things today in the time it took to do one thing in 1971. the fact that there are more than a million times as many transistors on a chip means we can do a lot more things at the same time today than we could do in 1971

36. what does this result in? we have computers today that seem able to understand languages people speak, recognise peoples faces which many see as the magic of artificial intelligence. however, these magical feats are are really due to fact that electronic idiots are able to run simple operations blazingly fast concurrently 

37. figure 1.6 shows a modern day microprocessor 

38. what two core ideas are presented at the end of the first chapter of the book? (a) all computers (fastest, slowest, cheapest, most expensive) are capable of computing exactly the same things if they are given enough time and memory (b) problems are expressed in human language but are solved by electrons moving due to voltage potentials inside the computer. this means a series of systematic transformations have to be made to convert a problem expressed in a human language in order for the electrons to do our bidding inside a computer

39. what has happened to these sequence of transformations over the last 70 years? they have been developed, refined and improved 

40. Before modern computers there were many kind of calculating machines. Give an example? analog machines 

41. how do analogue machines work? produce an answer by measuring some physical quantity such as distance or voltage

42. why are analogue machines difficult to work with? it is very hard to increase their accuracy 

43. why did digital machines (machines that perform computations by manipulating a fixed finite set of digits or letters) come to dominate computing by ? it is easy to increase their accuracy

44. what limitations did digital machines of yesteryear (adding machines or abacus) have? the mechanical or electro-mechanical devices could only perform a specific type of computation

45. why are computers referred to as universal computation devices? when you think of a new computation, you do not have to buy or design a new computer, you just give the same computer a new set of instructions to carry out the new computation

46. what does the study of computing involve? study the fundamentals of all computing along with learning what computation is and what can be computed

47. who is attributed with the idea of a universal computation device?

48. what did alan turing propose? that all computations could be carried out by a particular kind of machine - a turing machine. he gave a mathematical description of the machine but never built one

49. what was he more interested in? defining computation

50. Figure 1.7 shows what we call “black box” models of Turing machines that add and multiply. In each case, the operation to be performed is described in the box. The data elements on which to operate are shown as inputs to the box. The result of the operation is shown as output from the box. A black box model provides no information as to exactly how the operation is performed

51. what did turing propose? that every computation can be performed by some Turing machine. We call this Turing’s thesis. Although Turing’s thesis has never been proved, there does exist a lot of evidence to suggest it is true. 

52. what argument did he give to support his thesis? that one way to try to construct a machine more powerful than any particular 
turing machine was to make a machine U that could simulate all turing machines. you would simply describe to U the particular turing machine you wanted it to simulate, give U the input data, and U would compute the appropriate output. Turing then showed that there was, in fact, a Turing machine that could do this

53. what do computers and turing machines have in common? they are programmable hence can compute anything that can be computed

54. what implication do the immediately above points have? big or expensive computers cannot do anything that a small cheap computer can't; more money will buy you a faster computer but if you have a small inexpensive laptop then you already have a universal computation device

55. how do we get the electrons to do our bidding inside of a computer? work through the levels of transformation for a particular problem see figure 1.9

56. what is the first level of transformation? describe a problem in a natural language whilst avoiding ambiguity because the electronic idiot would not know what to do

57. what is the second level of transformation? convert the problem expressed in natural language into an algorithm thereby getting rid of the ambiguity inherent in natural language. an algorithm is a step by step procedure that is guaranteed to terminate

58. what are the three characteristics of an algorithm? finiteness (guaranteed to terminate), definteness (each step is precisely stated), and effective computability (can be carried out by a computer)

59. For every problem there are usually many diﬀerent algorithms for solving that problem. One algorithm may require the fewest steps. Another algorithm may allow some steps to be performed concurrently. A computer that allows more than one thing to be done at a time can often solve the problem in less time, even though it is likely that the total number of steps to be performed has increased.

60. what is the 3rd level of transformation? transform the algorithm into source code using a chosen programming language. programming languages are mechanical languages (lacking the ambiguity of natural languages) that were invented to specify instructions to a computer

61. what are the two kinds of programming language that exist? high level and low level

62. Give a definition for a high level language? independent of the computer from which they will execute on i.e. they are machine 
independent

63. give a definition for a low level langauge? tied to the computer on which the programs will execute i.e. assembly language

64. what is the fourth level of transformation? translating the source code into the ISA of the computer that will be used to
execute the program.

65. what does ISA specify? the interface between the source code and the hardware of the machine that will be used to execute the program

66. with regards to ISA, use the automobile and driver analogy to help better understand the concept? the human driver of a car represents the computer program represented as 0's and 1's in the computer; the car corresponds to the microprocessor hardware. the ISA of the automobile is the specification of everything the human needs to know in order to get the car to move and everything the car needs to know to carry out the tasks specified by the human driver

67. what do opcode and operand mean in relation to ISA of a computer? opcode refers to and operation the computer can perform whilst operand is an individual data value

68. ISA specifies the acceptable representations for operands, what are the operands called? data types (is a representation of an operand such that the computer can perform operations on that representation)

69. ISA specifies the mechanisms that the computer can use to figure out where operands are located. what are these mechanisms called? addressing modes

70. are the number of opcodes, operands and addressing modes unique to each ISA? yes

71. name a few ISAs in use today? x86 by intel (currently also developed by AMD an other companies), SPARC oracle, power (IBM), arm and thumb (ARM)

72. what is the name of the program that is used to translate source code into the ISA of the machine that will be responsible for 
executing it? compiler

73. what is the name of the program that translates assembly language of a computer to its ISA? assembler

74. what is the 5th level of transformation? the implementation of the ISA referred to as its micro-architecture which is about what goes on underneath the hood

75. using the automobile analogy from earlier where the automobile ISA describes what the driver needs to know as he/she sits inside
the automobile to make the automobile carry out the driver's wishes, how can you use it to describe the microarchitecture that implements the ISA? since microarchitecture is about what goes on underneath the hood, here all automobile models can be different depending on the cost  and performance tradeoffs made by the designer of the car

76. previously you were introduced to a number of ISAs (x86 intel, powerpc IBM and motorola, thumb arm), what can you say about the implementation of these ISAs? each has been implemented by many different microarchitectures e.g. x86 original implementation was in 1979 was the 8086 followed by 80286, 80386 amongst others in the 1980s. more recently in 2015, intel introduced skylake. each of these x86 microprocessors has its own microarchitecture

77. Each microarchitecture is an opportunity for computer designers to make different tradeoﬀs between the cost of the microprocessor, the performance that the microprocessor will provide, and the energy that is required to power the micro- processor. Computer design is always an exercise in tradeoﬀs, as the designer opts for higher (or lower) performance, more (or less) energy required, at greater (or lesser) cost.

78. what is the 6th level of transformation? logic circuits i.e. implement each component of the microarchitecture out of simple logic circuits

79. are there choices to be made in the 6th level? yes, the logic designer decides how to best make the tradeoffs between cost and performance so for example even for operation as simple as addition, there are several choices of logic circuits to perform the operation at differing speeds and energy costs.

80. what is 7th level of transformation? each logic circuits is implemented in accordance with the requirements of the particular  device technologies used e.g. CMOS circuits, NMOS circuits, and gallium arsenide circuits all differ from each other

81. what else does ISA specify (apart from opcodes, operands, and addressing modes)? number of unique locations that comprise the computers memory (address space) and the number of bits contained in each location (addressability)

82. At each level of transformation, there are choices as to how to proceed. Our handling of those choices determines the resulting cost and performance of our computer. this book describe each of these transformations. We show how tran- sistors combine to form logic circuits, how logic circuits combine to form the microarchitecture, and how the microarchitecture implements a particular ISA.
In our case, the ISA is the LC-3. We complete the process by going from the English-language description of a problem to a C or C++ program that solves the problem, and we show how that C or C++ program is translated (i.e., compiled) to the ISA of the LC-3.


chapter 2

1. how is a computer organised? as a system with several levels of transformation i.e. a problem stated in a natural language is
actually solved by electrons moving around inside the components of a computer

2. what do you call the tiny little devices inside computer that control the movement of said electrons by reacting to the presence or absence of voltages in electronic circuits?

3. could these tiny devices be designed to detect actual value of voltages instead of the presence or absence of them? yes

4. why is this not done? it would make the control and detection circuits more complex than they need to be; it is much easier to
detect whether or not a voltage exists at a point in a circuit than it is to measure exactly what that voltage is

5. how do we symbollicaly represent the presence of a voltage? 1

6. how do we symbollically represent the absence of a voltage? 0

7. what word is used to refer to each 0 and each 1? bit which is a shortened form binary digit

8. is it the case that computers differentiate the absolute presence of a voltage (i.e. 1) from the absolute absence of a voltage
(i.e. 0)? no, electronic circuits in the computer differentiate voltages very close to 0 from voltages very far from 0

9. how many things can we differentiate with one wire and what values are assigned to them? two things; one of them can be assigned
the value 0 and the other can be assigned a value of 1

10. in order to get useful work done by a computer, it is necessary to be able to distinguish a large number of distinct values and assign each of them a unique representation. what is done in order to achieve this? combining many wires, that is, many bits. for
example, if we use eight bits (corresponding to the voltage present on each of eight wires), we can represent one particular value as 01001110, and another value as 11100111. In fact, if we are limited to eight bits, we can differentiate at most only 256 (i.e.
2^8) different things 

11. what qualifies a representation of information as a data type? if there are operations in the computer that can operate on information encoded in that representation

12. Each instruction set architecture (ISA) has its own set of data types and its own set of instructions that can operate on those data types. 

13. what data types will be used in the book? 2’s complement integers for representing positive and negative integers that we wish to perform arithmetic on, and ASCII codes for representing characters that we wish to input to a computer via the keyboard or output from the computer via a monitor. 

14. describe the unsigned integer representation?

15. describe the signed integer number representation (how many bits are assigned to positive numbers and how many bits to negative 
numbers)? each takes half the amount of bits available

16. how do signed magnitude, 1's complement, and 2's complement encode information? see figure 2.1

17. The ﬁrst thought that usually comes to mind is: If a leading 0 signiﬁes a positive integer, how about letting a leading 1 signify a negative integer? The result is the signed-magnitude data type. A second thought (which was actually used on some early computers such as the Control Data Corpora- tion 6600) was the following: Let a negative number be represented by taking the representation of the positive number having the same magnitude, and “ﬂipping” all the bits. That is, if the original representation had a 0, replace it with a 1; if it originally had a 1, replace it with a 0. This data type is referred to in the computer engineering community as 1’s complement 

18. can a computer designer assign any bit pattern he wants to represent any integer he wants? yes

19. why would this not be a good idea? it would complicate matters when they try to build electronic circuits capable of adding the numbers. In fact, the signed-magnitude and 1’s complement data types both require unnecessarily cumbersome hardware to do addition. 

20. how are negative integers represented in 2's complement encoding? choice of representations for the negative integers is based on the wish to keep the logic circuits as simple as possible. all computers use the same basic mechanism to perform addition. It is called an arithmetic and logic unit, usually known by its acronym ALU. It performs addition by adding the binary bit patterns at its inputs, producing a bit pattern at its output that is the sum of the two input bit patterns. What is particularly relevant is that the binary ALU does not know (and does not care) what the two patterns it is adding represent. the 2’s complement data type speciﬁes the representation for each negative integer so that when the ALU adds it to the representation of the positive integer of the same magnitude, the result will be the representation for 0. Moreover, and actually more importantly, as we sequence through representations of say −15 to +15 (as seen in figure 2.1), the ALU is adding 00001 to each successive representation. using this system, any carries are always ignored

21. almost all computers use the same mechanism to perform addition, what is it called? ALU

22. if you know the bit representation of integer A, what is a short cut you can use to work out -A? flip all of the bits and add 1
see example21.png 

23. algorithm for binary to decimal conversion see binaryToDecimal.png example22.png

24. algorith for the conversion of decimal to binary see decimalToBinary

25. algorithm to convert fractional binary to fractional decimal see binaryFractionToDecimal.png 

26. operations on bits: addition and subtraction see example23 example24 example25

27. why is sign extension used? in order to be able to operate on representations of different lengths

28. what happens if the sum of two integers is not small enough to be represented by the available bits? overflow of the msb occurs

29. what must you watch out for when dealing with overflow in signed encoding e.g. 2's complement? addition of positive numbers that
result in overflow into msb which indicate the sign of the value. since the result is negative, this is easy to detect. likewise when adding two negative numbers and the msb overflows and becomes zero, detection would be easy since the result of the ALU operation would positive

30. what are the basic logical operations performed by the ALU? binary logical AND function that requires two source operands see example26 and example27, binary logical OR function that aalso requires two source operands see example28, unary logical function NOT which operates on only one source operand (also known as the complement operation), the binary logical XOR function that requires two source operands see example29 and example210

31. what do you call an m-bit pattern where each bit has a logical value (0 or 1) independent of the other bits? a bit vector. It is a convenient mechanism for identifying a property such that some of the bits identify the presence of the property and other bits identify the absence of the property.

32. There are many uses for bit vectors. The most common use is a bit mask. The bit mask is a bit vector, where our choice of 0 or 1 for each bit allows us to isolate the bits we are interested in focusing on and ignore the bits that don’t matter. Another common use of bit vectors involves managing a complex system made up of several units, each of which is individually and independently either busy or available. The system could be a manufacturing plant where each unit is a particular machine. Or the system could be a taxicab network where each unit is a particular taxicab. In both cases, it is important to identify which units are busy and which are available so that work can be properly assigned. Say we have m such units. We can keep track of these m units with a bit vector, where a bit is 1 if the unit is free and 0 if the unit is busy. see example211

33. There are many other representations of information that are used in computers. can you name two that are among the most useful?
ASCII and floating point representation

34. what encoding format does the lc-3 use to represent integers? 16 bit 2's complement where the msb (most significant bit) identifies whether the number is positive or negative and the rest of the bits represent the magnitude of the value. with 16 bits used this way, we can express integer values between -32768 and 32767, that is between -2^(15) and (2^15) - 1.

35. what do we say about the precision of the value and its range? precision of the value is 15 bits and the range is 2^(16)

36. what if you need to represent fractional decimals (such as avogadros constant) inside of a computer? can you do it with the 16 bit 2's complement encoding format used to represent integers? no, because the range of avogadros constant 10^(23) is far too great to be expressed in the range available 2^(15) - 1. on the other hand, the 15 bits of precision is overkill for expressing the four significant decimal digits (6022) 

37. what data type can solve this problem? floating point data type

38. how does it solve this problem? instead of using most of the bits to represent the precision of a value, the floating point data type allocates some of the bits to the range of values that can be expressed. the rest of the bits (except for the sign bit) are used for precision

39. most ISA's today specify more than one floating point data type; float and double

40. how many bits does the float data type have? 32 bits

41. how are the 32 bits of the float data apportioned? 1 bit (the msb) for the sign (positive or negative), 8 bits for the range (exponent), 23 bits for the precision (the fraction field) see figure23

42. what types of numbers are represented by floating data type? numbers expressed in scientific notation as follows

N = (-1)^S x 1.fraction x 2^(exponent-127), where the exponent is greater than or equal to 1 and less than or equal to 254

here S, fraction, and exponent are the binary numbers in the fields of figure 2.3

43. what form is the equation noted in point 42 in? normalised form

44. why is the equation referred to as being in normalised form? because the data represents a floating point number only if the 8 bit exponent is restricted to 254 unsigned integer values, 1 (00000001) through 254 (11111110)

45. how many values can be represented uniquely with 8 bits? 256 values

46. which two integer values does the exponent not represent normalised values? 00000000 and 11111111

47. what values does the sign bit take? it is a single binary digit; 0 for positive numbers and 1 for negative numbers. it evaluates to +1 if S=0 and -1 if S=1

48. what do the 23 fraction bits form a part of in normalised form? a 24 bit quantity represnted by 1.fraction as the normalised form demands exactly one none zero binary digit to the left of the binary point. this one non zero bit does not need to be explicitly stored in the 32 bit floating point format. This is infact how we get 24 bits of precision

49. how are the 8 exponent bits (the range) in the format? in excess code

50. why is it referred to as an excess code? because one can get the *real* exponent by treating the code as an unsigned integer and subtracting the bias/excess 

51. what is the range of the exponent field in normalised form? exponent ﬁeld gives us numbers as large as 2^(127) for an exponent field containing 254 and as small as 2^(-126) for an exponent ﬁeld containing 1. see example212, example213, example214

52. what numbers are represented by the floating point data type when the exponent contains all 1's i.e. 11111111? the notion of infinity

53. what must the fractional field contain when infinity is being represented? all zeros

54. how are positive infinity and negative infinity distinguished? positive infinity is distinguished by the sign bit being 0 and negative infinity is distinguished by the sign bit being set to 1

55. what is the smallest number that can be represented in normalised form? 1.00000000000000000000000 × 2^(-126)

56. what are subnormal numbers? numbers smaller than 2^(-126) but larger than 0. they are given this name because they cannot be represented in normalised form.

57. what is the largest subnormal number? 0.11111111111111111111111 × 2^(-126)

58. what is the smallest subnormal number? 0.00000000000000000000001 × 2^(-126)

59. what equation is used to represent subnormal numbers? (-1)^S x 0.fraction x 2^(-126)

60. how is the exponent field represented for subnormal numbers? 00000000. see example215

61. why are subnormal numbers used? allows very, very tiny numbers to be represented

62. wht are ASCII codes used for? transferring characters between main computer processing unit and the input and output devices

63. how many bits is the ASCII code made up of? 8 bits

64. how does ascii work? Each key on the keyboard is identiﬁed by its unique ASCII code. When you type a key on the keyboard, the corresponding eight-bit code is stored and made available to the computer (PC?). Most keys are associated with more than one code. For example, the ASCII code for the letter E is 01000101, and the ASCII code for the letter e is 01100101.


chapter 3

1. what are most computer processors today constructed out of? out of MOS transistors

2. what does MOS stand for? metal oxide semiconductor

3. what are the two type of MOS transistors? p-type and n-type

4. how many terminals does a transistor have? 3

5. what are they called? gate, source, and drain see figure32

6. how do n type transistors work? when the gate is supplied with 1.2v, the transistor acts like a piece of wire (short circuit); when supplied with 0 volts it acts like an open wire (open circuit) see figure32

7. what of the p type transistor - how does it work? When the gate is supplied with 0 volts, the P-type transistor acts (more or less) like a piece of wire, closing the circuit. When the gate is supplied with 1.2 volts, the P-type transistor acts like an open circuit. see figure33

8. because the p-type and n-type transistors work in a complimentary manner, what do we refer to circuits that contain both of them? CMOS circuits

9. what is the next step up from logic elements? logic gate ie transistor circuits (comprising of a combination of the logic elements) that implement the logical values of AND, OR, NOT

10. how is the NOT gate (inverter) constructed? from two MOS transistors; n-type and p-type see figure34

11. how does the inverter work? when supplied with 0 volts, P-type transistor acts like a short circuit and the N-type transistor acts like an open circuit. The output is, therefore, connected to 1.2 volts. On the other hand, if the input is supplied with 1.2 volts, the P-type transistor acts like an open circuit, but the N-type transistor acts like a short circuit. The output in this case is connected to ground (i.e., 0 volts). 

12. how is the NOR contructed? made out of 2 p-type and 2 n-type transistors see figure35

13. how does the NOR gate work? if A is supplied with 0 volts and B is supplied with 1.2 volts. In this case, the lower of the two P-type transistors produces an open circuit, and the output C is disconnected from the 1.2-volt power supply. However, the leftmost N-type transistor acts like a piece of wire, connecting the output C to 0 volts.

Note that if both A and B are supplied with 0 volts, the two P-type transistors conduct, and the output C is connected to 1.2 volts. Note further that there is no ambiguity here, since both N-type transistors act as open circuits, and so C is disconnected from ground.

If either A or B is supplied with 1.2 volts, the corresponding P-type transistor results in an open circuit. That is suﬃcient to break the connection from C to the 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type transistors is suﬃcient to cause that transistor to conduct, resulting in C being connected to ground (i.e., 0 volts).

14. how is the OR gate constructed? made out of 2 p-type and 2 n-type transistors (similar to NOR gate) but augmented by adding an inverter at its output, as shown in Figure 3.6a

15. how is the AND gate contructed? made out of 2 p-type and 2 n-type transistors along with an inverter see figure38

16. how does it work? if either A or B is supplied with 0 volts, there is a direct connection from C to the 1.2-volt power supply. The fact that C is at 1.2 volts means the N-type transistor whose gate is connected to C provides a path from D to ground. Therefore, if either A or B is supplied with 0 volts, the output D of the circuit of Figure 3.8 is 0 volts.

On the other hand, if both A and B are supplied with 1.2 volts, then both of their corresponding P-type transistors are open. However, their corresponding N-type transistors act like pieces of wire, providing a direct connection from C to ground. Because C is at ground, the rightmost P-type transistor acts like a closed circuit, forcing D to 1.2 volts.

17. The gates just discussed are very common in digital logic circuits and in digital computers. There are billions of inverters (NOT gates) in Intel’s Skylake microprocessor. As a convenience, we can represent each of these gates by stan- dard symbols, as shown in Figure 3.9. The bubble shown in the inverter, NAND, and NOR gates signiﬁes the complement (i.e., NOT) function.

18. Gates with More Than Two Inputs: the notion of AND, OR, NAND, and NOR gates extends to larger numbers of inputs. One could build a three-input AND gate or a four-input OR gate, for example. An n-input AND gate has an output value of 1 only if ALL n input variables have values of 1. If anyofthen inputs has a value of 0, the output of the n-input AND gate is 0. An n-input OR gate has an output value of 1 if ANY of the n input variables has a value of 1. That is, an n-input OR gate has an output value of 0 only if ALL n input variables have values of 0. see figure310

19. Now that we understand the workings of the basic logic gates, what is the next step? build some of the logic structures that are important components of the microarchitecture of a computer.

20. there are two types of logic structures, what are they called and what do? those that include storage of information and those that do not. those that do not store information are called decision elements or combinational logic structures. those that store information and make decisions as well are called sequential logic circuits

21. why are combinational logic structures called so? because their outputs are strictly dependent on the combination of input values that are being applied to the structure right now. Their outputs are not at all dependent on any past history of information that is stored internally, since no information can be stored internally in a combinational logic circuit.

22. name three decision elements? decoder, mux (multiplexer), full adder 

23. how does the decoder work? has the property that exactly one of its outputs is 1 and all the rest are 0s. The one output that is logically 1 is the output corresponding to the input pattern that it is expected to detect. In general, decoders have n inputs and 2^n outputs. We say the output line that detects the input pattern is asserted. That is, that output line has the value 1, rather than 0 as is the case for all the other output lines. see figure311. The decoder is useful in determining how to interpret a bit pattern. 

24. how does the mux work? multiplexer, more commonly referred to as a mux is used to select one of the inputs (A or B) and connect it to the output. The select signal (S in Figure 3.12) determines which input is connected to the output.

The mux of Figure 3.12 works as follows: Suppose S  =  0, as shown in Figure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out- put of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is whatever the input A is. That is, if A = 0, then the output of the leftmost AND gate is 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output of the rightmost AND gate is 0, it has no eﬀect on the OR gate. Consequently, the output at C is exactly the same as the output of the leftmost AND gate. The net result of all this is that if S = 0, the output C is identical to the input A.

On the other hand, if S = 1, it is B that is ANDed with 1, resulting in the output of the OR gate having the value of B. We say S selects the source of the mux (either A or B) to be routed through to the output C. In general, a mux consists of 2^n  inputs and n select lines. see figure313

25. how does the One-Bit Adder (a.k.a. a Full Adder) work? Figure 3.14 is a truth table that describes the result of binary addition on one column of bits within two n-bit operands. At each column, there are three values that must be added: one bit from each of the two operands A and B and the carry from the previous column. Note that if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e., Ci+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If all three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and a carry of 1.

Figure 3.15 shows a logic gate implementation of a one-bit adder. Note that each AND gate in Figure 3.15 produces an output 1 for exactly one of the eight input combinations. The output of the OR gate for Ci+1 must be 1 in exactly those cases where the corresponding input combinations in Figure 3.14 produce an output 1. Therefore, the inputs to the OR gate that generates Ci+1 are the outputs of the AND gates corresponding to those input combinations. Similarly, the inputs to the OR gate that generates Si are the outputs of the AND gates corresponding to the input combinations that require an output 1 for Si in the truth table of Figure 3.14. Note that since the input combination 000 does not result in an output 1 for either Ci+1 or Si, its corresponding AND gate is not an input to either of the two OR gates.

26. what does Figure 3.16 show? a circuit for adding two 4-bit binary numbers, using four of the one-bit adder circuits of Figure 3.15. Note that the carry out of column i is an input to the addition performed in column i + 1. 

27. if you wish to implement a logic circuit for adding two 16 bit numbers, how many one bit adders will you require? 16 one bit adders

28. describe the half adder? Note that the carry into the rightmost column in Figure 3.16 is 0. That is, in the rightmost circuit, S0 and C1 depend only on two inputs, A and B . Since that circuit depends on only two inputs, it has been referred to as a half adder. Since the other circuits depend on all three inputs, they are referred to as full adders. 

29. what is the name of the building block that is used to implement any collection of logic functions? a programmable logic array (PLA)

30. what does the PLA consist of? an array of AND gates (called an AND array) followed by an array of OR gates (called an OR array). The number of AND gates corresponds to the number of input combinations (rows) in the truth table. For n-input logic functions, we need a PLA with 2^n n-input AND gates. In Figure 3.17, we have 2^3 three-input AND gates, corresponding to three logical input variables. 

31. what does the number of OR gates correspond to in the PLA? the number of logic functions that we wish to implement i.e. the number of output columns in the truth table. The implementation algorithm is simply to connect the output of an AND gate to the input of an OR gate if the corresponding row of the truth table produces an output 1 for that output column. Hence the notion of programmable.

Figure 3.15 shows seven AND gates connected to two OR gates since our requirement was to implement two functions (sum and carry) of three input variables. Figure 3.17 shows a PLA that can implement any four functions of three variables by appropriately connecting AND gate outputs to OR gate inputs. That is, any function of three variables can be implemented by connecting the outputs of all AND gates corresponding to input combinations for which the output is 1 to inputs of one of the OR gates. Thus, we could implement the one-bit adder by programming the two OR gates in Figure 3.17 whose outputs are W and X by connecting or not connecting the outputs of the AND gates to the inputs of those two OR gates as speciﬁed by the two output columns of Figure 3.14.

32. describe the concept of logical completeness: set of gates {AND, OR, and NOT} (provided by the PLA) is logically complete because a barrel of AND gates, a barrel of OR gates, and a barrel of NOT gates are suﬃcient to build a logic circuit that carries out the speciﬁcation of any desired truth table. 

33. name 2 logic structures that do include the storage information? R-S latch and gated D latch

34. how does the R-S latch work? stores 1 bit of information, a 0 or a 1. can be implemented in many ways simplest one is shown in figure318 where two 2-input NAND gates are connected such that the output of each is connected to one of the inputs of the other; inputs S and R are normally held at a logic level 1.

35. where does the RS latch get its name? because setting the latch to store a 1 was referred to as setting the latch, and setting the latch to store a 0 was referred to as resetting the latch. Ergo, R-S

36. what is meant when we state that the R-S latch is in a quiescent state? when the latch is storing a value, either 0 or 1, and nothing is trying to change that value. 

37. what contributes to this quiescent state? the fact that both R and S are held at a logic level of 1

38. what letter in figure318 designates the value that is currently stored in the latch? letter a; also referred to as the output of the latch.

39. Consider ﬁrst the case where the value stored and therefore the output a is 1. Since that means the value A is 1 (and since we know the input R is 1 because we are in the quiescent state), the NAND gate’s output b must be 0. That, in turn, means B must be 0, which results in the output a equal to 1. As long as the inputs S and R remain 1, the state of the circuit will not change. That is, the R-S latch will continue to store the value 1 (the value of the output a).

40. If, on the other hand, we assume the output a is 0, then A must be 0, and the output b must be 1. That, in turn, results in B equal to 1, and combined with the input S equal to 1 (again due to quiescence), results in the output a equal to 0. Again, as long as the inputs S and R remain 1, the state of the circuit will not change. In this case, we say the R-S latch stores the value 0.

41. how can you set the latch to a 1 or a 0? you an set the latch to a 1 by temporarily setting s to 0 and having R remain at logic level 1; the latch can be set to 0 by temporarily setting R to 0 and having S remain at logic level 1

42. in order for the R-S latch to work properly, what must be done to R and S? must never be set to 0 at the same time

43. If we set S to 0 for a very brief period of time, this causes a to equal 1, which in turn causes A to equal 1. Since R is also 1, the output at b must be 0. This causes B to be 0, which in turn makes a equal to 1. If, after that very brief period of time, we now return S to 1, it does not aﬀect a. Why? Answer: Since B is also 0, and since only one input 0 to a NAND gate is enough to guarantee that the output of the NAND gate is 1, the latch will continue to store a 1 long after S returns to 1.

44. what can you do to clear the latch? we can clear the latch (set the latch to 0) by setting R to 0 for a very short period of time.

45. what happens if both S and R are set to 0 at the same time? outputs a and b would both be 1 and the final state of the latch would depend on the electrical properties of the transistors making up the gates and not on the logic being performed.

46. we should note that when a digital circuit is powered on, the latch can be in either of its two states, 0 or 1. It does not matter which state since we never use that information until after we have set it to 1 or 0.

47. how is the gated d latch implemented? It consists of the R-S latch of Figure 3.18, plus two additional NAND gates that allow the latch to be set to the value of D, but only when WE is asserted (i.e., when WE equals 1). WE stands for write enable. see figure319

48. what happens when WE is not asserted (i.e., when WE equals 0)? the outputs of S and R are both equal to a 1. Since S and R are inputs to the R-S latch, if they are kept at 1, the value stored in the latch remains unchanged

49. what happens when WE is momentarily set to 1? exactly one of the outputs S or R is set to 0, depending on the value of D.IfD equals 1, then S is set to 0. If D equals 0, then both inputs to the lower NAND gate are 1, resulting in R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R is set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according to whether D is 1 or 0. When WE returns to 0, S and R return to 1, and the value stored in the R-S latch persists.

50. what is memory made up of? a (usually large) number of locations, each uniquely identiﬁable and each having the ability to store a value. 

51. what do we call the unique identiﬁer associated with each memory location? address

52. what name do we give to the number of bits of information stored in each location? addressability

53. what do we call the total number of uniquely identiﬁable memory locations? address space

54. what do we mean by stating that a computing device has a 2GB memory? colloquially we say the computing device has 2 billion memory locations, however, the address space is actually 1024x1024x1024x2 which yields 2,147,483,648 locations

55. why 1024? because 1024 bytes make a kb and 1024 kb make a mb and 1024 mb make a GB

56. Actually, the number two billion is only an approximation, due to the way we specify memory locations. Since everything else in the computer is represented by sequences of 0s and 1s, it should not be surprising that memory locations are identiﬁed by binary addresses as well. With n bits of address, we can uniquely identify 2^n locations. Ten bits provide 1024 locations, which is approximately 1000. If we have 20 bits to represent each address, we have 2^(20) uniquely identiﬁable locations, which is approximately one million. With 30 bits, we have 2^(30) locations, which is approximately one billion. In the same way we use the preﬁxes “kilo” to represent 2^(10) (approximately 1000) and “mega” to represent 2^(20) (approximately one million), we use the preﬁx “giga” to represent 2^(30) (approximately one billion). Thus, 2 giga really corresponds to the number of uniquely iden- tiﬁable locations that can be speciﬁed with 31 address bits. We say the address space is 2^(31), which is exactly 2,147,483,648 locations, rather than 2,000,000,000, although we colloquially refer to it as two billion. 

57. A 2-gigabyte memory (written 2GB) is a memory consisting of 2,147,483,648 memory locations, each containing one byte (i.e., eight bits) of storage. 

58. why are most memories byte-adddressable? most computers got their start processing data where one character stroke on the keyboard corresponds to one 8-bit ASCII code. If the memory is byte-addressable, then each ASCII character occupies one location in memory. Uniquely identifying each byte of memory allows individual bytes of stored information to be changed easily.

59. Many computers that have been designed speciﬁcally to perform large scientiﬁc calculations are 64-bit addressable. This is due to the fact that numbers used in scientiﬁc calculations are often represented as 64-bit ﬂoating-point quantities. Since scientiﬁc calculations are likely to use numbers that require 64 bits to represent them, it is reasonable to design a memory for such a computer that stores one such number in each uniquely identiﬁable memory location.

60. Figure 3.20 illustrates a memory of size 2^2 by 3 bits. That is, the memory has an address space of four locations and an addressability of three bits. A memory of size 2^2 requires two bits to specify the address. We describe the two-bit address as A[1:0]. A memory of addressability three stores three bits of information in each memory location. We describe the three bits of data as D[2:0]. In both cases, our notation A[high:low] and D[high:low] reﬂects the fact that we have numbered the bits of address and data from right to left, in order, starting with the rightmost bit, which is numbered 0. The notation [high:low] means a sequence of high − low + 1 bits such that “high” is the bit number of the leftmost (or high) bit number in the sequence and “low” is the bit number of the rightmost (or low) bit number in the sequence.

Accesses of memory require decoding the address bits. Note that the address decoder takes as input the address bits A[1:0] and asserts exactly one of its four outputs, corresponding to the word line being addressed. In Figure 3.20, each row of the memory corresponds to a unique three-bit word, thus the term word line. Memory can be read by applying the address A[1:0], which asserts the word line to be read. Note that each bit of the memory is ANDed with its word line and then ORed with the corresponding bits of the other words. Since only one word line can be asserted at a time, this is eﬀectively a mux with the output of the decoder providing the select function to each bit line. Thus, the appropriate word is read at D[2:0].

Figure 3.21 shows the process of reading location 3. The code for 3 is 11. The address A[1:0]=11 is decoded, and the bottom word line is asserted. Note that the three other decoder outputs are not asserted. That is, they have the value 0. The value stored in location 3 is 101. These three bits are each ANDed with their word line producing the bits 101, which are supplied to the three output OR gates. Note that all other inputs to the OR gates are 0, since they have been produced by ANDing with their unasserted word lines. The result is that D[2:0] = 101. That is, the value stored in location 3 is output by the OR gates.

Memory can be written in a similar fashion. The address speciﬁed by A[1:0] is presented to the address decoder, resulting in the correct word line being asserted. With write enable (WE) also asserted, the three bits D[2:0] can be written into the three gated latches corresponding to that word line.

61. summarise how sequential logic circuits operate? they base their decisions not only on the input values now present but also on what has happened before; they contain storage elements that allow them to keep track of prior history information (what distinguishes from combinational logic circuits) see figure322 (Note the storage elements. Note also that the output can be dependent on both the inputs now and the values stored in the storage elements. The values stored in the storage elements reﬂect the history of what has happened before)

62. what machines are sequential logic circuits used to implement? finite state machines

63. can you give two examples; one for a combination decision element and the other for a sequential logic element? combination lock and sequential locks respectively see figure323

64. define what is meant by the state of a mechanism/system? snapshot of a system with all relevant items explicitly expressed.

65. how many elements does a finite state machine consist of? 5

66. can you state all 5? a finite number of states, a finite number of external inputs, a finite number of external outputs, 
an explicit specification of all state transitions, an explicit specification of what determines each external output value

67. what does the set of states of a system represent? all possible snapshots/configurations that the system can be in. Each state transition describes what it takes to get from one state to another. 

68. A state diagram is a convenient representation of a ﬁnite state machine. see figure326. The explicit speciﬁcations of all state transitions are shown by the arrows in the state diagram. The arrowhead on each arc speciﬁes which state the system is coming from and which state it is going to. We refer to the state the system is coming from as the current state, and the state it is going to as the next state. The combination lock has eight state transitions. Associated with each transition is the input that causes the transition from the current state to the next state.

A couple of things are worth noting. First, it is usually the case that from a current state there are multiple transitions to next states. The state transition that occurs depends on both the current state and the value of the external input. In short, the next state is determined by the combination of the current state and the current external input.

The output values of a system can also be determined by the combination of the current state and the value of the current external input. However, as is the case for the combination lock, where states A, B, and C specify the lock is “locked,” and state D speciﬁes the lock is “unlocked,” the output can also be determined solely by the current state of the system. In all the systems we will study in this book, the output values will be speciﬁed solely by the current state of the system.

69. what is a characteristic of asynchronous finite state machines? there is nothing that synchronises when each state transition occurs; a transition from a current state to a next state in our ﬁnite state machine happened when it happened

70. are computers synchronous or aynchronous systems? synchronous because state transitions take place one after another at identical fixed units of time.

71. what is the important common characteristic shared between synchronous and asynchronous finite state machines? they carry out work, one state transition at a time, moving closer to a goal

72. what controls the behaviour of a synchronous finite state machine as it transitions from a current state to its next state after an identical fixed interval of time? a clock circuit produces a signal, called THE clock, whose value alternates between 0 volts and some specified fixed voltage. In digital logic terms, the clock is a signal whose value alternates between 0 and 1. figure328 shows the value of the clock signal as a function of time. Each of the repeated sequence of identical intervals is referred to as a clock cycle. 

73. what is meant by a laptop running at a frequency of 2GHz? it can perform 2 billion pieces of work each second since 2 GHz means 2 billion clock cycles each second 

74. how many state transitions does a synchronous finite state machine make each clock cycle? one state transition

75. In electronic circuit implementations of a synchronous ﬁnite state machine, the transition from one state to the next occurs at the start of each clock cycle.

76. A Danger Sign: Figure 3.29 shows the danger sign; it contains 5 lights labelled 1 to 5. The synchronous ﬁnite state machine will be used to control it. The purpose of the synchronous finite state machine (a.k.a. a controller) is to direct the behaviour of the system i.e. the set of lights. The controller is equipped with a switch. When the switch is in the ON position, the controller directs the lights as follows: During one unit of time, all lights will be oﬀ. In the next unit of time, lights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on. Then all ﬁve lights will be on. Then the sequence repeats. The lights continue to sequence through these four states as long as the switch is on. If the switch is turned oﬀ, all the lights are turned oﬀ and remain oﬀ. Figure 3.30 is a state diagram for the synchronous ﬁnite state machine that controls the lights. There are four states, one for each of the four conditions corresponding to which lights are on. Note that the outputs (whether each light is on or oﬀ) are determined by the current state of the system. If the switch is on (input  =  1), the transition from each state to the next state happens at one-second intervals, causing the lights to ﬂash in the sequence described. If the switch is turned oﬀ (input = 0), the state always transitions to state A, the “all oﬀ” state.

77. Figure 3.31 is a block diagram of the speciﬁc sequential logic circuit we need to control the lights. Several things are important to note in this ﬁgure. First, the two external inputs: the switch and the clock. The switch determines whether the ﬁnite state machine will transition through the four states or whether it will transition to state A, where all lights are oﬀ. The other input (the clock) controls the transition from state A to B, B to C, C to D, and D to A by controlling the state of the storage elements. 

78. Second, there are two storage elements for storing state information. Since there are four states, and since each storage element can store one bit of informa- tion, the four states are identiﬁed by the contents of the two storage elements: A (00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage element 1 contains the low bit.

79. Third, combinational logic circuit 1 shows that the on/oﬀ behavior of the lights is controlled by the storage elements. That is, the input to the combinational logic circuit is from the two storage elements, that is, the current state of the ﬁnite state machine.

80. Finally, combinational logic circuit 2 shows that the transition from the cur- rent state to the next state depends on the two storage elements and the switch. If the switch is on, the output of combinational logic circuit 2 depends on the state of the two storage elements.

81. Figure 3.32 shows the logic that implements combinational logic circuits 1 and 2. Two sets of outputs are required for the controller to work properly: a set of external outputs for the lights and a set of internal outputs for the inputs to the two storage elements that keep track of the state. Light 5 is controlled by the output of the AND gate labeled V, since the only time light 5 is on is when the controller is in state 11. Lights 3 and 4 are controlled by the output of the OR gate labeled X, since there are two states in which those lights are on, those labeled 10 and 11.

82. Why are lights 1 and 2 controlled by the output of the OR gate labeled W? because input to OR gate W comes from the top NAND gate whose output is 1 when the storage elements are set to 01

83. Storage element 2 should be set to 1 for the next clock cycle if the next state is 10 or 11. This is true only if the switch is on and the current state is either 01 or 10. Therefore, the output signal that will make storage element 2 be 1 in the next clock cycle is the output of the OR gate labeled Y. if you look at the diagram you should be able to deduce why

84. Why is the next state of storage element 1 controlled by the output of the OR gate labeled Z? storage element 1 should be set to 1 in the next clock cycle if the switch is on and the current state is either 00 or 10

85. In order for the danger sign controller to work, the state transitions must occur once per second when the switch is on.

86. what is the problem with having gated D latches as the storage elements? when WE is asserted (i.e. the clock signal value is 1), the the output of OR gates Y and Z would immediately change the bits stored in the two gated D latches. This would produce new input values to the three AND gates that are input to OR gates Y and Z, producing new outputs that would be applied to the inputs of the gated latches, which would in turn change the bits stored in the gated latches, which would in turn mean new inputs to the three AND gates and new outputs of OR gates Y and Z. This would happen again and again, continually changing the bits stored in the two storage elements as long as the Write Enable signal to the gated D latches was asserted. The result: We have no idea what the state of the ﬁnite state machine would be for the next clock cycle. And, even in the current clock cycle, the state of the storage elements would change so fast that the ﬁve lights would behave erratically.

87. what is the problem is the gated D latch? We want the output of OR gates Y and Z to transition to the next state at the end of the current clock cycle and allow the current state to remain unchanged until then. That is, we do not want the input to the storage elements to take eﬀect until the end of the current clock cycle. We need storage elements that allow us to read the current state throughout the current clock cycle, and not write the next state values into the storage elements until the beginning of the next clock cycle.

88. how do flip flops work? they are storage elements that allow the reading of the current state throughout the current clock cycle and not write the next state into the storage elements until the beginning of the next clock cycle. reading must be allowed throughout the current clock cycle and writing must only occur at the end of the clock cycle

89. what is a flip flop made of? two gated D latches in a master/slave relationship; the write enable signal of the master is 1 when the clock is 0, and the write enable signal of the slave is 1 when the clock is 1. see figure333

90. how do flip flops work when used as storage elements? see Figure 3.34 (timing diagram for the master/slave ﬂip-ﬂop). A timing diagram shows time passing from left to right. Note that clock cycle n starts at the time labeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time labeled 4. at the start of each clock cycle, the output of the storage elements are the outputs of the two slave latches. These outputs (starting at time 1) are input to the AND gates, resulting in OR gates Y and Z producing the next state values for the storage elements (at time 2). The timing diagram shows the propagation delay of the combinational logic, that is, the time it takes for the combinational logic to produce outputs of OR gates Y and Z. Although OR gates Y and Z produce the Next State value sometime during half-cycle A, the write enable signal to the master latches is 0, so the next state cannot be written into the master latches.

91. what happens in half cycle b? At the start of half-cycle B (at time 3), the clock signal is 0, which means the write enable signal to the master latches is 1, and the master latches can be written. However, during the half-cycle B, the write enable to the slave latches is 0, so the slave latches cannot write the new information now stored in the master latches. At the start of clock cycle n+1 (at time 4), the write enable signal to the slave latches is 1, so the slave latches can store the next state value that was created by the combinational logic during clock cycle n. This becomes the current state for clock cycle n+1.

Since the write enable signal to the master latches is now 0, the state of the master latches cannot change. Thus, although the write enable signal to the slave latches is 1, those latches do not change because the master latches cannot change.

92. In short, the output of the slave latches contains the current state of the system for the duration of the clock cycle and produces the inputs to the six AND gates in the combinational logic circuits. Their state changes at the start of the clock cycle by storing the next state information created by the combinational logic during the previous cycle but does not change again during the clock cycle.

93. why don't they change? During half-cycle A, the master latches cannot change, so the slave latches continue to see the state information that is the current state for the new clock cycle. During half-cycle B, the slave latches cannot change because the clock signal is 0. Meanwhile, during half-cycle B, the master latches can store the next state information produced by the combinational logic, but they cannot write it into the slave latches until the start of the next clock cycle, when it becomes the state information for the next clock cycle.

94. what does the data path of a computer consist of? all of the logic structures that combine to process information in the core of the computer see Figure 3.35 you should be able to spot 5 muxes, adder (ALU with +), ALU, PC, IR, MAR, and MDR are all 16-bit registers that store 16 bits of information each, three 1-bit registers, N, Z, and P

The arrows in Figure 3.35 represent wires that transmit values from one struc- ture to another. Most of the arrows include a cross-hatch with a number next to it. The number represents the number of wires, corresponding to the number of bits being transmitted. 

95. what is a register made of? a set of n flip flops that collectively are used to store one n-bit value. i.e. gated D latches where one bit of information can be stored in one flip flop hence a 16 bit register is composed of 16 flip flops see figure336

96. why use flip flops to make registers instead of latches? because it is usually important to be able to both read the contents of a register throughout a clock cycle and also store a new value

chapter 4

1. what two things do you need to get a task done by a computer? a computer that will do the work and a computer program specifying the 
task that needs to be achieved

2. what is the name of the smallest piece of work specified in a computer program? an instruction

3. who proposed the fundamental model of computers?

4. what are the basic components of the model? 

5. in which of the components is the computer program contained in?

6. which of the components can hold the data that the program will operate on?

7. which component controls the order in which operations are carried out?

8. what does the 16GB when talking about memory refer to? the "16 giga" refers to the 2^(34) memory locations and the "byte" refers to the 
eight bits stored in each location the term is 16 giga because 16 is 2^(4) and giga is the term used to represent 2^(30) which is 
approximately one billion, 2^(4) x 2^(30) = 2^(34)

9. if you have k bits, how many unique items can you represent?

10. ergo, to uniquely identify 2^(34) memory locations, how many bits do you need? 

11. to read the contents of a memory location, where do we place the address of that location in memory? in the memory address register

12. where will the information stored in the address stored in MAR be placed? in the MDR register

13. what is the process of writing or storing a value in a memory location? location of the address to be written to is first stored in 
MAR and the data stored to be written is stored in the MDR register. computer memory is then interrogated using with the WE signal asserted 
such that the information stored in MDR is writtent to the address stored in MAR

14. what are the two characteristics of a memory location? its address and what is stored there

15. what component carries out the processing of information in a computer? 

16. what is processing unit in a computer comprised of? many sophisticated functional units each performing one particular operation 
(divide, square root etc) 
17. what is the name of simplest processing unit and the one thought of when discussing the von neumann model? ALU which is capable of 
performing basic arithmetic functions and basic logic operations 

18. what is the name given to the fixed size elements that the ALU processes?

19. what specifies the word length (depends on the intended use of the computer) of a computer? the ISA

20. what word length is specified by most ISAs today? 64 bits and 32 bits (though 32 bits is slowly being demised)

21. what range of word lengths can you expect to find being processed in inexpensive processors today? 8 bits to 16 bits

22. what is the name given to the most common form of storage used to temporarily cache results (e.g. close to the ALU) that will need to 
be accessed in the near future?

23. typically what is the size of a register equal to? values being processed by the ALU i.e. a word or any other processing unit 

24. what is the result of the importance of temporary storage for values that most modern computers will need shortly? many processors have 
access to an additional set of special purpose registers consisting of 128 bits to handle special needs

25. what component is used to keep track of both where we are within the process of executing a program and where we are in the process of 
executing an instruction?

26. what mechanism does the control unit use to keep track of which instruction is being executed? instruction register contains the 
instruction being executed

27. what mechanism does the control unit use to keep track of which instruction is to be executed next? program counter/(instruction pointer) 
contains the next instructions address

28. what is the keyboard data register (KBDR) used for? used for holding of ASCII codes of keys that have been struck

29. what is the keyboard status register (KBSR) used for? used to maintain status information about keys struck

30. what is the display data register (DDR) used for? for holding ASCII code of information to be displayed on a screen

31. what is the display status register (DSR) used for? for maintaining associated status of data displayed 

32. what does the control unit do? it comprises of all the structures needed to manage the processing that is carried out by the computer

33. what is the most important structure of the control unit? the finite state machine which directs all processing activity clock cycle by 
clock cycle

34. what is the central idea in the von neumann model of computer processing? program and data are both stored as sequences of bits in the 
computer's memory and the program is executed one instruction at a time under the direction of the control unit

35. what is the most basic unit of computer processing? an instruction

36. what is an instruction made up of? opcode (what the instruction does) and operand (what is does it to)

37. there are 3 types of opcodes (instructions), what are they? operates, data movement, and control

38. what do operate opcodes do? operate on data

39. what do data movement opcodes do? move data from processing unit to and from memory and to and from input/output devices

40. what do control opcodes do? they alter the sequential processing of instructions

41. for LC-3, which has 16 bits, and are numbered right to left from 0 to 15, how are they divided? bits 15:12 contain the opcode and bits 
11:0 are used to figure out where the operands are

42. how many opcodes does the LC-3 have? 15 opcodes; one is reserved for future use

43. how many operands does the ADD operate instruction have? 3 operands, two source operands the numbers to be added and one destination 
operand (where the sum is to be stored)

44. what does the ADD operate require? at least one of the two source operands must be stored in one of the 8 registers; the result should 
also be dumped in one of the 8 registers

45. since the LC-3 has 8 registers, how many bits are necessry to identify each register? 3

46. what is the four bit opcode (from 15:12) for the ADD operate in LC-3? 0001

47. in the ADD opcode, what do bits 11:9 specify? the register for storing the result 

48 in the ADD opcode, what do bits 8:6 specify? the register storing one of the two source operands

49. how many formats does the ADD instruction have? two

50. what is the difference between the two formats of the ADD instruction? the 1 or 0 stored in bit 5 and what they each mean

51. what does the 0 stored in bit 5 mean? that the second source operand is in the register specified by bits 2:0

52. what does the 1 stored in bit 5 mean? the second source operand is formed by sign-extending the integer in bits [4:0] to 16 bits.

53. what is the four bit opcode (from 15:12) for the AND operate in LC-3? 0101

54. what is the four bit opcode (from 15:12) for the LD operate in LC-3? 0010

55. what is load instruction used for? goes to a particular memory location, reads the value that is stored there and stores that value in 
one of the registers

56. how many operands does the load instruction require? two operands; are the value to be read from memory and the destination register 
that will contain that value after the instruction has completed processing

57. what is addressing mode? formulas that are used to calculate the address of the memory location to be read

58. what do bits 11:9 in the LD instruction represent? identifies the register that will contain the value read from memory after the 
instruction is executed. 

59. what do bits 8:0 in the LD instruction represent? used to calculate the address of the location to be read. 

60. what addressing mode is used by the load instruction? PC+offset where the address specified in bits 8:0 is sign extended to 16 bits 
and then added to the contents of the PC

61. what system is used to control instruction processing? the program counter

62. what is the entire sequence of steps needed to process an instruction called? instruction cycle

63. how many steps does the instruction cycle consist of? 6 sequential phases, each phase consisting of zero or more steps  

64. name the six phases of the instruction life cycle? fetch, decode, evaluate address, fetch operands, execute, store result

65. what does the fetch stage involve? fetches the next instruction from memory and loads it in the IR of the control unit

66. elaborate more on this fetch phase i.e. how it works from end to end? program made up of instructions; instructions comprised of bits
stored in memory per von neumann model; PC or instruction pointer contains address of next instruction to be executed; this address (in the 
PC) is loaded into the MAR (1); once in the MAR, the address is interrogated resulting in the instruction being loaded from the memory into 
the MDR (2); finally the IR is loaded with the instruction from the MDR (3)

67. what other task is accomplished in the fetch phase? incrementing of the PC/IP to point to the next instruction in memory; this
is done after the MAR has been loaded with the contents of the PC/IP

68. how many clock cycles does each of the 3 steps in the fetch cycle take? step 1 takes one clock cycle; step 2 takes one or many clock
cycles depending on how long it takes to access a computers memory; step 3 takes one clock cycle

69. in a modern digital computer, how long is a clock cycle? a very small fraction of a second

70. what does the decode phase of the instruction life cycle do? examines an instruction to figure out what the micro architecture is being
asked to do

71. how does the decode phase work? a decoder is used to identify which of the 16 opcodes is to be processed with the input being bits
15:12

72. what occurs in the evaluate address phase? CPU computes the address of the memory location that is needed to process the instruction; 
not all instructions access memory to load or store data

73. what occurs in the fetch operands phase? obtains the source operands needed to process the instruction

74. what is notable about the store result phase? in many computers, for some instructions such as ADD, the fetching of source operands,
performing the ADD in the ALU, and storing the result in the destination register happens all in a single clock cycle hence a separate 
store result phase is not needed

75. so far you have come across 2 types of instructions; operate (ADD, AND) and data movement (LOAD), what is the 3rd type of 
instruction called? control instruction

76. what do control instructions do? they change the sequence of instruction execution

77. if you want to change the sequence of instructions executed, what must be changed? the contents of the PC must change between the time
it is incremented (during the FETCH phase of one instruction) and the start of the FETCH phase of the next instruction

78. at which stage do the control instructions load the PC? execute

79. what is the most common control instruction? conditional branching

80. what is the 4 bit pattern that specifies conditional branching? 0000

81. what do bits 11:9 represent in the BR instruction? the condition to be tested

82. what do bits 8:0 contain in the BR instruction? the addressing mode bits that are used to form the address to be loaded into the PC if
the result of the previous instruction agrees with the test specified by bits 11:9.

83. what is the instruction cycle controlled by? a synchronous finite state machine

84. in the state diagram of the instruction cycle, how many clock cycles are used in the FETCH phase? 3

85. what happens in each of the 3 clock cycles of the FETCH phase? MAR <- PC and PC <- PC + 1; MDR <- MAR; IR <- MDR

86. when are registers loaded? at the end of a clock cycle if the corresponding control signal is asserted

87. in order for the contents of the PC to be loaded into the MAR, what must be asserted by the finite state machine? GatePC and LD.MAR

88. in order for the instruction to be loaded from MDR to the IR, what must the finite state machine assert? LD.IR and GateMDR

89. how many clock cycles does the DECODE phase (the 4th state) take? 1

90. what happens in this fourth state? the external input IR, in particular the opcode bits of the instruction 15:12, are read by the 
finite state machine to go to the appropriate next state for processing

91. what do we call instructions that change the flow of instruction processing? control instructions

92. what happens at the end of the conditional branch instruction? the PC contains one of two addresses; either the incremented PC that 
was loaded in state 1 or the new address computed from sign extending bits 8:0 of the BR instruction and adding it to the PC

93. what determines which address gets loaded into the PC? the test of the most recent result determined by testing against bit 11:9 of the
BR instruction

94. what controls the execution of a user program in a computer? the OS

95. are OS computer programs?

96. what can we do if we want to stop the infinite sequence of instruction cycles? stopping the clock which controls the transition from
state cycle to a different state cycle

97. how can this be achieved? setting the run latch of the clock circuit to 0 which makes the output of the clock circuit 0

98. how do machines set this run latch to 0? in some older machines, the HALT instruction is executed; in the LC-3 the TRAP instruction
(with opcode 1111 and an eight bit code called a trap vector x25) informs the OS that a program has finished executing and ergo the 
PC can stop executing instructions

99. What is misleading about the name program counter? The program counter does not maintain a count of any sort. 

100. Why is the name instruction pointer more insightful? The value stored in the program counter is the address of the next instruction 
to be processed. Hence the name ’Instruction Pointer’is more appropriate for it.

101. If a HALT instruction can clear the RUN latch, thereby stopping the instruction cycle, what instruction is needed to set the RUN 
latch, thereby reinitiating the instruction cycle? Once the RUN latch is cleared, the clock stops, so no instructions can be processed. 
Thus, no instruction can be used to set the RUN latch. In order to re-initiate the instruction cycle, an external input must be applied. 
This can be in the form of an interrupt signal or a front panel switch, for example.

chapter 5

1. what does the ISA specify? all the information about the computer that the sofware has to be aware of i.e. specifies everything in the 
computer that is available to a programmer when they write programs in the computer's own machine language.

2. what 3 things does ISA specify? memory organisation, register set, and instruction set

3. why do computers such as the LC-3 have/use registers? it takes more than one clock cycle to access an address in memory.
registers can be accessed in one clock cycle 

4. what do you call the number of bits stored in a register? a word

5. what is an instruction made up of? opcode (what the instruction is asking the computer to do)
and its operands (what the computer is expected to do it to)

6. what does the memory organisation specified by an ISA entail? address space (quantity of memory locations) and addressability (amount of 
bits in each memory location)

7. what does the register set specified by an ISA entail? these are temporary locations used to store data that a computer will need shortly.
these locations can be accessed quicker than memory (usually by a single clock cycle). each register is referred to as GPR. the number of
bits stored in each GPR is referred to as a word. also called a register file

8. what does the instruction set specified by an ISA entail? comprises of a set of opcodes, data types, and addressing modes.

7. what do addressing modes determine? where operands are located

8. what is a data type? a representation of an operand in 0's and 1's or you can also say it is a representation of information such that 
the ISA has opcodes that operate on that representation

9 what are the two addressing modes of the ADD instruction of the LC-3? register mode and immediate mode

10. how is the immediate addressing mode different to the register mode? one of the two operands is contained in bits 4:0 of the instruction

11. there are many ISA's out there, do they have the same number of opcodes? 

12. how many instructions does the LC-3 have? 15; each identified by a unique opcode

13. there are 3 different types of instructions, what are they? operate, data movement, and control

14. what does each of the 3 different types of instruction do? operate instructions process information; data movement instructions move
data between registers to memory and between registers/memory and input/output devices; control instructions change the sequence of
instructions that will be executed

15. how do opcodes interpret bit patterns of an operand? according to the data type it is designed to support

16. An operand can generally be found in one of three places, what are they? in memory, in a register, or in an instruction

17. if an operand is part of an instruction, what do we refer to it as? a literal or an immediate operand

18. where does the term 'literal' come from? from the fact that the bits of the instruction literally form the operand

19. where does the term immediate come from? from the fact that we can obtain the operand immediately from the instruction i.e. we do Not
have to look elsewhere for it

20. what are the 5 addressing modes supported by the LC-3? literal (or immediate), register, PC-relative, indirect, and base + offset

21. operate instructions use two addressing modes, what are they? register and immediate

22. how many addressing modes do data movement instructions use? 4 of 5 of the addressing modes

23. how many 3 single bit registers does the LC-3 have and what are they called? 3; they are referred to as N, Z, and P

24. what do N, Z, and P stand for? N stands for negative, Z stands for zero, and P stands for positive

25. how are these 3 single bit registers used? they are individually set (set to 1) or cleared (set to 0) each time one of the eight GPR's
is written into as a result of execution of one of the operate instructions or one of the load instructions 

26. how do the operate and load instructions work with respect to the GPRs? each operate instruction performs a computation and writes
the result into a GPR; each load instruction reads the contents of a memory location and writes the value found there into a general
purpose register

27. what is the set of the 3 single bit registers referred to as? condition codes 

28. why are the 3 single bit registers referred to as condition codes? because the condition of the 3 bits is used to change the sequence
of execution of instructions in a computer program

29. how many source operands does the NOT instruction operate on and what addressing mode does it use for both source and destination? one;
register mode

30. in the NOT instruction, what must bits 5:0 contain? all 1's

31. what 2 addressing modes can be used to specify the second source operand for the AND and ADD instructions? register mode for one of the
source operands and the destination operand; second operand is specified by either register mode or as an immediate operand

32. which bit determines which addressing mode will be used for the second operand in the AND and ADD instructions? bit 5

33. if bit 5 is 0 which addressing mode is used and what are bits 4:3 set to? register mode; set to 0

34. if bit 5 is 1 which addressing mode is used and what is done to bits 4:0? literal or immediate mode; they are sign extended to 16 bits

35. what must happen at the end of executing the AND and ADD Instruction (infact for load intructions as well)? the condition codes must be set

36. what implication does using immediate addressing mode present? since the literal operand must fit into bits 4:0 of the instruction, not
all 2 complement integers can be used as immediate operands (range of -32 through to 16)

37. can a register can be used as a source and also as a destination in the same instruction? yes

38. what does the LEA instruction do? loads a register specified by bits 11:9 with an address specified by adding the PC + sign extended
bits 8:0 of the instruction (the address is not accessed)

39. what do data movement instructions do? move information between GPRs and memory and between GPRs and input/output devices

40. what do you call the process of moving information from memory to a register? loading

41. what do you call the process of moving information from a register to a memory? storing

42. how many instructions does the LC-3 contain for moving information? 6

43. how many operands do data movement instructions require? two operands

44. name these operands? a source (data to be moved) and a destination

45. what do bits 8:0 contain for data movement instructions? address generation bits i.e. information used to compute the 16 bit address
of the second operand

46. in the case of LC-3, how many ways are there to interpret bits 8:0 for data movement instructions? 3 ways that are called addressing
modes

47. what speciﬁes how to interpret bits [8:0] in data movement instructions? the opcode

48. what two data movement instructions specify the PC-relative addressing mode? LD and ST

49. why is the PC-relative addressing mode named as such? because bits 8:0 of the instruction specify an offset relative to the PC

50. how is the memory address computed in pc-relative addressing mode? by sign extending bit 8:0 to 16 bits and adding the result to the incremented PC

51. in this case, what happens if the instruction is LD? the computed address (PC + offset) specifies the memory to be accessed. its
contents are loaded into register specified by bits 11:9 of the instruction

52. what if the instruction is ST? the contents of the register specified by bits 11:9 of the instruction is written into the memory 
location whose address is PC + offset

53. what happens after either the LD or ST instruction has been executed? the N, Z, P condition codes are set

54. what stands about the address generated in pc-relative addressing mode using bits 8:0? the range is limited to within 256 and -256
of the ld and st instruction 

55. which two instructions use indirect addressing mode? LDI and STI

56. how are the LDI and STI operand addresses formed? the same way LD and ST are formed i.e. sign extending bits 8:0 from the instruction and 
adding it to the incremented value of the PC

57. how is the addressing mode of the LDI and STI instructions different from that of the LD and ST instructions? instead of it being
the address of the operand to be loaded or stored, it is the address of the address of the operand to be loaded or stored

58. where in the instruction is the destination register for the LDI instruction and the source register for the STI instruction found? in
bits 11:9

59. what is the difference between the indirect mode and PC relative addressing modes in terms of the address of the operand in a computers
memory? for indirect addressing mode, the address of the operand can be anywhere in the computers memory and not just within the range 
provided by bits 8:0 of the instruction

60. which two instructions use the base + offset addressing mode? LDR and STR

61. how does the base + offset addressing mode work? address of the operand is obtained by adding a sign extended six bit offset (5:0)
to a base register specified by bits 8:6 

62. does the Base+oﬀset addressing mode also allow the address of the operand to be anywhere in 
the computer’s memory like indirect addressing mode?

63. what do control instructions do? they change the sequence of instructions to be executed otherwise the next instruction fetched would
always be after the current instruction finishes in the next sequential memory location

64. how many opcodes does the LC-3 have that allow sequential execution flow to be broken? 5

65. can you name them? conditional branch, unconditional jump, subroutine call, TRAP, and RTI (return from trap or interrupt)

66. what does the TRAP instruction (often called service call) do? allows programmer to get help from operating system to do things that
the typical programmer does not understand how to do e.g. getting information from input device into the cpu, sending information to output
device. it breaks the sequential execution of a user program to start a sequence of instructions in the OS 

67. summarise how the conditional branch (BR which is opcode 0000) works? when executed it decides based on a test whether to execute the 
next instruction in memory or to jump to another instruction in a different part of the memory that was allocated to the program by the
OS

68. what stage of instruction cycle does the BR instruction decide whether to load the PC with a new address? in the EXECUTE stage; if  
nothing occurs in this stage then the incremented PC will remain unchanged and the next instruction executed will be the next instruction
in the sequence, otherwise a new address will be loaded

69. what is the decision whether to do nothing to the incremented PC or whether to change it based on? it is based on the execution of
previous instruction in the program which are reflected in the condition codes (the conditional branch's execute phase results in either doing 
nothing or it loads the PC with the address of the instruction it wishes to execute next) 

70. what is the format of the conditional branch instruction? bits 15:12 display the opcode which in this case is all 0's; bits 11:9
display the condition codes N, Z, P respectively; bits 8:0 are sign extended to make 16 bits which are then added to the PC

71. in the LC-3, what instructions write into the GPR's and also set the condition codes? the operate instructions (ADD, AND, and NOT)
and the three load instructions (LD, LDI, LDR)

72. what can you summise about the contents of bits 11:9 in the BR instruction and its operation? the BR instruction uses the information
to determine whether to depart from the usual sequential execution of instructions that we get as a result of incrementing the PC during
the FETCH phase of each instruction

73. how does the above happen? during the EXECUTE phase of the BR instruction cycle, the processor examines the condition codes whose
associted bits in the instruction bits 11:9 are 1. if bit 11 is 1 then condition code N is examined; if bit 10 is 1 then condition code
Z is examined; similar action is taken for bit 9. if any of the bits 11:9 are 0 then the associated condition codes are not examined. if
any of the condition codes examined are set, then the PC is loaded with the address obtained in the EVALUTE ADDRESS phase. if none of the 
condition codes that are examined are set, the incremented PC is left unchanged

74. how is the address obtained during the EVALUATE ADDRESS phase generated? using PC-relative addressing mode

75. why is the uconditonal branch named so? because all bits 11:9 are set to 1 ergo all conditions are examined based on the
output of the previous instruction. ergo the instruction flow is changed unconditionally, independent ofthe data

76. there are two methods of loop control, name them: loop control with a counter and loop control with a sentinel

77. what is the difference between the two? a counter will control the number of times a loop executes by being decremented in
each cycle and then checked to see if it is zero (conditional branch instruction). if not the loop body is executed again; otherwise 
next instruction to be executed is after the last instruction of the loop body. on the other hand, sentinels are used when we do
not know beforehand how many iterations we will want to perform; each iteration will be based on processing a value and when the
sentinel is encountered the loop is broken

78. what is the limitation of the conditional branch instruction? the next instruction that it loads the PC with (computed from 
sign-extending bits 8:0 and adding them to the incremented PC) can be at most +256 or -256 locations from the currently executing branch 
instruction

79. what if you want to execute an instruction that is 2000 locations from the BR instruction? you cannot use the BR instruction
since you cannot fit 2000 into the 9 bit field 

80. what instruction provided by the LC-3 can do the above? the jmp instruction

81. how does it work? the jmp instruction loads the PC with the contents of the register specified by bits 8:6

82. how the TRAP instruction work? it changes the PC to a memory address that is part of the operating system so that the
OS will perform some task on behalf of the executing program. it is said to invoke an operating system service call

83. how are the bits arranged in the TRAP instruction? bits 7:0 form the trap vector; which is an eight bit code that
identifies the os service call 

84. what happens after an os finishes executing a service call? the PC is set to the address of the instruction following the 
TRAP instruction thereby allowing a program to resume execution

85. what are the basic components of the data path? the global bus, memory, the alu and the register file, the pc and the pcmux,
and the marmux

86. what does the LC-3 global bus consist of? 16 wires and associated electronics

87. what is the global bus used for? allows one structure to transfer upto 16 bits of information to another structure by making
the necessary electronic connections on the bus. exactly one value can be transferred on the bus at one time. each structure that 
supplies a signal to the bus has a triangle just behind its input arrow to the bus

88. what is this triangle called? the tri-state device

89. what is it used for? allows the computer's control logic to enable exactly one supplier to provide information
to the bus at one time. the structure wishing to obtain the value being supplied can do so by asserting its ld.x (load enable)
signal

90. how does memory work? it contains both instructions and data. it is accessed by loading the MAR with the address of the 
location to be accessed. control signals then read the contents of that memory location and the result of that read
is stored in the MDR

91. how does it work when it comes to storing? value to be stored in loaded into the MDR. the control signals then assert a WE signal
in order to store the value contained in the MDR in the memory location specified by the MAR

92. how does the ALU work? as the processing element, it has two inputs, source 1 from a register and source 2 from either
a register or the sign-extended immediate value provided by the instruction

93. what happens at the start of each instruction cycle? the PC supplies the MAR via the global bus the address of the instruction to be
fetched 

94. the PC is supplied via the three-to-one PCMUX. how is this three-to-one MUX configured? the rightmost input to the PCMUX is used to
increment and write into the PC by one; when a control signal such as BR is executed and the sign-extended bits 8:0 are added to the incremented
PC (addition takes place in a separate adder not the ALU), the output of this adder is the middle input to the PCMUX; the third input to the 
PCMUX is obtained from the global bus

95. what does the MARMUX component do? it controls which source out of two will supply the MAR with the appropriate address during
the execution of a load, a store, or a TRAP instruction. The right input to the MARMUX is obtained by adding either the incremented PC or
a base register to zero or a literal value supplied by the IR. Whether the PC or a base register and what literal value depends on which 
opcode is being processed. The control signal ADDR1MUX speciﬁes the PC or base register. The control signal ADDR2MUX speciﬁes which of four 
values is to be added.

chapter 6

1. what two things will you learn about in this chapter? develop a methodology for constructing
programs to solve problems and develop a methodology for fixing said programs under the 
likely condition that i did not get things right the first time

2. what is structured programming? methodology developed in the 60s to dramatically improve
the ability of average programmers to take a complex description of a problem and systematically
decompose it into smaller manageable units so that they could ultimately write a program 
that executed correctly

3. what is the methodology also called? systematic decomposition

4. define the systematic decomposition/ stepwise refinement process? process of taking A
unit of work and breaking it into smaller units of work such that the collection of smaller
units carries out the same task as the larger unit.

5. what is the essential idea behind systematic decomposition or stepwise refinement?
replace a larger unit of work with a construct that correctly decomposes it

6. how many constructs are there for doing this? 3

7. what are they called? sequential, conditional, and iterative

chapter 7

1. mechanical languages are generally partitioned into two classes. what are they? high level and low level

2. describe high level languaages? are more user friendly, almost resemble statements in natural languge such as english

3. what must happen before a program that is written in a high-level language can be executed? it must be translated into a program
in the ISA of the computer on which it is expected to execute

4. is it often the case that each statement in a high level language specifies several instructions in the ISA of a computer? yes

5. what do you call the small step up from the ISA of a machine? the ISAs assembly language

6. what is assembly language? a low level language which cannot be confused with a statement in english language. it is ISA dependent
i.e. each ISA has only one assembly language 

7. does each assembly language instruction usually specify a single instruction in the ISA? yes

8. what is the purpose of assembly language? to make the programming process more user friendly than programming in machine language
i.e. in the ISA of the computer you are working on while providing the programmer with detailed control over the instructions the
computer can execute

9. what does assembly language do that help the program avoid minutae? let us use mnemonic devices for opcodes and give meaningful 
symbolic names to memory locations (they are called symbolic addresses)

10. what do you lose when you use a high level language like C? whilst it is user friendly you relinquish control over which ISA instruction
are to be used to carry out the work specified by the high level statement

11. what are pseudo ops (assembler directives)? message from the programmer to the translation program to help in the translation 
process (translating a program represented by a string of characters in assembly language to machine language)

12. what is the translation program that is used to translate assembly programs to machine language? assembler

13. what is the translation process called? assembly

14. instead of an instruction being 16 0's and 1's as is the case of the LC-3 ISA, an instruction in assembly language consists of 
four parts. what are they called? label, opcode, operands, comment

15. which two are optional? label and comment

16. in LC-3 assembly language, why are symbolic names called labels are assigned to memory locations? so that we do not have to remember
explicit 16 bit addresses

17. for literal values in assembly language that are used as operands in assembly language, how will know a number is a decimal,
hex, or a binary? decimal numbers will be preceeded by #, binary by b, and hex by X

18. what are the two reasons for explicitly referring to a memory location using a label? the location is a target of a branch 
instruction and the location contains a value that is loaded or stored

19. how many pseudo-ops does the LC-3 assembly language contain? 5

20 what are they? .ORIG, .FILL, .BLKW, .STRINGZ, and .END.

21. what does the .ORIG pseudo-op do? it tells the assembler where in memory to place the LC-3 program i.e. first instruction is placed
in the memory addressed specified, whilst the rest of the instructions in the program are placed in subsequent sequential locations.

22. what does the .FILL pseudo-op do? tells the assembler to set aside the next location in the program and initialise it with the value
of the operand. the value can either be a number or a label

23. what does the .BLKW pseudo op do? tells the assembler to set aside some number of sequential memory locations in the program. The
actual number is the operand of the .BLKW pseudo op

24. what does the .STRINGZ pseudo op do? tells the assembler to initialise a sequence of n+1 memory locations. The argument is a 
sequence of n characters inside double quotation marks. first n words of memory are initialised with zero extended ascii codes of the
corresponding characters in the string. final word of memory is initialised to 0

25. what does the .END pseudo op do? tells the assembler it has reached the end of the program and need not look at anything after it

26. how does the assembly process unfold? it is done in two complete passes (from beginning to .END) through the entire assembly language
program. the objective of the first pass is to identify the actual binary addresses corresponding to the symbolic names (or labels).
this is set of correspondences is known as the symbol table. in pass 2, we translate the individual assembly language instructions
into their corresponding machine language instructions

27. why are labels used in assembly language? to refer to memory locations either because it is a target of a branch instruction or because
it contains data that must be loaded or stored

28. how does the lc-3 assembler keep track of the location assigned to  each instruction? by meaans of the location counter. it is 
initialised to the address specified in .ORIG

29. when a computer begins execution of a program, what is the entity being executed called? executable image

30. what is the executable image created from? from modules created independently by individual pogrammers where each module is 
translated separately into an object file

31. are all modules written by users? no, some are supplied as library routines by the operating system

32. what does each object file consist of? instructions in the ISA of the computer being used along with its associated data

33. what is normally the final step? combining (linking) all of the object modules together into one executable image

34. what happens during the execution of the program? the fetch, decode... instruction cycle is applied to instructions in the executable
image

35. what does the pseudo-op .EXTERNAL do? identifies the symbolic name of an address that is not known at the time a program is 
assembled. it sends a message to the LC-3 assembler that the absence of a symbolic label is not an error in the program i.e. the 
symbolic name is a label in another module that will be translated independently. it allows references by one module to symbolic 
references in another module without a problem

chapter 8

1. how many instructions does the call/return mechanism consist of? two instructions

2. what is the first instruction? JSR(R) is in the caller program

3. what does the JSR(R) instruction do? it loads the PC with the starting address of the subroutine and it loads R7 with the address
immediately after the address of the JSR(R) instruction. the address immediately after the address of the JSR(R) instruction is
the address to come back to after executing the subroutine

4. what name is given to the address we come back to? return linkage

5. what is the second instruction? JMP R7; which happens to be the laast instruction in the subroutine

6. what does this instruction do? it loads the PC with the contents of R7, the address just after the address of the JSR instruction
thereby completing the round trip flow of control from the caller to the callee and back

7. the LC-3 specifies one control instruction for calling subroutines; what is it called? JSR

8. how many addressing modes does the JSR(R) control instruction have for computing the starting address of a subroutine? 2

9. what are the addressing modes? PC-relative addressing and base register addressing

10. what are the two things that the JSR(R) instruction does? loads the PC hence overwriting the incremented PC that was loaded 
during the fetch phase of the JSR(R) instruction and then it saves the return address in R7

11. what is the return address that is saved in R7? it is the incremented PC which is the address of the instruction following the
JSR(R) instruction

12. how many parts does the JSR(R) instruction consist of? 3 parts

13. what are the 3 parts? bits 15:12 specify the opcode; bit 11 specifies what addressing mode will be used (1 for PC-relative); bits
10:0 are the address evaluation bits

14. how does the JSR instruction compute the target address of a subroutine? by sign-extending the 11 bit offset (bits 10:0) of the
instruction to 16 bits and adding it to the incremented PC

15. What other control instruction shares a addressing mode? the BR instruction except in this instruction 9 bits are sign extended 
instead of 11 bits

16. how does the JSRR instruction differ from the JSR instruction? the addressing mode used;  it obtains the starting address of A
subroutine in exactly the same way that the JMP instruction  does i.e. bits 8:6 identify the base register that contains the address 
to be loaded into the PC

17. what problems do  subroutines pose when it comes to registers? they overwrite the contents of a register and therefore if the 
contents that were overwritten are needed by the caller program then you have a major problem at hand

18. how is the above problem aavoided? by having either the subroutine save the contents of the registers it will overwrite and restore
them before it finishes executing or have the caller program save the contents of the registers it is using before it invokes a 
subroutine and restore the contents when the subroutine has finished executing

19. of the two options, which is more efficient? having the callee program (the subroutine) save the contents of the registers is
more efficient because it knows which registers it will overwrite. The caller program has no way of knowing this in advance

20. what is the technique called? callee save

21. is the caller program or the callee program responsible for saving and restoring the contents of R7? the caller is responsible 
since it alone is capable of overwriting the value in R7 after the subroutine returns

22. what do we call the above process? caller save

23. what are some of the uses of the call/return mechanism expanded on above? ability of a user program to call library routines
that are part of the OS

24. can the stack (abstract data type), be implemented in many different ways? yes

25. what makes stack special? LIFO i.e. specification of how it is to be accessed that is the last thing that you stored in a stack 
is the first thing that you remove from it

26. what is the definition of an abstract data type? a storage mechanism that is defined by the operations performed on it and not
by the specific manner in which it is implemented

27. how is a stack implemented in computer memory? consists of a sequence of memory locations along with a mechanism called a stack
pointer which keeps track of the top of the stack

28. how are values inserted into the stack stored? they are stored in memory locations having decreasing addresses i.e. the stack 
grows towards zero.

30. when values are pushed and popped to and from the stack implemented in sequential memory locations, does the data already stored
on the stack physically move? no

31. in the LC-3, what is process of pushing a value onto a stack? first load the value into R0, then decrement the value in R6 
(which contains the memory address of the stack pointer) 

32. in the LC-3, what is the process of popping a value from the stack? the value is read and the stack pointer is incremented

33. what is the fancy name given to the rules that the stack follows? stack protocol

34. what does attempting to pop items that have not been previously pushed result in? underflow

35. What happens when we run out of available space and we try to push a value onto the stack? result in an overflow
