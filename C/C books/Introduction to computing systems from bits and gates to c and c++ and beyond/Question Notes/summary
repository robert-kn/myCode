what should you know about fundamentals? they do not change very often

what will mastery of the fundamentals yield? there is no limit to how high I will soar provided I continue to put in the
work 

what is this book about? mastering the fundamentals of computing

what is the conventional wisdom when it comes to teaching computer science? start  with a high level programming language
which leads to the memorisation of technical details without an understanding of basic underpinnings

how does the book approach mastering computing fundamentals? a bottom up approach where it continually builds on what is 
covered (scaffolding)

what is technique is widely pervasive when it comes to teaching students any subject? information hiding approach

why is it used? because it is a useful productivity enhancer and allows the student to hit the ground running

what are its drawbacks? information hiding gets in the way of understanding which leads to problems when students have to
think from first principles about for example why a particular error is happening (understanding not memorising)

information hiding is only useful once one has an understanding of the fundamentals. bottom up approach is best for learning
in order to acquire an understanding

does that mean that the top down design approach is flawed?

what was their  concern about including C++ in their instructional approach? many of the languages features are too far 
abstracted from the underlying layers to make for an easy fit to their instructional approach. Additionally C++ is a vast 
language that would have necessiated more pages to be added to the book. 

why do they still use C in the book? serves as the defacto development language for systems aand hardware oriented projects

what are the two major segments that the book can be broken down into? (a) the underlying structure of a computer (b) programming
in a high level language

why does the C programming language fit very nicely with the bottom up approach? its low level nature allows students to see
clearly the connection between software and the underlying hardware 

why lc-3 ISA and not ARM or RISCV? because commercial ISAs have no place in an introductory course but still have to be
understood in order to be used effectively. They wanted an ISA that was clean with no special cases to deal with, with as few
opcodes as necessary so that you the student spends all your time on the fundamental concepts of the course and very little 
time on the nuaances of the instruction set

the lc-3 instruction set with only 15 four-bit opcodes, is small enough that the students can absorb the ISA without struggling
too much

what are the stand out observations of the approach taken in the book?

(a) understanding and not memorising: bottom up learning approach leads to less memorisation of seemingly arbitrary rules
which is prevalent in traditional programming courses. by the time a topic is taught, you will have an understanding of how
the topic is implemented at the levels below it. this approach is good for design courses where understanding of and insights
gained from fundamentals are essential to making the required design trade offs.
(b) you get to debug your own programs
(c) preparation for the future: cutting through protective layers; as a professional, if you are ignorant of what is going
on inside computers you will likely discover the hard way that the effectiveness of your solutions is impacted adversely by
things other than the actual programs you write. serious programmers will write more efficient code if they understand what
is going on beyond the statements in their high level language. high level programming language courses where the compiler 
protects the student from everything ugly underneath does not serve most engineering students well
(d) rippling effects through the curriculum: the material taught in the book has a rippling effect on what can be taught in 
subsequent courses. subsequent programming courses cannot only assume the students know the syntax of C/C++ but also understand
how it relates to the underlying architecture. ergo, the focus can be on problem solving and more sophisticated data structures

how to install the simulator debugger written by the authors for the lc-3 ISA:

download link for the material can be found here: https://highered.mheducation.com/sites/1260150534/student_view0/lc-3_simulator.html

LC3Tools is a cross-platform set of tools to build code for and simulate the LC-3 system described in the book.

The latest version of LC3Tools for all platforms can be found at https://github.com/chiragsakhuja/lc3tools/releases

Windows Users: lc3tools-setup-VERSION.exe
macOS Users: LC3Tools-VERSION.dmg
Linux Users: lc3tools-VERSION-x86_64.AppImage

There are several other resources, including more documentation and instructions on the command line tools, at https://github.com/chiragsakhuja/lc3tools.git 

chapter 1

1. what is the intent of the book? there is no magic to computing as computers are deterministic systems; computers are not 
electronic geniues infact they are electronic idiots that do exactly as they are told 

2. a computer is a complex system made up of? systematically interconnected collection of very simple parts; it is
these simple parts that the book will introduce, explain and tie together to show how they make up a computer

3. what is the main goal of the book? by the time you are finished, you will be able to write programs using a language like C 
and be able to understand what is going on underneath the computer

4. how will we get there? you will see that all information processed by computers is in the form of 0's and 1's. therefors, all 
information processed by computers is therefore encoded as 0's and 1's. you will learn how to process such information

5. what is meant by all information processed by computers is encoded in 0's and 1's? each piece of information has a code point made up
of 1's and 0's

6. what is the lc-3? it is a microprocessor that has all the important characteristics of real microprocessors without being so complicated that it gets in the way of my understanding. so it is a piece of hardware except that it doesn't exist hence when you write code in assembly language or machine language for the lc3 processor, you will need a simulator to see what would happen in the registers and memory of a real lc3 during the execution of a program. all that exists is the ISA and microarchitecture which would implement the ISA

7. what do high level languages enable programmers to do? effectively develop complex sofware by abstracting away the details of the underlying hardware

8. what are the two recurring themes in the book? abstraction and not separating hardware and software layers of a computer

9. define abstraction? technique of establishing a simpler way to interact with a system

10. what is the premise that allows us to use abstraction effectively? allows not getting bogged down in the details of a system when everything is working fine; establishes a simpler way for a person to interect with a system

11. why is abstraction favoured? it is a productivity enhancer as i can deal with a situation at a higher level focusing on the essential aspects whilst keeping component ideas in the background

12. when the detail is not working fine, what must you be able to do? unabstract i.e. go from the abstraction back to its component parts

13. what are modern processors comprised of? transistors

14. what do transistors combine to form? logic gates 

15. what do opportunity do logic gates provide? the ability to think in 1's and 0's instead of varying voltages across the transistor wires

16. what does a combination of logic gates produce? a logic circuit - a further abstraction

17. when putting together logic circuits, should a designer be thinking of the internals of each individual gate or treat each gate as a component? treat each gate as a component as thinking of the internals of each gate would slow down the process of designing the logic circuit

18. how can the above thinking be applied to the design of an application program? each component should be thought of as an abstraction (thinking about the internals of each component would be detrimental to progress)

19. why should you be careful about letting abstractions be the deepest level of my understanding? because you will be at the mercy of the component parts working together; if one of them breaks then you won't be able to intervene to get the system up and running again

20. how will the material in the book be presented in terms of abstraction? the level of abstraction will be continually raised i.e. from individual transistors to gates to logic circuits to even larger abstractions

21. what should you keep in mind during your study and practice of computing? hardware and software are component parts of a computing system that work best when they are designed by people who take into account the capabilities and limitations of both

22. what allows engineers to be able to work on computing systems whilst ignoring the other side i.e. software people ignoring the hardware it runs on, and hardware people ignoring the software that will run on it? abstraction

23. why is being clueless about the underlying layers not advisable? because you will not be able to take advantage of the nuances of underlying layers when it is important to be able to do so

24. what do the authors suggest about the your study and practice of computing? that hardware and software are names for components of two parts of a computing system that work best when they are designed by people who take into account the capabilities and limitations of both

25. what path will the book set you on? the path to mastery of computer hardware and computer software which will make you more capable as a software engineer i.e. you will be able to come up with better solutions to computing problems when you have the capability of both at your finger tips

26. what is the definition of a computer? system consisting of software that directs and specifies the processing of information and the hardware that performs the actual processing of information in response to what the software specifies

27. what piece of hardware actually does the processing in a computer? CPU

28. when did the first computers show their face on planet earth? 1940s

29. what was the name of one the the first computers? ENIAC

30. what is marked about the computing devices of today and those of yesteryear? the weights have decreased tremendously, so has the power consumption all the whilst the computing power has increased by many orders of magnitude

31. If you want to see a picture of the ENIAC see figure1.1 (17,000 vacuum tubes, 2.4 metres high, 30 metres wide, 30 tons weight, 140kw to operate)

32. 40 years later (in the 1980s) and many computing companies later, the burroughs A series was created. you can see a picture of it in figure 1.2 (one of the dozen or so boards 18 inch boards that comprised the machine). each board contained 50 or more integrated circuit packages. weighed about 1 ton, and required approximately 25 kw to operate 

33. fast forward to today and the relative weights and power consumption of computing devices has decreased massively. the speed at which they process information has also increased enormously. they estimate that the computing power in a smartphone today is more than 4 million times the computing power of the ENIAC

34. what has brought about this increase in computational power? integrated circuit packages have seen phenomenal improvement e.g first  intel microprocessor in 1971 (intel 4004) contained 2300 transistors and operated at 106 KHz; one of the latest intel microprocessors Core i9-13900K contains a reported 25.9 billion transistors and and can operate at a frequency of 5.8 GHz. this factor of one million since 1971 in both the number of transistors and the frequency of microprocessors has had important implications

35. what are these implications? the fact that each operation can be performed in one millionth of the time it took in 1971 means that the processor can do one million things today in the time it took to do one thing in 1971. the fact that there are more than a million times as many transistors on a chip means we can do a lot more things at the same time today than we could do in 1971

36. what does this result in? we have computers today that seem able to understand languages people speak, recognise peoples faces which many see as the magic of artificial intelligence. however, these magical feats are are really due to fact that electronic idiots are able to run simple operations blazingly fast concurrently 

37. figure 1.6 shows a modern day microprocessor 

38. what two core ideas are presented at the end of the first chapter of the book? (a) all computers (fastest, slowest, cheapest, most expensive) are capable of computing exactly the same things if they are given enough time and memory (b) problems are expressed in human language but are solved by electrons moving due to voltage potentials inside the computer. this means a series of systematic transformations have to be made to convert a problem expressed in a human language in order for the electrons to do our bidding inside a computer

39. what has happened to these sequence of transformations over the last 70 years? they have been developed, refined and improved 

40. Before modern computers there were many kind of calculating machines. Give an example? analog machines 

41. how do analogue machines work? produce an answer by measuring some physical quantity such as distance or voltage

42. why are analogue machines difficult to work with? it is very hard to increase their accuracy 

43. why did digital machines (machines that perform computations by manipulating a fixed finite set of digits or letters) come to dominate computing by ? it is easy to increase their accuracy

44. what limitations did digital machines of yesteryear (adding machines or abacus) have? the mechanical or electro-mechanical devices could only perform a specific type of computation

45. why are computers referred to as universal computation devices? when you think of a new computation, you do not have to buy or design a new computer, you just give the same computer a new set of instructions to carry out the new computation

46. what does the study of computing involve? study the fundamentals of all computing along with learning what computation is and what can be computed

47. who is attributed with the idea of a universal computation device?

48. what did alan turing propose? that all computations could be carried out by a particular kind of machine - a turing machine. he gave a mathematical description of the machine but never built one

49. what was he more interested in? defining computation

50. Figure 1.7 shows what we call “black box” models of Turing machines that add and multiply. In each case, the operation to be performed is described in the box. The data elements on which to operate are shown as inputs to the box. The result of the operation is shown as output from the box. A black box model provides no information as to exactly how the operation is performed

51. what did turing propose? that every computation can be performed by some Turing machine. We call this Turing’s thesis. Although Turing’s thesis has never been proved, there does exist a lot of evidence to suggest it is true. 

52. what argument did he give to support his thesis? that one way to try to construct a machine more powerful than any particular 
turing machine was to make a machine U that could simulate all turing machines. you would simply describe to U the particular turing machine you wanted it to simulate, give U the input data, and U would compute the appropriate output. Turing then showed that there was, in fact, a Turing machine that could do this

53. what do computers and turing machines have in common? they are programmable hence can compute anything that can be computed

54. what implication do the immediately above points have? big or expensive computers cannot do anything that a small cheap computer can't; more money will buy you a faster computer but if you have a small inexpensive laptop then you already have a universal computation device

55. how do we get the electrons to do our bidding inside of a computer? work through the levels of transformation for a particular problem see figure 1.9

56. what is the first level of transformation? describe a problem in a natural language whilst avoiding ambiguity because the electronic idiot would not know what to do

57. what is the second level of transformation? convert the problem expressed in natural language into an algorithm thereby getting rid of the ambiguity inherent in natural language. an algorithm is a step by step procedure that is guaranteed to terminate

58. what are the three characteristics of an algorithm? finiteness (guaranteed to terminate), definteness (each step is precisely stated), and effective computability (can be carried out by a computer)

59. For every problem there are usually many diﬀerent algorithms for solving that problem. One algorithm may require the fewest steps. Another algorithm may allow some steps to be performed concurrently. A computer that allows more than one thing to be done at a time can often solve the problem in less time, even though it is likely that the total number of steps to be performed has increased.

60. what is the 3rd level of transformation? transform the algorithm into source code using a chosen programming language. programming languages are mechanical languages (lacking the ambiguity of natural languages) that were invented to specify instructions to a computer

61. what are the two kinds of programming language that exist? high level and low level

62. Give a definition for a high level language? independent of the computer from which they will execute on i.e. they are machine 
independent

63. give a definition for a low level langauge? tied to the computer on which the programs will execute i.e. assembly language

64. what is the fourth level of transformation? translating the source code into the ISA of the computer that will be used to
execute the program.

65. what does ISA specify? the interface between the source code and the hardware of the machine that will be used to execute the program

66. with regards to ISA, use the automobile and driver analogy to help better understand the concept? the human driver of a car represents the computer program represented as 0's and 1's in the computer; the car corresponds to the microprocessor hardware. the ISA of the automobile is the specification of everything the human needs to know in order to get the car to move and everything the car needs to know to carry out the tasks specified by the human driver

67. what do opcode and operand mean in relation to ISA of a computer? opcode refers to and operation the computer can perform whilst operand is an individual data value

68. ISA specifies the acceptable representations for operands, what are the operands called? data types (is a representation of an operand such that the computer can perform operations on that representation)

69. ISA specifies the mechanisms that the computer can use to figure out where operands are located. what are these mechanisms called? addressing modes

70. are the number of opcodes, operands and addressing modes unique to each ISA? yes

71. name a few ISAs in use today? x86 by intel (currently also developed by AMD an other companies), SPARC oracle, power (IBM), arm and thumb (ARM)

72. what is the name of the program that is used to translate source code into the ISA of the machine that will be responsible for 
executing it? compiler

73. what is the name of the program that translates assembly language of a computer to its ISA? assembler

74. what is the 5th level of transformation? the implementation of the ISA referred to as its micro-architecture which is about what goes on underneath the hood

75. using the automobile analogy from earlier where the automobile ISA describes what the driver needs to know as he/she sits inside
the automobile to make the automobile carry out the driver's wishes, how can you use it to describe the microarchitecture that implements the ISA? since microarchitecture is about what goes on underneath the hood, here all automobile models can be different depending on the cost  and performance tradeoffs made by the designer of the car

76. previously you were introduced to a number of ISAs (x86 intel, powerpc IBM and motorola, thumb arm), what can you say about the implementation of these ISAs? each has been implemented by many different microarchitectures e.g. x86 original implementation was in 1979 was the 8086 followed by 80286, 80386 amongst others in the 1980s. more recently in 2015, intel introduced skylake. each of these x86 microprocessors has its own microarchitecture

77. Each microarchitecture is an opportunity for computer designers to make different tradeoﬀs between the cost of the microprocessor, the performance that the microprocessor will provide, and the energy that is required to power the micro- processor. Computer design is always an exercise in tradeoﬀs, as the designer opts for higher (or lower) performance, more (or less) energy required, at greater (or lesser) cost.

78. what is the 6th level of transformation? logic circuits i.e. implement each component of the microarchitecture out of simple logic circuits

79. are there choices to be made in the 6th level? yes, the logic designer decides how to best make the tradeoffs between cost and performance so for example even for operation as simple as addition, there are several choices of logic circuits to perform the operation at differing speeds and energy costs.

80. what is 7th level of transformation? each logic circuits is implemented in accordance with the requirements of the particular  device technologies used e.g. CMOS circuits, NMOS circuits, and gallium arsenide circuits all differ from each other

81. what else does ISA specify (apart from opcodes, operands, and addressing modes)? number of unique locations that comprise the computers memory (address space) and the number of bits contained in each location (addressability)

82. At each level of transformation, there are choices as to how to proceed. Our handling of those choices determines the resulting cost and performance of our computer. this book describe each of these transformations. We show how tran- sistors combine to form logic circuits, how logic circuits combine to form the microarchitecture, and how the microarchitecture implements a particular ISA.
In our case, the ISA is the LC-3. We complete the process by going from the English-language description of a problem to a C or C++ program that solves the problem, and we show how that C or C++ program is translated (i.e., compiled) to the ISA of the LC-3.


chapter 2

1. how is a computer organised? as a system with several levels of transformation i.e. a problem stated in a natural language is
actually solved by electrons moving around inside the components of a computer

2. what do you call the tiny little devices inside computer that control the movement of said electrons by reacting to the presence or absence of voltages in electronic circuits?

3. could these tiny devices be designed to detect actual value of voltages instead of the presence or absence of them? yes

4. why is this not done? it would make the control and detection circuits more complex than they need to be; it is much easier to
detect whether or not a voltage exists at a point in a circuit than it is to measure exactly what that voltage is

5. how do we symbollicaly represent the presence of a voltage? 1

6. how do we symbollically represent the absence of a voltage? 0

7. what word is used to refer to each 0 and each 1? bit which is a shortened form binary digit

8. is it the case that computers differentiate the absolute presence of a voltage (i.e. 1) from the absolute absence of a voltage
(i.e. 0)? no, electronic circuits in the computer differentiate voltages very close to 0 from voltages very far from 0

9. how many things can we differentiate with one wire and what values are assigned to them? two things; one of them can be assigned
the value 0 and the other can be assigned a value of 1

10. in order to get useful work done by a computer, it is necessary to be able to distinguish a large number of distinct values and assign each of them a unique representation. what is done in order to achieve this? combining many wires, that is, many bits. for
example, if we use eight bits (corresponding to the voltage present on each of eight wires), we can represent one particular value as 01001110, and another value as 11100111. In fact, if we are limited to eight bits, we can differentiate at most only 256 (i.e.
2^8) different things 

11. what qualifies a representation of information as a data type? if there are operations in the computer that can operate on information encoded in that representation

12. Each instruction set architecture (ISA) has its own set of data types and its own set of instructions that can operate on those data types. 

13. what data types will be used in the book? 2’s complement integers for representing positive and negative integers that we wish to perform arithmetic on, and ASCII codes for representing characters that we wish to input to a computer via the keyboard or output from the computer via a monitor. 

14. describe the unsigned integer representation?

15. describe the signed integer number representation (how many bits are assigned to positive numbers and how many bits to negative 
numbers)? each takes half the amount of bits available

16. how do signed magnitude, 1's complement, and 2's complement encode information? see figure 2.1

17. The ﬁrst thought that usually comes to mind is: If a leading 0 signiﬁes a positive integer, how about letting a leading 1 signify a negative integer? The result is the signed-magnitude data type. A second thought (which was actually used on some early computers such as the Control Data Corpora- tion 6600) was the following: Let a negative number be represented by taking the representation of the positive number having the same magnitude, and “ﬂipping” all the bits. That is, if the original representation had a 0, replace it with a 1; if it originally had a 1, replace it with a 0. This data type is referred to in the computer engineering community as 1’s complement 

18. can a computer designer assign any bit pattern he wants to represent any integer he wants? yes

19. why would this not be a good idea? it would complicate matters when they try to build electronic circuits capable of adding the numbers. In fact, the signed-magnitude and 1’s complement data types both require unnecessarily cumbersome hardware to do addition. 

20. how are negative integers represented in 2's complement encoding? choice of representations for the negative integers is based on the wish to keep the logic circuits as simple as possible. all computers use the same basic mechanism to perform addition. It is called an arithmetic and logic unit, usually known by its acronym ALU. It performs addition by adding the binary bit patterns at its inputs, producing a bit pattern at its output that is the sum of the two input bit patterns. What is particularly relevant is that the binary ALU does not know (and does not care) what the two patterns it is adding represent. the 2’s complement data type speciﬁes the representation for each negative integer so that when the ALU adds it to the representation of the positive integer of the same magnitude, the result will be the representation for 0. Moreover, and actually more importantly, as we sequence through representations of say −15 to +15 (as seen in figure 2.1), the ALU is adding 00001 to each successive representation. using this system, any carries are always ignored

21. almost all computers use the same mechanism to perform addition, what is it called? ALU

22. if you know the bit representation of integer A, what is a short cut you can use to work out -A? flip all of the bits and add 1
see example21.png 

23. algorithm for binary to decimal conversion see binaryToDecimal.png example22.png

24. algorith for the conversion of decimal to binary see decimalToBinary

25. algorithm to convert fractional binary to fractional decimal see binaryFractionToDecimal.png 

26. operations on bits: addition and subtraction see example23 example24 example25

27. why is sign extension used? in order to be able to operate on representations of different lengths

28. what happens if the sum of two integers is not small enough to be represented by the available bits? overflow of the msb occurs

29. what must you watch out for when dealing with overflow in signed encoding e.g. 2's complement? addition of positive numbers that
result in overflow into msb which indicate the sign of the value. since the result is negative, this is easy to detect. likewise when adding two negative numbers and the msb overflows and becomes zero, detection would be easy since the result of the ALU operation would positive

30. what are the basic logical operations performed by the ALU? binary logical AND function that requires two source operands see example26 and example27, binary logical OR function that aalso requires two source operands see example28, unary logical function NOT which operates on only one source operand (also known as the complement operation), the binary logical XOR function that requires two source operands see example29 and example210

31. what do you call an m-bit pattern where each bit has a logical value (0 or 1) independent of the other bits? a bit vector. It is a convenient mechanism for identifying a property such that some of the bits identify the presence of the property and other bits identify the absence of the property.

32. There are many uses for bit vectors. The most common use is a bit mask. The bit mask is a bit vector, where our choice of 0 or 1 for each bit allows us to isolate the bits we are interested in focusing on and ignore the bits that don’t matter. Another common use of bit vectors involves managing a complex system made up of several units, each of which is individually and independently either busy or available. The system could be a manufacturing plant where each unit is a particular machine. Or the system could be a taxicab network where each unit is a particular taxicab. In both cases, it is important to identify which units are busy and which are available so that work can be properly assigned. Say we have m such units. We can keep track of these m units with a bit vector, where a bit is 1 if the unit is free and 0 if the unit is busy. see example211

33. There are many other representations of information that are used in computers. can you name two that are among the most useful?
ASCII and floating point representation

34. what encoding format does the lc-3 use to represent integers? 16 bit 2's complement where the msb (most significant bit) identifies whether the number is positive or negative and the rest of the bits represent the magnitude of the value. with 16 bits used this way, we can express integer values between -32768 and 32767, that is between -2^(15) and (2^15) - 1.

35. what do we say about the precision of the value and its range? precision of the value is 15 bits and the range is 2^(16)

36. what if you need to represent fractional decimals (such as avogadros constant) inside of a computer? can you do it with the 16 bit 2's complement encoding format used to represent integers? no, because the range of avogadros constant 10^(23) is far too great to be expressed in the range available 2^(15) - 1. on the other hand, the 15 bits of precision is overkill for expressing the four significant decimal digits (6022) 

37. what data type can solve this problem? floating point data type

38. how does it solve this problem? instead of using most of the bits to represent the precision of a value, the floating point data type allocates some of the bits to the range of values that can be expressed. the rest of the bits (except for the sign bit) are used for precision

39. most ISA's today specify more than one floating point data type; float and double

40. how many bits does the float data type have? 32 bits

41. how are the 32 bits of the float data apportioned? 1 bit (the msb) for the sign (positive or negative), 8 bits for the range (exponent), 23 bits for the precision (the fraction field) see figure23

42. what types of numbers are represented by floating data type? numbers expressed in scientific notation as follows

N = (-1)^S x 1.fraction x 2^(exponent-127), where the exponent is greater than or equal to 1 and less than or equal to 254

here S, fraction, and exponent are the binary numbers in the fields of figure 2.3

43. what form is the equation noted in point 42 in? normalised form

44. why is the equation referred to as being in normalised form? because the data represents a floating point number only if the 8 bit exponent is restricted to 254 unsigned integer values, 1 (00000001) through 254 (11111110)

45. how many values can be represented uniquely with 8 bits? 256 values

46. which two integer values does the exponent not represent normalised values? 00000000 and 11111111

47. what values does the sign bit take? it is a single binary digit; 0 for positive numbers and 1 for negative numbers. it evaluates to +1 if S=0 and -1 if S=1

48. what do the 23 fraction bits form a part of in normalised form? a 24 bit quantity represnted by 1.fraction as the normalised form demands exactly one none zero binary digit to the left of the binary point. this one non zero bit does not need to be explicitly stored in the 32 bit floating point format. This is infact how we get 24 bits of precision

49. how are the 8 exponent bits (the range) in the format? in excess code

50. why is it referred to as an excess code? because one can get the *real* exponent by treating the code as an unsigned integer and subtracting the bias/excess 

51. what is the range of the exponent field in normalised form? exponent ﬁeld gives us numbers as large as 2^(127) for an exponent field containing 254 and as small as 2^(-126) for an exponent ﬁeld containing 1. see example212, example213, example214

52. what numbers are represented by the floating point data type when the exponent contains all 1's i.e. 11111111? the notion of infinity

53. what must the fractional field contain when infinity is being represented? all zeros

54. how are positive infinity and negative infinity distinguished? positive infinity is distinguished by the sign bit being 0 and negative infinity is distinguished by the sign bit being set to 1

55. what is the smallest number that can be represented in normalised form? 1.00000000000000000000000 × 2^(-126)

56. what are subnormal numbers? numbers smaller than 2^(-126) but larger than 0. they are given this name because they cannot be represented in normalised form.

57. what is the largest subnormal number? 0.11111111111111111111111 × 2^(-126)

58. what is the smallest subnormal number? 0.00000000000000000000001 × 2^(-126)

59. what equation is used to represent subnormal numbers? (-1)^S x 0.fraction x 2^(-126)

60. how is the exponent field represented for subnormal numbers? 00000000. see example215

61. why are subnormal numbers used? allows very, very tiny numbers to be represented

62. wht are ASCII codes used for? transferring characters between main computer processing unit and the input and output devices

63. how many bits is the ASCII code made up of? 8 bits

64. how does ascii work? Each key on the keyboard is identiﬁed by its unique ASCII code. When you type a key on the keyboard, the corresponding eight-bit code is stored and made available to the computer (PC?). Most keys are associated with more than one code. For example, the ASCII code for the letter E is 01000101, and the ASCII code for the letter e is 01100101.


chapter 3

1. what are most computer processors today constructed out of? out of MOS transistors

2. what does MOS stand for? metal oxide semiconductor

3. what are the two type of MOS transistors? p-type and n-type

4. how many terminals does a transistor have? 3

5. what are they called? gate, source, and drain see figure32

6. how do n type transistors work? when the gate is supplied with 1.2v, the transistor acts like a piece of wire (short circuit); when supplied with 0 volts it acts like an open wire (open circuit) see figure32

7. what of the p type transistor - how does it work? When the gate is supplied with 0 volts, the P-type transistor acts (more or less) like a piece of wire, closing the circuit. When the gate is supplied with 1.2 volts, the P-type transistor acts like an open circuit. see figure33

8. because the p-type and n-type transistors work in a complimentary manner, what do we refer to circuits that contain both of them? CMOS circuits

9. what is the next step up from logic elements? logic gate ie transistor circuits (comprising of a combination of the logic elements) that implement the logical values of AND, OR, NOT

10. how is the NOT gate (inverter) constructed? from two MOS transistors; n-type and p-type see figure34

11. how does the inverter work? when supplied with 0 volts, P-type transistor acts like a short circuit and the N-type transistor acts like an open circuit. The output is, therefore, connected to 1.2 volts. On the other hand, if the input is supplied with 1.2 volts, the P-type transistor acts like an open circuit, but the N-type transistor acts like a short circuit. The output in this case is connected to ground (i.e., 0 volts). 

12. how is the NOR contructed? made out of 2 p-type and 2 n-type transistors see figure35

13. how does the NOR gate work? if A is supplied with 0 volts and B is supplied with 1.2 volts. In this case, the lower of the two P-type transistors produces an open circuit, and the output C is disconnected from the 1.2-volt power supply. However, the leftmost N-type transistor acts like a piece of wire, connecting the output C to 0 volts.

Note that if both A and B are supplied with 0 volts, the two P-type transistors conduct, and the output C is connected to 1.2 volts. Note further that there is no ambiguity here, since both N-type transistors act as open circuits, and so C is disconnected from ground.

If either A or B is supplied with 1.2 volts, the corresponding P-type transistor results in an open circuit. That is suﬃcient to break the connection from C to the 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type transistors is suﬃcient to cause that transistor to conduct, resulting in C being connected to ground (i.e., 0 volts).

14. how is the OR gate constructed? made out of 2 p-type and 2 n-type transistors (similar to NOR gate) but augmented by adding an inverter at its output, as shown in Figure 3.6a

15. how is the AND gate contructed? made out of 2 p-type and 2 n-type transistors along with an inverter see figure38

16. how does it work? if either A or B is supplied with 0 volts, there is a direct connection from C to the 1.2-volt power supply. The fact that C is at 1.2 volts means the N-type transistor whose gate is connected to C provides a path from D to ground. Therefore, if either A or B is supplied with 0 volts, the output D of the circuit of Figure 3.8 is 0 volts.

On the other hand, if both A and B are supplied with 1.2 volts, then both of their corresponding P-type transistors are open. However, their corresponding N-type transistors act like pieces of wire, providing a direct connection from C to ground. Because C is at ground, the rightmost P-type transistor acts like a closed circuit, forcing D to 1.2 volts.

17. The gates just discussed are very common in digital logic circuits and in digital computers. There are billions of inverters (NOT gates) in Intel’s Skylake microprocessor. As a convenience, we can represent each of these gates by stan- dard symbols, as shown in Figure 3.9. The bubble shown in the inverter, NAND, and NOR gates signiﬁes the complement (i.e., NOT) function.

18. Gates with More Than Two Inputs: the notion of AND, OR, NAND, and NOR gates extends to larger numbers of inputs. One could build a three-input AND gate or a four-input OR gate, for example. An n-input AND gate has an output value of 1 only if ALL n input variables have values of 1. If anyofthen inputs has a value of 0, the output of the n-input AND gate is 0. An n-input OR gate has an output value of 1 if ANY of the n input variables has a value of 1. That is, an n-input OR gate has an output value of 0 only if ALL n input variables have values of 0. see figure310

19. Now that we understand the workings of the basic logic gates, what is the next step? build some of the logic structures that are important components of the microarchitecture of a computer.

20. there are two types of logic structures, what are they called and what do? those that include storage of information and those that do not. those that do not store information are called decision elements or combinational logic structures. those that store information and make decisions as well are called sequential logic circuits

21. why are combinational logic structures called so? because their outputs are strictly dependent on the combination of input values that are being applied to the structure right now. Their outputs are not at all dependent on any past history of information that is stored internally, since no information can be stored internally in a combinational logic circuit.

22. name three decision elements? decoder, mux (multiplexer), full adder 

23. how does the decoder work? has the property that exactly one of its outputs is 1 and all the rest are 0s. The one output that is logically 1 is the output corresponding to the input pattern that it is expected to detect. In general, decoders have n inputs and 2^n outputs. We say the output line that detects the input pattern is asserted. That is, that output line has the value 1, rather than 0 as is the case for all the other output lines. see figure311. The decoder is useful in determining how to interpret a bit pattern. 

24. how does the mux work? multiplexer, more commonly referred to as a mux is used to select one of the inputs (A or B) and connect it to the output. The select signal (S in Figure 3.12) determines which input is connected to the output.

The mux of Figure 3.12 works as follows: Suppose S  =  0, as shown in Figure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out- put of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is whatever the input A is. That is, if A = 0, then the output of the leftmost AND gate is 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output of the rightmost AND gate is 0, it has no eﬀect on the OR gate. Consequently, the output at C is exactly the same as the output of the leftmost AND gate. The net result of all this is that if S = 0, the output C is identical to the input A.

On the other hand, if S = 1, it is B that is ANDed with 1, resulting in the output of the OR gate having the value of B. We say S selects the source of the mux (either A or B) to be routed through to the output C. In general, a mux consists of 2^n  inputs and n select lines. see figure313

25. how does the One-Bit Adder (a.k.a. a Full Adder) work? Figure 3.14 is a truth table that describes the result of binary addition on one column of bits within two n-bit operands. At each column, there are three values that must be added: one bit from each of the two operands A and B and the carry from the previous column. Note that if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e., Ci+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If all three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and a carry of 1.

Figure 3.15 shows a logic gate implementation of a one-bit adder. Note that each AND gate in Figure 3.15 produces an output 1 for exactly one of the eight input combinations. The output of the OR gate for Ci+1 must be 1 in exactly those cases where the corresponding input combinations in Figure 3.14 produce an output 1. Therefore, the inputs to the OR gate that generates Ci+1 are the outputs of the AND gates corresponding to those input combinations. Similarly, the inputs to the OR gate that generates Si are the outputs of the AND gates corresponding to the input combinations that require an output 1 for Si in the truth table of Figure 3.14. Note that since the input combination 000 does not result in an output 1 for either Ci+1 or Si, its corresponding AND gate is not an input to either of the two OR gates.

26. what does Figure 3.16 show? a circuit for adding two 4-bit binary numbers, using four of the one-bit adder circuits of Figure 3.15. Note that the carry out of column i is an input to the addition performed in column i + 1. 

27. if you wish to implement a logic circuit for adding two 16 bit numbers, how many one bit adders will you require? 16 one bit adders

28. describe the half adder? Note that the carry into the rightmost column in Figure 3.16 is 0. That is, in the rightmost circuit, S0 and C1 depend only on two inputs, A and B . Since that circuit depends on only two inputs, it has been referred to as a half adder. Since the other circuits depend on all three inputs, they are referred to as full adders. 

29. what is the name of the building block that is used to implement any collection of logic functions? a programmable logic array (PLA)

30. what does the PLA consist of? an array of AND gates (called an AND array) followed by an array of OR gates (called an OR array). The number of AND gates corresponds to the number of input combinations (rows) in the truth table. For n-input logic functions, we need a PLA with 2^n n-input AND gates. In Figure 3.17, we have 2^3 three-input AND gates, corresponding to three logical input variables. 

31. what does the number of OR gates correspond to in the PLA? the number of logic functions that we wish to implement i.e. the number of output columns in the truth table. The implementation algorithm is simply to connect the output of an AND gate to the input of an OR gate if the corresponding row of the truth table produces an output 1 for that output column. Hence the notion of programmable.

Figure 3.15 shows seven AND gates connected to two OR gates since our requirement was to implement two functions (sum and carry) of three input variables. Figure 3.17 shows a PLA that can implement any four functions of three variables by appropriately connecting AND gate outputs to OR gate inputs. That is, any function of three variables can be implemented by connecting the outputs of all AND gates corresponding to input combinations for which the output is 1 to inputs of one of the OR gates. Thus, we could implement the one-bit adder by programming the two OR gates in Figure 3.17 whose outputs are W and X by connecting or not connecting the outputs of the AND gates to the inputs of those two OR gates as speciﬁed by the two output columns of Figure 3.14.

32. describe the concept of logical completeness: set of gates {AND, OR, and NOT} (provided by the PLA) is logically complete because a barrel of AND gates, a barrel of OR gates, and a barrel of NOT gates are suﬃcient to build a logic circuit that carries out the speciﬁcation of any desired truth table. 

33. name 2 logic structures that do include the storage information? R-S latch and gated D latch

34. how does the R-S latch work? stores 1 bit of information, a 0 or a 1. can be implemented in many ways simplest one is shown in figure318 where two 2-input NAND gates are connected such that the output of each is connected to one of the inputs of the other; inputs S and R are normally held at a logic level 1.

35. where does the RS latch get its name? because setting the latch to store a 1 was referred to as setting the latch, and setting the latch to store a 0 was referred to as resetting the latch. Ergo, R-S

36. what is meant when we state that the R-S latch is in a quiescent state? when the latch is storing a value, either 0 or 1, and nothing is trying to change that value. 

37. what contributes to this quiescent state? the fact that both R and S are held at a logic level of 1

38. what letter in figure318 designates the value that is currently stored in the latch? letter a; also referred to as the output of the latch.

39. Consider ﬁrst the case where the value stored and therefore the output a is 1. Since that means the value A is 1 (and since we know the input R is 1 because we are in the quiescent state), the NAND gate’s output b must be 0. That, in turn, means B must be 0, which results in the output a equal to 1. As long as the inputs S and R remain 1, the state of the circuit will not change. That is, the R-S latch will continue to store the value 1 (the value of the output a).

40. If, on the other hand, we assume the output a is 0, then A must be 0, and the output b must be 1. That, in turn, results in B equal to 1, and combined with the input S equal to 1 (again due to quiescence), results in the output a equal to 0. Again, as long as the inputs S and R remain 1, the state of the circuit will not change. In this case, we say the R-S latch stores the value 0.

41. how can you set the latch to a 1 or a 0? you an set the latch to a 1 by temporarily setting s to 0 and having R remain at logic level 1; the latch can be set to 0 by temporarily setting R to 0 and having S remain at logic level 1

42. in order for the R-S latch to work properly, what must be done to R and S? must never be set to 0 at the same time

43. If we set S to 0 for a very brief period of time, this causes a to equal 1, which in turn causes A to equal 1. Since R is also 1, the output at b must be 0. This causes B to be 0, which in turn makes a equal to 1. If, after that very brief period of time, we now return S to 1, it does not aﬀect a. Why? Answer: Since B is also 0, and since only one input 0 to a NAND gate is enough to guarantee that the output of the NAND gate is 1, the latch will continue to store a 1 long after S returns to 1.

44. what can you do to clear the latch? we can clear the latch (set the latch to 0) by setting R to 0 for a very short period of time.

45. what happens if both S and R are set to 0 at the same time? outputs a and b would both be 1 and the final state of the latch would depend on the electrical properties of the transistors making up the gates and not on the logic being performed.

46. we should note that when a digital circuit is powered on, the latch can be in either of its two states, 0 or 1. It does not matter which state since we never use that information until after we have set it to 1 or 0.

47. how is the gated d latch implemented? It consists of the R-S latch of Figure 3.18, plus two additional NAND gates that allow the latch to be set to the value of D, but only when WE is asserted (i.e., when WE equals 1). WE stands for write enable. see figure319

48. what happens when WE is not asserted (i.e., when WE equals 0)? the outputs of S and R are both equal to a 1. Since S and R are inputs to the R-S latch, if they are kept at 1, the value stored in the latch remains unchanged

49. what happens when WE is momentarily set to 1? exactly one of the outputs S or R is set to 0, depending on the value of D.IfD equals 1, then S is set to 0. If D equals 0, then both inputs to the lower NAND gate are 1, resulting in R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R is set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according to whether D is 1 or 0. When WE returns to 0, S and R return to 1, and the value stored in the R-S latch persists.

50. what is memory made up of? a (usually large) number of locations, each uniquely identiﬁable and each having the ability to store a value. 

51. what do we call the unique identiﬁer associated with each memory location? address

52. what name do we give to the number of bits of information stored in each location? addressability

53. what do we call the total number of uniquely identiﬁable memory locations? address space

54. what do we mean by stating that a computing device has a 2GB memory? colloquially we say the computing device has 2 billion memory locations, however, the address space is actually 1024x1024x1024x2 which yields 2,147,483,648 locations

55. why 1024? because 1024 bytes make a kb and 1024 kb make a mb and 1024 mb make a GB

56. Actually, the number two billion is only an approximation, due to the way we specify memory locations. Since everything else in the computer is represented by sequences of 0s and 1s, it should not be surprising that memory locations are identiﬁed by binary addresses as well. With n bits of address, we can uniquely identify 2^n locations. Ten bits provide 1024 locations, which is approximately 1000. If we have 20 bits to represent each address, we have 2^(20) uniquely identiﬁable locations, which is approximately one million. With 30 bits, we have 2^(30) locations, which is approximately one billion. In the same way we use the preﬁxes “kilo” to represent 2^(10) (approximately 1000) and “mega” to represent 2^(20) (approximately one million), we use the preﬁx “giga” to represent 2^(30) (approximately one billion). Thus, 2 giga really corresponds to the number of uniquely iden- tiﬁable locations that can be speciﬁed with 31 address bits. We say the address space is 2^(31), which is exactly 2,147,483,648 locations, rather than 2,000,000,000, although we colloquially refer to it as two billion. 

57. A 2-gigabyte memory (written 2GB) is a memory consisting of 2,147,483,648 memory locations, each containing one byte (i.e., eight bits) of storage. 

58. why are most memories byte-adddressable? most computers got their start processing data where one character stroke on the keyboard corresponds to one 8-bit ASCII code. If the memory is byte-addressable, then each ASCII character occupies one location in memory. Uniquely identifying each byte of memory allows individual bytes of stored information to be changed easily.

59. Many computers that have been designed speciﬁcally to perform large scientiﬁc calculations are 64-bit addressable. This is due to the fact that numbers used in scientiﬁc calculations are often represented as 64-bit ﬂoating-point quantities. Since scientiﬁc calculations are likely to use numbers that require 64 bits to represent them, it is reasonable to design a memory for such a computer that stores one such number in each uniquely identiﬁable memory location.

60. Figure 3.20 illustrates a memory of size 2^2 by 3 bits. That is, the memory has an address space of four locations and an addressability of three bits. A memory of size 2^2 requires two bits to specify the address. We describe the two-bit address as A[1:0]. A memory of addressability three stores three bits of information in each memory location. We describe the three bits of data as D[2:0]. In both cases, our notation A[high:low] and D[high:low] reﬂects the fact that we have numbered the bits of address and data from right to left, in order, starting with the rightmost bit, which is numbered 0. The notation [high:low] means a sequence of high − low + 1 bits such that “high” is the bit number of the leftmost (or high) bit number in the sequence and “low” is the bit number of the rightmost (or low) bit number in the sequence.

Accesses of memory require decoding the address bits. Note that the address decoder takes as input the address bits A[1:0] and asserts exactly one of its four outputs, corresponding to the word line being addressed. In Figure 3.20, each row of the memory corresponds to a unique three-bit word, thus the term word line. Memory can be read by applying the address A[1:0], which asserts the word line to be read. Note that each bit of the memory is ANDed with its word line and then ORed with the corresponding bits of the other words. Since only one word line can be asserted at a time, this is eﬀectively a mux with the output of the decoder providing the select function to each bit line. Thus, the appropriate word is read at D[2:0].

Figure 3.21 shows the process of reading location 3. The code for 3 is 11. The address A[1:0]=11 is decoded, and the bottom word line is asserted. Note that the three other decoder outputs are not asserted. That is, they have the value 0. The value stored in location 3 is 101. These three bits are each ANDed with their word line producing the bits 101, which are supplied to the three output OR gates. Note that all other inputs to the OR gates are 0, since they have been produced by ANDing with their unasserted word lines. The result is that D[2:0] = 101. That is, the value stored in location 3 is output by the OR gates.

Memory can be written in a similar fashion. The address speciﬁed by A[1:0] is presented to the address decoder, resulting in the correct word line being asserted. With write enable (WE) also asserted, the three bits D[2:0] can be written into the three gated latches corresponding to that word line.

61. summarise how sequential logic circuits operate? they base their decisions not only on the input values now present but also on what has happened before; they contain storage elements that allow them to keep track of prior history information (what distinguishes from combinational logic circuits) see figure322 (Note the storage elements. Note also that the output can be dependent on both the inputs now and the values stored in the storage elements. The values stored in the storage elements reﬂect the history of what has happened before)

62. what machines are sequential logic circuits used to implement? finite state machines

63. can you give two examples; one for a combination decision element and the other for a sequential logic element? combination lock and sequential locks respectively see figure323

64. define what is meant by the state of a mechanism/system? snapshot of a system with all relevant items explicitly expressed.

65. how many elements does a finite state machine consist of? 5

66. can you state all 5? a finite number of states, a finite number of external inputs, a finite number of external outputs, 
an explicit specification of all state transitions, an explicit specification of what determines each external output value

67. what does the set of states of a system represent? all possible snapshots/configurations that the system can be in. Each state transition describes what it takes to get from one state to another. 

68. A state diagram is a convenient representation of a ﬁnite state machine. see figure326. The explicit speciﬁcations of all state transitions are shown by the arrows in the state diagram. The arrowhead on each arc speciﬁes which state the system is coming from and which state it is going to. We refer to the state the system is coming from as the current state, and the state it is going to as the next state. The combination lock has eight state transitions. Associated with each transition is the input that causes the transition from the current state to the next state.

A couple of things are worth noting. First, it is usually the case that from a current state there are multiple transitions to next states. The state transition that occurs depends on both the current state and the value of the external input. In short, the next state is determined by the combination of the current state and the current external input.

The output values of a system can also be determined by the combination of the current state and the value of the current external input. However, as is the case for the combination lock, where states A, B, and C specify the lock is “locked,” and state D speciﬁes the lock is “unlocked,” the output can also be determined solely by the current state of the system. In all the systems we will study in this book, the output values will be speciﬁed solely by the current state of the system.

69. what is a characteristic of asynchronous finite state machines? there is nothing that synchronises when each state transition occurs; a transition from a current state to a next state in our ﬁnite state machine happened when it happened

70. are computers synchronous or aynchronous systems? synchronous because state transitions take place one after another at identical fixed units of time.

71. what is the important common characteristic shared between synchronous and asynchronous finite state machines? they carry out work, one state transition at a time, moving closer to a goal

72. what controls the behaviour of a synchronous finite state machine as it transitions from a current state to its next state after an identical fixed interval of time? a clock circuit produces a signal, called THE clock, whose value alternates between 0 volts and some specified fixed voltage. In digital logic terms, the clock is a signal whose value alternates between 0 and 1. figure328 shows the value of the clock signal as a function of time. Each of the repeated sequence of identical intervals is referred to as a clock cycle. 

73. what is meant by a laptop running at a frequency of 2GHz? it can perform 2 billion pieces of work each second since 2 GHz means 2 billion clock cycles each second 

74. how many state transitions does a synchronous finite state machine make each clock cycle? one state transition

75. In electronic circuit implementations of a synchronous ﬁnite state machine, the transition from one state to the next occurs at the start of each clock cycle.

76. A Danger Sign: Figure 3.29 shows the danger sign; it contains 5 lights labelled 1 to 5. The synchronous ﬁnite state machine will be used to control it. The purpose of the synchronous finite state machine (a.k.a. a controller) is to direct the behaviour of the system i.e. the set of lights. The controller is equipped with a switch. When the switch is in the ON position, the controller directs the lights as follows: During one unit of time, all lights will be oﬀ. In the next unit of time, lights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on. Then all ﬁve lights will be on. Then the sequence repeats. The lights continue to sequence through these four states as long as the switch is on. If the switch is turned oﬀ, all the lights are turned oﬀ and remain oﬀ. Figure 3.30 is a state diagram for the synchronous ﬁnite state machine that controls the lights. There are four states, one for each of the four conditions corresponding to which lights are on. Note that the outputs (whether each light is on or oﬀ) are determined by the current state of the system. If the switch is on (input  =  1), the transition from each state to the next state happens at one-second intervals, causing the lights to ﬂash in the sequence described. If the switch is turned oﬀ (input = 0), the state always transitions to state A, the “all oﬀ” state.

77. Figure 3.31 is a block diagram of the speciﬁc sequential logic circuit we need to control the lights. Several things are important to note in this ﬁgure. First, the two external inputs: the switch and the clock. The switch determines whether the ﬁnite state machine will transition through the four states or whether it will transition to state A, where all lights are oﬀ. The other input (the clock) controls the transition from state A to B, B to C, C to D, and D to A by controlling the state of the storage elements. 

78. Second, there are two storage elements for storing state information. Since there are four states, and since each storage element can store one bit of informa- tion, the four states are identiﬁed by the contents of the two storage elements: A (00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage element 1 contains the low bit.

79. Third, combinational logic circuit 1 shows that the on/oﬀ behavior of the lights is controlled by the storage elements. That is, the input to the combinational logic circuit is from the two storage elements, that is, the current state of the ﬁnite state machine.

80. Finally, combinational logic circuit 2 shows that the transition from the cur- rent state to the next state depends on the two storage elements and the switch. If the switch is on, the output of combinational logic circuit 2 depends on the state of the two storage elements.

81. Figure 3.32 shows the logic that implements combinational logic circuits 1 and 2. Two sets of outputs are required for the controller to work properly: a set of external outputs for the lights and a set of internal outputs for the inputs to the two storage elements that keep track of the state. Light 5 is controlled by the output of the AND gate labeled V, since the only time light 5 is on is when the controller is in state 11. Lights 3 and 4 are controlled by the output of the OR gate labeled X, since there are two states in which those lights are on, those labeled 10 and 11.

82. Why are lights 1 and 2 controlled by the output of the OR gate labeled W? because input to OR gate W comes from the top NAND gate whose output is 1 when the storage elements are set to 01

83. Storage element 2 should be set to 1 for the next clock cycle if the next state is 10 or 11. This is true only if the switch is on and the current state is either 01 or 10. Therefore, the output signal that will make storage element 2 be 1 in the next clock cycle is the output of the OR gate labeled Y. if you look at the diagram you should be able to deduce why

84. Why is the next state of storage element 1 controlled by the output of the OR gate labeled Z? storage element 1 should be set to 1 in the next clock cycle if the switch is on and the current state is either 00 or 10

85. In order for the danger sign controller to work, the state transitions must occur once per second when the switch is on.

86. what is the problem with having gated D latches as the storage elements? when WE is asserted (i.e. the clock signal value is 1), the the output of OR gates Y and Z would immediately change the bits stored in the two gated D latches. This would produce new input values to the three AND gates that are input to OR gates Y and Z, producing new outputs that would be applied to the inputs of the gated latches, which would in turn change the bits stored in the gated latches, which would in turn mean new inputs to the three AND gates and new outputs of OR gates Y and Z. This would happen again and again, continually changing the bits stored in the two storage elements as long as the Write Enable signal to the gated D latches was asserted. The result: We have no idea what the state of the ﬁnite state machine would be for the next clock cycle. And, even in the current clock cycle, the state of the storage elements would change so fast that the ﬁve lights would behave erratically.

87. what is the problem is the gated D latch? We want the output of OR gates Y and Z to transition to the next state at the end of the current clock cycle and allow the current state to remain unchanged until then. That is, we do not want the input to the storage elements to take eﬀect until the end of the current clock cycle. We need storage elements that allow us to read the current state throughout the current clock cycle, and not write the next state values into the storage elements until the beginning of the next clock cycle.

88. how do flip flops work? they are storage elements that allow the reading of the current state throughout the current clock cycle and not write the next state into the storage elements until the beginning of the next clock cycle. reading must be allowed throughout the current clock cycle and writing must only occur at the end of the clock cycle

89. what is a flip flop made of? two gated D latches in a master/slave relationship; the write enable signal of the master is 1 when the clock is 0, and the write enable signal of the slave is 1 when the clock is 1. see figure333

90. how do flip flops work when used as storage elements? see Figure 3.34 (timing diagram for the master/slave ﬂip-ﬂop). A timing diagram shows time passing from left to right. Note that clock cycle n starts at the time labeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time labeled 4. at the start of each clock cycle, the output of the storage elements are the outputs of the two slave latches. These outputs (starting at time 1) are input to the AND gates, resulting in OR gates Y and Z producing the next state values for the storage elements (at time 2). The timing diagram shows the propagation delay of the combinational logic, that is, the time it takes for the combinational logic to produce outputs of OR gates Y and Z. Although OR gates Y and Z produce the Next State value sometime during half-cycle A, the write enable signal to the master latches is 0, so the next state cannot be written into the master latches.

91. what happens in half cycle b? At the start of half-cycle B (at time 3), the clock signal is 0, which means the write enable signal to the master latches is 1, and the master latches can be written. However, during the half-cycle B, the write enable to the slave latches is 0, so the slave latches cannot write the new information now stored in the master latches. At the start of clock cycle n+1 (at time 4), the write enable signal to the slave latches is 1, so the slave latches can store the next state value that was created by the combinational logic during clock cycle n. This becomes the current state for clock cycle n+1.

Since the write enable signal to the master latches is now 0, the state of the master latches cannot change. Thus, although the write enable signal to the slave latches is 1, those latches do not change because the master latches cannot change.

92. In short, the output of the slave latches contains the current state of the system for the duration of the clock cycle and produces the inputs to the six AND gates in the combinational logic circuits. Their state changes at the start of the clock cycle by storing the next state information created by the combinational logic during the previous cycle but does not change again during the clock cycle.

93. why don't they change? During half-cycle A, the master latches cannot change, so the slave latches continue to see the state information that is the current state for the new clock cycle. During half-cycle B, the slave latches cannot change because the clock signal is 0. Meanwhile, during half-cycle B, the master latches can store the next state information produced by the combinational logic, but they cannot write it into the slave latches until the start of the next clock cycle, when it becomes the state information for the next clock cycle.

94. what does the data path of a computer consist of? all of the logic structures that combine to process information in the core of the computer see Figure 3.35 you should be able to spot 5 muxes, adder (ALU with +), ALU, PC, IR, MAR, and MDR are all 16-bit registers that store 16 bits of information each, three 1-bit registers, N, Z, and P

The arrows in Figure 3.35 represent wires that transmit values from one struc- ture to another. Most of the arrows include a cross-hatch with a number next to it. The number represents the number of wires, corresponding to the number of bits being transmitted. 

95. what is a register made of? a set of n flip flops that collectively are used to store one n-bit value. i.e. gated D latches where one bit of information can be stored in one flip flop hence a 16 bit register is composed of 16 flip flops see figure336

96. why use flip flops to make registers instead of latches? because it is usually important to be able to both read the contents of a register throughout a clock cycle and also store a new value at the end of the clock cycle

chapter 4

1. what two things do you need to get a task done by a computer? a computer that will do the work and a computer program specifying the task that needs to be achieved

2. what is a computer program comprised of? a set of instructions, each specifying a well-deﬁned piece of work for the computer to carry out. 

3. what is the name of the smallest piece of work specified in a computer program? an instruction

4. who proposed the fundamental model of computers? john von neumann

5. what are the 5 basic components of the model? memory, cpu, input, output, and control unit see figure41

6. in which of the components is the computer program contained in? memory

7. which of the components can hold the data that the program will operate on? memory or input device

8. which component controls the order in which operations are carried out? control unit

9. what does the 16GB when talking about memory refer to? the "16 giga" refers to 2^(34) memory locations and the "byte" refers to the eight bits stored in each location. the term is 16 giga because 16 is 2^(4) and giga is the term used to represent 2^(30) which is approximately one billion, 2^(4) x 2^(30) = 2^(34)

10. if you have k bits, how many unique items can you represent? 2^k

11. ergo, to uniquely identify 2^(34) memory locations, how many bits do you need? 34 bits where each location must have its own 34-bit address

12. to read the contents of a memory location, where do we place the address of that location in memory? in the memory address register (MAR) and then interrogate the computer's memory

13. where will the information stored in the address stored in MAR be placed? in the MDR register

14. what is the process of writing or storing a value in a memory location? location of the address to be written to is first stored in MAR and the data to be written is stored in the MDR register. computer memory is then interrogated with the WE signal asserted such that the information stored in MDR is written to the address stored in MAR

15. what are the two characteristics of a memory location? its address and what is stored there

16. what component carries out the processing of information in a computer? processing unit

17. what is processing unit in a computer comprised of? many sophisticated functional units each performing one particular operation (divide, square root etc) 

18. what is the name of simplest processing unit and the one thought of when discussing the von neumann model? ALU which is capable of performing basic arithmetic functions and basic logic operations 

19. what is the name given to the fixed size elements that the ALU processes? word length of the computer; the data elements are called words

20. what specifies the word length (depends on the intended use of the computer) of a computer? the ISA

21. what word length is specified by most ISAs today? 64 bits and 32 bits (though 32 bits has mostly been done away with)

22. what range of word lengths can you expect to find being processed in inexpensive processors today? 8 bits to 16 bits

23. how many bits are processed in the ALU of the lc-3? 16 bits

24. It is almost always the case that a computer provides some small amount of storage very close to the ALU to allow results to be temporarily stored if they will be needed to produce additional results in the near future. what is the most common form of aforementioned temporary storage? a set of registers

25. typically what is the size of one of these register's equal to? values being processed by the ALU i.e. a word 

26. Current microprocessors typically contain 32 registers, each consisting of 32 or 64 bits, depending on the architecture. These serve the same purpose as the eight 16-bit registers in the LC-3. 

27. what is the consequence of the importance of temporary storage for values that most modern computers will need shortly? many processors have access to an additional set of special purpose registers consisting of 128 bits to handle special needs

28. what component is used to keep track of both where we are within the process of executing a program and where we are in the process of executing an instruction? control unit

29. what mechanism does the control unit use to keep track of which instruction is being executed? the instruction register contains the instruction being executed

30. what mechanism does the control unit use to keep track of which instruction is to be executed next? program counter(instruction pointer) contains the next instructions address

31. We  constructed  Figure  4.3  by  starting  with  the  LC-3’s  full  data  path (Figure 3.35) and removing all elements that are not essential to pointing out the ﬁve basic components of the von Neumann model. Note that there are two kinds of arrowheads in Figure 4.3: ﬁlled-in and not-ﬁlled-in. Filled-in arrowheads denote data elements that ﬂow along the corresponding paths. Not-ﬁlled-in arrowheads denote control signals that control the processing of the data elements. For example, the box labeled ALU in the processing unit processes two 16-bit values and produces a 16-bit result. The two sources and the result are all data, and are designated by ﬁlled-in arrowheads. The operation performed on those two 16-bit data elements (it is labeled ALUK) is part of the control—therefore, a not-ﬁlled-in arrowhead.

32. MEMORY consists of the storage elements, along with the Memory Address Register (MAR) for addressing individual locations and the Memory Data Register (MDR) for holding the contents of a memory location on its way to/from the storage.
Note that the MAR contains 16 bits, reﬂecting the fact that the memory address space of the LC-3 is 2^(16) memory locations. The MDR contains 16 bits, reﬂecting the fact that each memory location contains 16 bits—that is, the LC-3 is 16-bit addressable.

33. INPUT/OUTPUT consists of a keyboard and a monitor. The simplest keyboard requires two registers: a keyboard data register (KBDR) for holding the ASCII codes of keys struck and a keyboard status register (KBSR) for maintaining status information about the keys struck.

34. The simplest monitor also requires two registers: a display data register (DDR) for holding the ASCII code of something to be displayed on the screen and a display status register (DSR) for maintaining associated status information. 

35. THE PROCESSING UNIT consists of a functional unit (ALU) that performs arithmetic and logic operations and eight registers (R0, … R7) for storing temporary values that will be needed in the near future as operands for subsequent instructions. The LC-3 ALU can perform one arithmetic operation (addition) and two logical operations (bitwise AND and bitwise NOT).

36. THE CONTROL UNIT consists of all the structures needed to manage the processing that is carried out by the computer. Its most important structure is the ﬁnite state machine, which directs all the activity. Processing is carried out step by step, or rather, clock cycle by clock cycle. Note the CLK input to the ﬁnite state machine in Figure 4.3. It speciﬁes how long each clock cycle lasts. The instruction register (IR) is also an input to the ﬁnite state machine since
the LC-3 instruction being processed determines what activities must be carried out. The program counter (PC) is also a part of the control unit; it keeps track of the next instruction to be executed after the current instruction ﬁnishes.

37. Note that all the external outputs of the ﬁnite state machine in Figure 4.3 have arrowheads that are not ﬁlled in. These outputs control the processing throughout the computer. For example, one of these outputs (two bits) is ALUK, which controls the operation performed in the ALU (ADD, AND, or NOT) during the current clock cycle. Another output is GateALU, which determines whether or not the output of the ALU is provided to the processor bus during the current clock cycle.

38. what is the central idea in the von neumann model of computer processing? program and data are both stored as sequences of bits in the computer's memory and the program is executed one instruction at a time under the direction of the control unit

39. what is the most basic unit of computer processing? an instruction

40. what is an instruction made up of? opcode (what the instruction does) and operand (what is does it to)

41. there are fundamentally 3 types of instructions, what are they? operates, data movement, and control, although many ISAs have some special instructions that are necessary for those ISAs.

42. what do operate instruction do? operate on data. The LC-3 has three operate instructions: one arithmetic (ADD) and two logicals (AND and NOT).

43. what do data movement instructions do? move data from processing unit to and from memory and to and from input/output devices. The LC-3 has six data movement instructions.

44. what do control instructions do? they alter the sequential processing of instructions; that is, normally the next instruction executed is the instruction contained in the next memory location.

45. for LC-3, which has 16 bits, and are numbered from left to right, bit [15] to bit [0], how are they divided? bits 15:12 contain the opcode and bits 11:0 are used to figure out where the operands are

46. how many opcodes does the LC-3 have? 15 opcodes; one is reserved for future use

47. how many operands does the ADD operate instruction have? 3 operands, two source operands (the numbers to be added) and one destination operand (where the sum is to be stored after the addition is performed)

48. what does the ADD operate require? at least one of the two source operands must be stored in one of the 8 registers; the result should also be written to one of the 8 registers

49. since the LC-3 has 8 registers, how many bits are necessry to identify each register? 3

50. how many forms does the 16-bit LC-3 ADD instruction have? 2 forms

51. what is the four bit opcode (from 15:12) for the ADD operate in LC-3? 0001

52. in the ADD instruction, what do bits 11:9 specify? the register for storing the result 

53. in the ADD instruction, what do bits 8:6 specify? the register storing one of the two source operands

54. what is the difference between the two formats of the ADD instruction? the 1 or 0 stored in bit 5 and what they each mean

55. what does the 0 stored in bit 5 in the instruction mean? that the second source operand is in the register specified by bits 2:0

56. what does the 1 stored in bit 5 in the instruction mean? the second source operand is formed by sign-extending the integer in bits [4:0] to 16 bits. see example410 and example411

57. what is the four bit opcode (from 15:12) for the AND operate in LC-3? 0101 see example42

58. what is the four bit opcode (from 15:12) for the LD operate in LC-3? 0010

59. what is load instruction used for? goes to a particular memory location, reads the value that is stored there and stores that value in one of the registers

60. how many operands does the LD instruction require and what are they? two operands; the value to be read from memory and the destination register that will contain the value after the instruction has completed processing 

61. There are many formulas that can be used for calculating the address of the memory location to be read. what is each formula called? an addressing mode.

62. what addressing mode does the LD instruction use? PC+OFFSET (computed by sign-extending the 2’s complement integer contained in bits [8:0] to 16 bits and adding it to the current contents of the program counter) see example43 

63. what system is used to control instruction processing in a step by step manner? the control unit

64. what is the entire sequence of steps needed to process an instruction called? instruction cycle

65. how many steps does the instruction cycle consist of? 6 sequential phases, each phase consisting of zero or more steps  

66. why do we say that each phase requires "zero" or more steps? we say zero steps to indicate that most computers have been designed that not all instructions require all 6 phases

67. name the six phases of the instruction life cycle? fetch, decode, evaluate address, fetch operands, execute, store result

68. what does the fetch stage involve? fetches the next instruction from memory and loads it in the IR of the control unit. Recall that a computer program consists of a number of instructions, that each instruction is represented by a sequence of bits, and that the entire program (in the von Neumann model) is stored in the computer’s memory. In order to carry out the work of an instruction, we must ﬁrst identify where it is. 

69. which structure contains the address of the next instruction to be processed? the program counter

70. what are the steps taken in the FETCH phase? First the MAR is loaded with the contents of the PC, and simultaneously  increment  the  PC; Next, the memory  is interrogated, which results in the next instruction being placed by the memory into the MDR; Finally, the IR is loaded with the contents of the MDR.

71. Each of these steps in the FETCH phase is under the direction of the control unit; how many clock cycles does each take? Step 1 takes one clock cycle. Step 2 could take one clock cycle or many clock cycles, depending on how long it takes to access the computer’s memory. Step 3 takes one clock cycle.

72. in a modern digital computer, how long is a clock cycle? a very small fraction of a second; indeed, a 3.1 GHz Intel Core i7 completes 3.1 billion clock cycles in one second. Said another way, one clock cycle takes 0.322 billionths of a second (0.322 nanoseconds).

73. what does the decode phase of the instruction life cycle do? examines an instruction to figure out what the micro architecture is being asked to do. 

74. how does the decode phase work? In the LC-3, a 4-to-16 decoder identiﬁes which of the 16 opcodes is to be processed (even though one of the 16 is not used) with the input being bits 15:12. The output line asserted is the one corresponding to the opcode at the input. Depending on which output of the decoder is asserted, the remaining 12 bits identify what else is needed to process that instruction.

75. what occurs in the evaluate address phase? computes the address of the memory location that is needed to process the instruction; not all instructions access memory to load or store data. For example, we have already seen that the ADD and AND instructions in the LC-3 obtain their source operands from registers or from the instruction itself and store the result of the ADD or AND instruction in a register. For those instructions, the EVALUATE ADDRESS phase is not needed. In contrast, the LD instruction causes a value stored in memory to be loaded into a register. The address was obtained by sign-extending bits [8:0] of the instruction to 16 bits and adding that value to the current contents of the PC. This calculation is performed during the EVALUATE ADDRESS phase. 

76. what occurs in the fetch operands phase? obtains the source operands needed to process the instruction. In the LD example, this phase took two steps: loading MAR with the address calculated in the EVALUATE ADDRESS phase and reading memory that resulted in the source operand being placed in MDR. In the ADD example, this phase consisted of obtaining the source operands from R2 and R6. In most current microprocessors, this phase (for the ADD instruction) can be done at the same time the instruction is being executed (the ﬁfth phase of the instruction cycle). Exactly how we can speed up the processing of an instruction in this way is a fascinating subject, but it is one we are forced to leave for later in your education.

77. what happens in the execute phase? carries out the execution of the instruction. 

78. what is notable about the store result phase? The result is written to its designated destination. In many computers, this action is performed during the EXECUTE phase. That is, in many computers, including the LC-3, an ADD instruction can fetch its source operands, perform the ADD in the ALU, and store the result in the destination register all in a single clock cycle. Hence a separate store result phase is not needed

79. Once the instruction cycle has been completed, the control unit begins the instruction cycle for the next instruction, starting from the top with the FETCH phase. Since the PC was updated during the previous instruction cycle, it contains at this point the address of the instruction stored in the next sequential memory location. Thus, the next sequential instruction is fetched next. Processing continues in this way until something breaks this sequential ﬂow, or the program ﬁnishes execution.

80. It is worth noting again that although the instruction cycle consists of six phases, not all instructions require all six phases. As already pointed out, the LC- 3 ADD instruction does not require a separate EVALUATE ADDRESS phase or a separate STORE RESULT phase. The LC-3 LD instruction does not require an EXECUTE phase. On the other hand, there are instructions in other ISAs that require all six phases. see example44

81. Everything we have said thus far happens when a computer program is executed in sequence. That is, the ﬁrst instruction is executed, then the second instruction is executed, followed by the third instruction, and so on.

82. so far you have come across 2 types of instructions; operate (ADD, AND) and data movement (LOAD), what is the 3rd type of instruction called? control instruction

83. what do control instructions do? they change the sequence of instruction execution

84. if you want to change the sequence of instructions executed, what must be done? the contents of the PC must change between the time it is incremented (during the FETCH phase of one instruction) and the start of the FETCH phase of the next instruction

85. at which stage do the control instructions load the PC with the new address? execute phase

86. what is the most common control instruction? conditional branching; which either changes the contents of the PC or does not change the contents of the PC, depending on the result of a previous instruction (usually the instruction that is executed immediately before the conditional branch instruction). see example450 and example451

87. what is the instruction cycle controlled by? a synchronous finite state machine; An abbreviated version of its state diagram, highlighting a few of the LC-3 instructions discussed in this chapter, is shown in Figure 4.4. Each state corresponds to one machine cycle of activity that takes one clock cycle to perform. The processing controlled by each state is described within the node representing that state. The arcs show the next state transitions.

88. Processing starts with State 1. The FETCH phase takes three clock cycles, corresponding to the three steps described earlier. In the ﬁrst clock cycle, the MAR is loaded with the contents of the PC, and the PC is incremented. In order for the contents of the PC to be loaded into the MAR (see Figure 4.3), the ﬁnite state machine must assert GatePC and LD.MAR. GatePC connects the PC to the processor bus. LD.MAR, the write enable signal of the MAR register, loads the contents of the bus into the MAR at the end of the current clock cycle. (Registers are loaded at the end of the clock cycle if the corresponding control signal is asserted.)

89. what happens in each of the 3 clock cycles of the FETCH phase? MAR <- PC and PC <- PC + 1; MDR <- MAR; IR <- MDR

90. In order for the PC to be incremented (again, see Figure 4.3), the ﬁnite state machine must assert the PCMUX select lines to choose the output of the box labeled +1 and must also assert the LD.PC signal to load the output of the PCMUX into the PC at the end of the current cycle. The ﬁnite state machine then goes to State 2. Here, the MDR is loaded with the instruction, which is read from memory.

91. In State 3, the instruction is transferred from the MDR to the instruction register (IR). This requires the ﬁnite state machine to assert GateMDR and LD.IR, which causes the IR to be loaded at the end of the clock cycle, concluding the FETCH phase of the instruction cycle.

92. The DECODE phase takes one clock cycle. In State 4, using the external input IR, and in particular the opcode bits of the instruction, the ﬁnite state machine can go to the appropriate next state for processing instructions depending on the particular opcode in IR [15:12]. Three of the 15 paths out of State 4 are shown. Processing continues clock cycle by clock cycle until the instruction completes execution, and the next state logic returns the ﬁnite state machine to State 1.

93. As has already been discussed, it is sometimes necessary not to execute the next sequential instruction but rather to access another location to ﬁnd the next instruction to execute. As we have said, instructions that change the ﬂow of instruction processing in this way are called control instructions. In the case of the conditional branch instruction (BR), at the end of its instruction cycle, the PC contains one of two addresses: either the incremented PC that was loaded in State 1 or the new address computed from sign-extending bits [8:0] of the BR instruction and adding it to the PC, which was loaded in State 63. Which address gets loaded into the PC depends on the test of the most recent result.

94. From everything we have said, it appears that the computer will continue processing instructions, carrying out the instruction cycle again and again, ad nauseum. Since the computer does not have the capacity to be bored, must this continue until someone pulls the plug and disconnects power to the computer?

95. what controls the execution of a user program in a computer? the OS

96. Operating systems are just computer programs themselves. As far as the computer is concerned, the instruction cycle continues whether a user program is being processed or the operating system is being processed. This is ﬁne as far as user programs are concerned since each user program terminates with a control instruction that changes the PC to again start processing the operating system—often to initiate the execution of another user program.

97. what can we do if we want to stop the infinite sequence of instruction cycles? stopping the clock which controls the transition from state cycle to a different state cycle

98. Figure 4.5a shows a block diagram of the clock circuit, consisting primarily of a clock generator and a RUN latch. The clock generator is a crystal oscillator, a piezoelectric device that you may have studied in your physics or chemistry class. For our purposes, the crystal oscillator is a black box that produces the oscillating voltage shown in Figure 4.5b. Every clock cycle, the voltage rises to 1.2 volts and then drops back to 0 volts.

99. If the RUN latch is in the 1 state (i.e., Q = 1), the output of the clock circuit is the same as the output of the clock generator. If the RUN latch is in the 0 state (i.e., Q = 0), the output of the clock circuit is 0. Thus, stopping the instruction cycle requires only clearing the RUN latch.

100. how do machines set this run latch to 0? in some older machines, the HALT instruction is executed; in the LC-3 the TRAP instruction (with opcode 1111 and an eight bit code called a trap vector x25) informs the OS that a program has finished executing and ergo the PC can stop executing instructions

101. What is misleading about the name program counter? The program counter does not maintain a count of any sort. 

100. Why is the name instruction pointer more insightful? The value stored in the program counter is the address of the next instruction 
to be processed. Hence the name ’Instruction Pointer’is more appropriate for it.

101. If a HALT instruction can clear the RUN latch, thereby stopping the instruction cycle, what instruction is needed to set the RUN 
latch, thereby reinitiating the instruction cycle? Once the RUN latch is cleared, the clock stops, so no instructions can be processed. 
Thus, no instruction can be used to set the RUN latch. In order to re-initiate the instruction cycle, an external input must be applied. 
This can be in the form of an interrupt signal or a front panel switch, for example.

chapter 5

1. what does the ISA specify? all the information about the computer that the sofware has to be aware of i.e. specifies everything in the computer that is available to a programmer when they write programs in the computer's own machine language. The ISA also specifies everything in the computer that is needed by someone (a compiler writer) who wishes to translate programs written in a high level language into the machine language of the computer

2. what 3 things does ISA specify? memory organisation, register set, and instruction set, including the opcodes, data types, and addressing modes of the instructions in the instruction set

3. The LC-3 memory has an address space of 2^(16) (i.e., 65,536) locations, and an addressability of 16 bits. Not all 65,536 addresses are actually used for memory locations. Since the normal unit of data that is processed in the LC-3 is 16 bits, we refer to 16 bits as one word, and we say the LC-3 is word-addressable.

4. why do computers such as the LC-3 use a set of registers? it takes more than one clock cycle to access an address in memory. registers can be accessed in one clock cycle. Each register in the set is called a general purpose register (GPR). Like memory locations, registers store information that can be operated on later. 

5. what do you call the number of bits stored in a register? a word. In the LC-3, this means 16 bits.

6. Registers must be uniquely identiﬁable. The LC-3 speciﬁes eight GPRs, each identiﬁed by a three-bit register number. They are referred to as R0, R1, … R7. Figure 5.1 shows a snapshot of the LC-3’s register set, sometimes called a register ﬁle, with the eight values 1, 3, 5, 7, −2, −4, −6, and −8 stored in R0, … R7, respectively. Figure 5.2 shows the contents of the register ﬁle of Figure 5.1 AFTER the instruction ADD  R2,  R1,  R0 is exucuted

7. what is an instruction made up of? opcode (what the instruction is asking the computer to do)
and its operands (what the computer is expected to do it to)

8. what is the instruction set defined by? by its set of opcodes, data types, and addressing modes

9. what does the addressing mode determine? where operands are located. The instruction ADD R2, R0, R1 has an opcode ADD, one addressing mode (register mode), and one data type (2’s complement integer). The instruction directs the computer to perform a 2’s complement integer addition and speci- ﬁes the locations (GPRs) where the computer is expected to ﬁnd the operands and the location (a GPR) where the computer is to write the result.

10. We saw in Chapter 4 that the ADD instruction can also have two addressing modes (register mode and immediate mode), where one of the two operands is literally contained in bits [4:0] of the instruction. Figure 5.3 lists all the instructions of the LC-3, the bit encoding [15:12] for each opcode, and the format of each instruction. 

11. how is the data type represented? as a representation of the operands in 0's and 1's or you can also say it is a representation of information such that the ISA has opcodes that operate on that representation

12. what does the memory organisation specified by an ISA entail? address space (quantity of memory locations) and addressability (amount of bits in each memory location)

13. Some ISAs have a very large number of opcodes, one for each of a very large number of tasks that a program may wish to carry out. The x86 ISA has more than 200 opcodes. Other ISAs have a very small set of opcodes. Some ISAs have specific opcodes to help with processing scientific calculations. For example, the Hewlett Packard Precision Architecture can specify the compound operation (A ⋅ B) + C with one opcode; that is, a multiply, followed by an add on three source operands A, B, and C.

14. Other ISAs have instructions that process video images obtained from the World Wide Web. The Intel x86 ISA added a number of instructions which they originally called MMX instructions because they eXtended the ISA to assist with MultiMedia applications that use the web. Still other ISAs have specific opcodes to help with handling the tasks of the operating system. For example, the VAX ISA, popular in the 1980s, used a single opcode instead of a long sequence of instructions that other computers used to save the information associated with a program that was in the middle of executing prior to switching to another program. 

15. The decision as to which instructions to include or leave out of an ISA is usually a hotly debated topic in a company when a new ISA is being specified. The LC-3 ISA has 15 instructions, each identiﬁed by its unique opcode. The opcode is speciﬁed in bits [15:12] of the instruction. Since four bits are used to specify the opcode, 16 distinct opcodes are possible. However, the LC-3 ISA speciﬁes only 15 opcodes. The code 1101 has been left unspeciﬁed, reserved for some future need that we are not able to anticipate today.

16. there are 3 different types of instructions, what are they? operate (process information), data movement (move information between memory and the registers and between registers/memory and input/output devices), and control (change the sequence of instructions that will be executed; that is, they enable the execution of an instruction other than the one that is stored in the next sequential location in memory)

17. A data type is a representation of information such that the ISA has opcodes that operate on that representation. There are many ways to represent the same information in a computer. That should not surprise us, since in our daily lives, we regularly represent the same information in many diﬀerent ways. 

18. In addition to the representation of a single number by diﬀerent bit patterns in diﬀerent data types, it is also the case that the same bit pattern can correspond to diﬀerent numbers, depending on the data type. For example, the 16 bits 0011000100110000 represent the 2’s complement integer 12,592, the ASCII code for 10 or a bit vector. This should also not surprise us, since in our daily lives, the same representation can correspond to multiple interpretations, as is the case with a red light. When you see it on the roadway while you are driving, it means you should stop. If you see a police car driving past with their lights flashing red, it means that they are on the way to respond to an incident that has occured.

19. how do opcodes interpret bit patterns of an operand? according to the data type it is designed to support. In the case of the ADD opcode, for example, the hardware will interpret the bit patterns of its operands as 2’s complement integers. Therefore, if a programmer stored the bit pattern 0011000100110000 in R3, thinking that the bit pattern represented the integer 10, the instruction ADD R4, R3, #10 would write the integer 12,602 into R4, and not the ASCII code for the integer 20. Why? Because the opcode ADD interprets the bit patterns of its operands as 2’s complement integers, and not ASCII codes, regardless what the person creating those numbers intended.

20. what is an addressing mode? is a mechanism for specifying where an operand is located.

21. An operand can generally be found in one of three places, what are they? in memory, in a register, or as part of an instruction

22. if an operand is part of an instruction, what do we refer to it as? a literal or an immediate operand

23. where does the term 'literal' come from? from the fact that the bits of the instruction literally form the operand

24. where does the term immediate come from? from the fact that we can obtain the operand immediately from the instruction i.e. we do Not have to look elsewhere for it

25. what are the 5 addressing modes supported by the LC-3? literal (or immediate), register, PC-relative, indirect, and base + offset

26. operate instructions use two addressing modes, what are they? register and immediate

27. how many addressing modes do data movement instructions use? 4 of 5 of the addressing modes

28. how many 3 single bit registers does the LC-3 have and what are they called? 3; they are referred to as N, Z, and P

29. what do N, Z, and P stand for? N stands for negative, Z stands for zero, and P stands for positive

30. how are these 3 single bit registers used? they are individually set (set to 1) or cleared (set to 0) each time one of the eight GPR's is written into as a result of execution of one of the operate instructions or one of the load instructions 

31. how do the operate and load instructions work with respect to the GPRs? each operate instruction performs a computation and writes the result into a GPR; each load instruction reads the contents of a memory location and writes the value found there into a general purpose register

32. what is the set of the 3 single bit registers referred to as? condition codes 

33. why are the 3 single bit registers referred to as condition codes? because the condition of the 3 bits is used to change the sequence of execution of the instructions in a computer program. Many ISAs use condition codes to change the execution sequence. 

34. The LC-3 has three operate instructions: ADD, AND, and NOT.

35. how many source operands does the NOT instruction operate on and what addressing mode does it use for both source and destination? one; register mode. The NOT instruction bit-wise complements a 16-bit source operand and stores the result in a destination register. NOT uses the register addressing mode for both its source and destination. Bits [8:6] specify the source register and bits [11:9] specify the destination register.

36. in the NOT instruction, what must bits 5:0 contain? all 1's

37. Figure 5.4 shows the key parts of the data path that are used to perform the NOT instruction shown here. Since NOT is a unary operation, only the A input of the ALU is relevant. It is sourced from R5. The control signal to the ALU directs the ALU to perform the bit-wise complement operation. The output of the ALU (the result of the operation) is stored in R3 and the condition codes are set, completing the execution of the NOT instruction.

38. Recall from Chapter 4 that the ADD (opcode = 0001) and AND (opcode = 0101) instructions both perform binary operations; they require two 16-bit source operands. The ADD instruction performs a 2’s complement addition of its two source operands. The AND instruction performs a bit-wise AND of each pair of bits of its two 16-bit operands. Like the NOT, the ADD and AND use the register addressing mode for one of the source operands and for the destina- tion operand. Bits [8:6] specify the source register, and bits [11:9] specify the destination register (where the result will be written).

39. what 2 addressing modes can be used to specify the second source operand for the AND and ADD instructions? register mode for one of the source operands and the destination operand; second operand is specified by either register mode or as an immediate operand

40. which bit determines which addressing mode will be used for the second operand in the AND and ADD instructions? bit 5

41. if bit 5 is 0 which addressing mode is used and what are bits 4:3 set to? register mode; set to 0

42. if bit 5 is 1 which addressing mode is used and what is done to bits 4:0? literal or immediate mode; they are sign extended to 16 bits

43. what must happen at the end of executing the AND and ADD Instruction (infact for load intructions as well)? the condition codes must be set

44. Figure 5.5 shows the key parts of the data path that are used to perform the instruction ADD  R1,  R4,  #-2.

45. what implication does using immediate addressing mode present? since the literal operand must fit into bits 4:0 of the instruction, not all 2 complement integers can be used as immediate operands

46. Which integers are OK (i.e., which integers can be used as immediate operands)? range of -16 through to 8 see example51, example52, example53

47. can a register can be used as a source and also as a destination in the same instruction? yes

48. what does the LEA instruction do? loads a register specified by bits 11:9 with an address specified by adding the PC + sign extended bits 8:0 of the instruction (the address is not accessed). Perhaps a better name for this opcode would be CEA (for Compute Eﬀective Address). If memory location x4018 contains the instruction LEA R5, #−3, and the PC contains x4018, R5 will contain x4016 after the instruction at x4018 is executed. Question: Why will R5 not contain the address x4015? because the PC is incremented before the offset is added to the incremented value

49. Figure 5.6 shows the relevant parts of the data path required to execute the LEA instruction. Note that the value to be loaded into the register does not involve any access to memory and nor does it have any eﬀect on the condition codes.

50. what do data movement instructions do? move information between GPRs and memory and between GPRs and input/output devices

51. what do you call the process of moving information from memory to a register? loading

52. what do you call the process of moving information from a register to a memory? storing

53. In both cases, the information in the location containing the source operand remains unchanged. In both cases, the location of the destination operand is over-written with the source operand, destroying in the process the previous value that was in the destination location.

54. how many instructions does the LC-3 contain for moving information? 6 (LD, LDR, LDI, ST, STR, and STI)

55. how many operands do data movement instructions require? two operands; a source and a destination. The source is the data to be moved; the destination is the location where it is moved to. One of these locations is a register, the other is a memory location or an input/output device. The format of the load and store instructions is as follows:

15  14  13  12  11  10  9  8  7  6  5  4  3  2  1  0
   opcode     | DR or SR |       Addr Gen bits

56. Bits [11:9] specify one of these operands, the register. If the instruction is a load, DR refers to the destination general purpose register that will contain the value after it is read from memory (at the completion of the instruction cycle). If the instruction is a store, SR refers to the register that contains the value that will be written to memory.
Bits [8:0] contain the address generation bits. That is, bits [8:0] contain information that is used to compute the 16-bit address of the second operand. 

57. in the case of LC-3, how many ways are there to interpret bits 8:0 for data movement instructions? 3 ways that are called addressing modes. The opcode speciﬁes how to interpret bits [8:0]. That is, the LC-3’s opcode speciﬁes which of the three addressing modes should be used to obtain the address of the operand from bits [8:0] of the instruction.

58. what two data movement instructions specify the PC-relative addressing mode? LD and ST

59. why is the PC-relative addressing mode named as such? because bits 8:0 of the instruction specify an offset relative to the PC

60. how is the memory address computed in pc-relative addressing mode? by sign extending bit 8:0 to 16 bits and adding the result to the incremented PC

61. in this case, what happens if the instruction is LD? the computed address (PC + offset) specifies the memory to be accessed. its contents are loaded into register specified by bits 11:9 of the instruction. If the following instruction is located at x4018, it will cause the contents of x3FC8 to be loaded into R2.

15  14  13  12  11  10  9   8  7  6  5  4  3  2  1  0
0   0   1   0 | 0   1   0 | 1  1  0  1  0  1  1  1  1
    LD             R2               X1AF

62. Figure 5.7 shows the relevant parts of the data path required to execute this instruction. The three steps of the LD instruction are identiﬁed. In step 1, the incremented PC (x4019) is added to the sign-extended value contained in IR [8:0] (xFFAF), and the result (x3FC8) is loaded into the MAR. In step 2, memory is read and the contents of x3FC8 is loaded into the MDR. Suppose the value stored in x3FC8 is 5. In step 3, the value 5 is loaded into R2, and the NZP condition codes are set, completing the instruction cycle.

63. what if the instruction is ST? the contents of the register specified by bits 11:9 of the instruction is written into the memory location whose address is PC + offset

64. what happens after either the LD or ST instruction has been executed? the N, Z, P condition codes are set depending on whether the value loaded is negative, positive, or zero.

65. what stands about the address generated in pc-relative addressing mode using bits 8:0? the range is limited to within ranges 256 and -256 f0r the ld and st instruction. If a load instruction needs to access a memory location further away from the load instruction, one of the other two addressing modes must be used. 

66. which two instructions use indirect addressing mode? LDI and STI

67. how are the LDI and STI operand addresses formed? the same way LD and ST are formed i.e. sign extending bits 8:0 from the instruction and adding it to the incremented value of the PC

68. how is the addressing mode of the LDI and STI instructions different from that of the LD and ST instructions? instead of this address being the address of the operand to be loaded or stored, it is the address of the address of the operand to be loaded or stored. hence the name indirect

69. what is the difference between the indirect mode and PC relative addressing modes in terms of the address of the operand in a computers memory? for indirect addressing mode, the address of the operand can be anywhere in the computers memory and not just within the range provided by bits 8:0 of the instruction

If the instruction

15  14  13  12  11  10  9   8  7  6  5  4  3  2  1  0
1   0   1   0 | 0   1   1 | 1  1  1  0  0  1  1  0  0
    LDI             R3               X1CC

is in x4A1B, and the contents of x49E8 is x2110, execution of this instruction results in the contents of x2110 being loaded into R3.

70. Figure 5.8 shows the relevant parts of the data path required to execute this instruction. As is the case with the LD and ST instructions, the ﬁrst step consists of adding the incremented PC (x4A1C) to the sign-extended value contained in IR [8:0] (xFFCC), and the result (x49E8) loaded into the MAR. In step 2, memory is in x4A1B and x2110 is in x49E8, and execution of this instruction results in the contents of x2110 being loaded into R3. In step 3, since x2110 is not the operand, but the address of the operand, it is loaded into the MAR. In step 4, memory is again read, and the MDR again loaded. This time the MDR is loaded with the contents of x2110. Suppose the value −1 is stored in memory location x2110. In step 5, the contents of the MDR (i.e., −1) is loaded into R3 and the NZP condition codes are set, completing the instruction cycle.

71. which two instructions use the base + offset addressing mode? LDR and STR

72. how does the base + offset addressing mode work? address of the operand is obtained by adding a sign extended six bit offset (5:0) to a base register specified by bits 8:6 

If R2 contains the 16-bit quantity x2345, the following instruction loads R1 with the contents of x2362.

15  14  13  12  11  10  9   8  7  6   5  4  3  2  1  0
0   1   1   0 | 0   0   1 | 0  1  0 | 0  1  1  1  0  1
    LDR             R1         R2          X1D

73. does the Base+oﬀset addressing mode also allow the address of the operand to be anywhere in 
the computer’s memory like indirect addressing mode? yes

74. Figure 5.9 shows the relevant parts of the data path required to execute this instruction. First the contents of R2 (x2345) is added to the sign-extended value contained in IR [5:0] (x001D), and the result (x2362) is loaded into the MAR. Second, memory is read, and the contents of x2362 is loaded into the MDR. Suppose the value stored in memory location x2362 is x0F0F. Third, and ﬁnally, the contents of the MDR (in this case, x0F0F) is loaded into R1 and the NZP condition codes are set, completing the execution of the LDR instruction.

75. Assume the contents of memory locations x30F6 through x30FC are as shown in Figure 5.10, and the PC contains x30F6. We will examine the eﬀects of carrying out the seven instructions starting at location x30FC. Since the PC points initially to location x30F6, the ﬁrst instruction to be executed is the one stored in location x30F6. The opcode of that instruction is 1110, load eﬀective address (LEA). LEA loads the register speciﬁed by bits [11:9] with the address formed by sign-extending bits [8:0] of the instruction and adding the result to the incremented PC. The 16-bit value obtained by sign-extending bits [8:0] of the instruction is xFFFD. The incremented PC is x30F7. Therefore, at the end of execution of the LEA instruction, R1 contains x30F4, and the PC contains x30F7.

76. Next, the instruction stored in location x30F7 is executed. Since the opcode 0001 speciﬁes ADD, the sign-extended immediate in bits [4:0] (since bit [5] is 1) is added to the contents of the register speciﬁed in bits [8:6], and the result is written to the register speciﬁed by bits [11:9]. Since the previous instruction wrote x30F4 into R1, and the sign-extended immediate value is x000E, the sum is x3102. At the end of execution of this instruction, R2 contains x3102, and the PC contains x30F8. R1 still contains x30F4.

77. Next, the instruction stored in x30F8. The opcode 0011 speciﬁes the ST instruction, which stores the contents of the register speciﬁed by bits [11:9] (R2) into the memory location whose address is computed using the PC-relative addressing mode. That is, the address is computed by adding the incremented PC (x30F9) to the 16-bit value obtained by sign-extending bits [8:0] of the instruction (xFFFB). Therefore, at the end of execution of the ST instruction, memory location x30F4 (i.e., x30F9 + xFFFB) contains the value stored in R2 (x3102) and the PC contains x30F9. 

78. Next the instruction at x30F9. The AND instruction, with an immediate operand x0000. At the end of execution, R2 contains the value 0, and the PC contains x30FA. At x30FA, the opcode 0001 speciﬁes the ADD instruction. After execution, R2 contains the value 5, and the PC contains x30FB.

79. At x30FB, the opcode 0111 signiﬁes the STR instruction. STR (like LDR) uses the Base+oﬀset addressing mode. The memory address is obtained by adding the contents of the BASE register (speciﬁed by bits [8:6]) to the sign- extended oﬀset contained in bits [5:0]. In this case, bits [8:6] specify R1, which contains x30F4. The 16-bit sign-extended oﬀset is x000E. Since x30F4 + x000E is x3102, the memory address is x3102. The STR instruction stores into x3102 the contents of the register speciﬁed by bits [11:9], in this case R2. Since R2 contains the value 5, at the end of execution of this instruction, M[x3102] contains the value 5, and the PC contains x30FC.

80. Finally the instruction at x30FC. The opcode 1010 speciﬁes LDI. LDI (like STI) uses the indirect addressing mode. The memory address is obtained by ﬁrst forming an address as is done in the PC-relative addressing mode. Bits [8:0] are sign-extended to 16 bits (xFFF7) and added to the incremented PC (x30FD). Their sum (x30F4) is the address of the operand address. Since M[x30F4] con- tains x3102, x3102 is the operand address. The LDI instruction loads the value found at this address (in this case 5) into the register identiﬁed by bits [11:9] of the instruction (in this case R3). At the end of execution of this instruction, R3 contains the value 5 and the PC contains x30FD.

81. what do control instructions do? they change the sequence of instructions to be executed otherwise the next instruction fetched would always be after the current instruction finishes in the next sequential memory location. As you know, this is because the PC is incremented in the FETCH phase of each instruction cycle.

82. how many opcodes does the LC-3 have that allow sequential execution flow to be broken? 5; conditional branch, unconditional jump, subroutine call (sometimes called function), TRAP, and RTI (return from trap or interrupt)

83. what does the TRAP instruction (often called service call) do? allows a programmer to get help from operating system to do things that the typical programmer does not understand how to do e.g. getting information from input device into the cpu, displaying information to output device. it breaks the sequential execution of a user program to start a sequence of instructions in the OS 

84. summarise how the conditional branch (BR which is opcode 0000) works? when executed it decides based on a test whether to execute the next instruction in memory or to jump to another instruction in a different part of the memory that was allocated to the program by the OS

85. what stage of instruction cycle does the BR instruction decide whether to load the PC with a new address? in the EXECUTE stage; if  nothing occurs in this stage then the incremented PC will remain unchanged and the next instruction executed will be the next instruction in the sequence, otherwise a new address will be loaded

86. what is the decision, whether to do nothing to the incremented PC or whether to change it, based on? it is based on the execution of previous instruction in the program which is reflected in the condition codes (the conditional branch's execute phase results in either doing nothing or it loads the PC with the address of the instruction it wishes to execute next) 

The format of the conditional branch instruction is as follows:

15  14  13  12  11  10  9   8  7  6  5  4  3  2  1  0
0   0   0   0 | n   z   p |          PCoﬀset
    

87. what is the format of the conditional branch instruction? bits 15:12 display the opcode which in this case is all 0's; bits 11:9 display the condition codes N, Z, P respectively; bits 8:0 are sign extended to make 16 bits which are then added to the incremented PC

88. in the LC-3, what instructions write into the GPR's and also set the condition codes? the operate instructions (ADD, AND, and NOT) and the three load instructions (LD, LDI, LDR)

89. what can you summise about the contents of bits 11:9 in the BR instruction and its operation? the BR instruction uses the information to determine whether to depart from the usual sequential execution of instructions that we get as a result of incrementing the PC during the FETCH phase of each instruction

90. how does the above happen? during the EXECUTE phase of the BR instruction cycle, the processor examines the condition codes whose associated bits in the instruction bits 11:9 are 1. if bit 11 is 1 then condition code N is examined; if bit 10 is 1 then condition code Z is examined; similar action is taken for bit 9. if any of the bits 11:9 are 0 then the associated condition codes are not examined. if any of the condition codes examined are set, then the PC is loaded with the address obtained in the EVALUTE ADDRESS phase. if none of the condition codes that are examined are set, the incremented PC is left unchanged, and the next sequential instruction will be fetched at the start of the next instruction cycle.

91. how is the address obtained during the EVALUATE ADDRESS phase generated? using PC-relative addressing mode

92. Suppose the following instruction is located at x4027, and the last value loaded into a general purpose register was 0

15  14  13  12  11  10  9   8  7  6  5  4  3  2  1  0
0   0   0   0 | 0   1   0 | 0  1  1  0  1  1  0  0  1        
     BR         n   z   p            x0D9 

93. Figure 5.11 shows the data path elements that are required to execute this instruction. Note the logic required to determine whether the sequential instruc- tion ﬂow should be broken. Each of the three AND gates corresponds to one of the three condition codes. The output of the AND gate is 1 if the corresponding condition code is 1 and if the associated bit in the instruction directs the hardware to check that condition code. If any of the three AND gates have an output 1, the OR gate has an output 1, indicating that the sequential instruction ﬂow should be broken, and the PC should be loaded with the address evaluated during the EVALUATE ADDRESS phase of the instruction cycle.

94. In the case of the conditional branch instruction at x4027, the answer is yes, and the PC is loaded with x4101, replacing x4028, which had been loaded into the PC during the FETCH phase of the BR instruction.

95. why is the uconditonal branch named so? because all bits 11:9 are set to 1 ergo all conditions are examined based on the output of the previous instruction. In this case, since the last result stored into a register had to be either negative, zero, or positive (there are no other choices!), one of the three condition codes must be in state 1. Since all three are examined, the PC is loaded with the address obtained in the EVALUATE ADDRESS phase. We call this an unconditional branch since the instruction ﬂow is changed unconditionally, that is, independent of the data.

For example, if the following instruction, located at x507B, is executed, the PC is loaded with x5001.

15  14  13  12  11  10  9   8  7  6  5  4  3  2  1  0
0   0   0   0 | 1   1   1 | 1  1  0  0  0  0  1  0  1        
     BR         n   z   p            x185 

96. there are two methods of loop control, name them: loop control with a counter and loop control with a sentinel

97. what is the difference between the two? a counter will control the number of times a loop executes by being decremented in each cycle and then checked to see if it is zero (conditional branch instruction). if not, the loop body is executed again; otherwise next instruction to be executed is after the last instruction of the loop body. on the other hand, sentinels are used when we do not know beforehand how many iterations we will want to perform; each iteration will be based on processing a value and when the sentinel is encountered the loop is broken

98. Loop Control with a Counter: Suppose we know that the 12 locations x3100 to x310B contain integers, and we wish to compute the sum of these 12 integers. A ﬂowchart for an algorithm to solve the problem is shown in Figure 5.12. First, as in all algorithms, we must initialize our variables. There are three such variables: the address of the next integer to be added (assigned to R1), the running sum (assigned to R3), and the number of integers left to be added (assigned to R2). The three variables are initialized as follows: The address of the ﬁrst integer to be added is put in R1. R3, which will keep track of the running sum, is initialized to 0. R2, which will keep track of the number of integers left to be added, is initialized to 12. Then the process of adding begins.

99. The program repeats the process of loading into R4 one of the 12 integers and adding it to R3. Each time we perform the ADD, we increment R1 so it will point to (i.e., contain the address of) the next number to be added and decrement R2 so we will know how many numbers still need to be added. When R2 becomes zero, the Z condition code is set, and we can detect that we are done.

100. The 10-instruction program shown in Figure 5.13 accomplishes the task. The details of the program execution are as follows: The program starts with PC = x3000. The ﬁrst instruction (at location x3000) initializes R1 with the address x3100. (The incremented PC is x3001; the sign-extended PCoﬀset is x00FF.). The instruction at x3001 clears R3. R3 will keep track of the running sum, so it must start with the value 0. As we said previously, this is called initializing the SUM to zero. The instructions at x3002 and x3003 initialize R2 to 12, the number of integers to be added. R2 will keep track of how many numbers have already been added. This will be done (by the instruction in x3008) by decrementing R2 after each addition takes place.

101. The instruction at x3004 is a conditional branch instruction. Note that bit [10] is a 1. That means that the Z condition code will be examined. If it is set, we know R2 must have just been decremented to 0. That means there are no more numbers to be added, and we are done. If it is clear, we know we still have work to do, and we continue with another iteration of the loop body.

102. The instruction at x3005 loads the next integer into R4, and the instruction at x3006 adds it to R3. The instruction at x3007 increments R1, so R1 will point to the next location in memory containing an integer to be added. The instruction at x3008 decrements R2, which is keeping track of the number of integers still to be added, and sets the condition codes.

103. The instruction at x3009 is an unconditional branch, since bits [11:9] are all 1. It loads the PC with x3004. It also does not aﬀect the condition codes, so the next instruction to be executed (the conditional branch at x3004) will be based on the instruction executed at x3008. The conditional branch instruction at x3004 examines the Z condition code. As long as Z is clear, the PC will not be aﬀected, and the next iteration of the loop body will begin. That is, the next instruction cycle will start with an instruction fetch from x3005.

104. The conditional branch instruction causes the execution sequence to follow: x3000, x3001, x3002, x3003, x3004, x3005, x3006, x3007, x3008, x3009, x3004, x3005, x3006, x3007, x3008, x3009, x3004, x3005, and so on. The loop body consists of the instructions at x3005 to x3009. When the value in R2 becomes 0, the PC is loaded with x300A, and the program continues at x300A with its next activity.

105. Loop Control with a Sentinel: This method is particularly eﬀective if we do not know ahead of time how many iterations we will want to perform. Each iteration is usually based on processing a value. We append to our sequence of values to be processed a value that we know ahead of time can never occur (i.e., the sentinel). For example, if we are adding a sequence of numbers, a sentinel could be a letter A or a *, that is, something that is not a number. Our loop test is simply a test for the occurrence of the sentinel. When we ﬁnd it, we know we are done.

106. Suppose we know the values stored in locations x3100 to x310B are all pos- itive. Then we could use any negative number as a sentinel. Let’s say the sentinel stored at memory address x310C is −1. The resulting ﬂowchart for this solution is shown in Figure 5.14, and the resulting program is shown in Figure 5.15.

107. As before, the instruction at x3000 loads R1 with the address of the ﬁrst value to be added, and the instruction at x3001 initializes R3 (which keeps track of the sum) to 0. At x3002, we load the contents of the next memory location into R4. If the sentinel is loaded, the N condition code is set. The conditional branch at x3003 examines the N condition code. If N=1, PC is loaded with x3008 and onto the next task. If N=0, R4 must contain a valid number to be added. In this case, the number is added to R3 (x3004), R1 is incremented to point to the next memory location (x3005), R4 is loaded with the contents of the next memory location (x3006), and the PC is loaded with x3003 to begin the next iteration (x3007).

108. what is the limitation of the conditional branch instruction? the next instruction that it loads the PC with (computed from sign-extending bits 8:0 and adding them to the incremented PC) can be at most +256 or -255 locations from the currently executing branch instruction

109. what if you want to execute an instruction that is 2000 locations from the BR instruction? you cannot use the BR instruction since you cannot fit 2000 into the 9 bit field 

110. what instruction provided by the LC-3 can do the above? the jmp instruction

111. how does it work? the jmp instruction loads the PC with the contents of the register specified by bits 8:6

If the following JMP instruction is located at address x4000

15  14  13  12  11  10  9   8  7  6   5  4  3  2  1  0
1   1   0   0 | 0   0   0 | 0  1  0 | 0  0  0  0  0  0        
     JMP                     BaseR

R2 contains the value x6600, and the PC contains x4000, then the instruction at x4000 (the JMP instruction) will be executed, followed by the instruction located at x6600. Since registers contain 16 bits (the full address space of memory), the JMP instruction has no limitation on where the next instruction to be executed must reside.

112. how does the TRAP instruction work? it changes the PC to a memory address that is part of the operating system so that the OS will perform some task on behalf of the executing program. In the language of operating system jargon, we say the TRAP instruction invokes an operating system service call.

113. Bits [7:0] of the TRAP instruction form the trapvector, an eight-bit code that identiﬁes the ser- vice call that the program wishes the operating system to perform on its behalf.

15  14  13  12  11  10  9  8   7  6  5  4  3  2  1  0
1   1   1   1 | 0   0   0  0 |     trapvector        
    
114. what happens after an os finishes executing a service call? Once the operating system is ﬁnished performing the service call, the program counter is set to the address of the instruction following the TRAP instruction, and the program continues. In this way, a program can, during its execution, request services from the operating system and continue processing after each such service is performed. 

115. what are the services that will be used in the lc-3? 
* Input a character from the keyboard (trapvector = x23).
* Output a character to the monitor (trapvector = x21). 
* Halt the program (trapvector = x25).

116. Suppose we would like to be able to input a character from the keyboard, then count the number of occurrences of that character in a ﬁle, and ﬁnally display that count on the monitor. We will simplify the problem by assuming that the number of occurrences of any character that we would be interested in is small enough that it can be expressed with a single decimal digit. That is, there will be at most nine occurrences. This simpliﬁcation allows us to not have to worry about complex conversion routines between the binary count and the ASCII display on the monitor. Figure 5.16 is a ﬂowchart of the algorithm that solves this problem. Note that each step is expressed both in English and also (in parentheses) in terms of an LC-3 implementation.

117. The ﬁrst step is (as always) to initialize all the variables. This means pro- viding starting values (called initial values) for R0, R1, R2, and R3, the four registers the computer will use to execute the program that will solve the prob- lem. R2 will keep track of the number of occurrences; in Figure 5.16, it is referred to as Count. It is initialized to zero. R3 will point to the next character in the ﬁle that is being examined. We refer to it as a pointer since it points to (i.e., contains the address of) the location where the next character of the ﬁle that we wish to examine resides. The pointer is initialized with the address of the ﬁrst character in the ﬁle. R0 will hold the character that is being counted; we will input that character from the keyboard and put it in R0. R1 will hold, in turn, each character that we get from the ﬁle being examined.

118. We should also note that there is no requirement that the ﬁle we are examining be close to or far away from the program we are developing. For example, it is perfectly reasonable for the program we are developing to start at x3000 and the ﬁle we are examining to start at x9000. If that were the case, in the initialization process, R3 would be initialized to x9000.

119. The next step is to count the number of occurrences of the input character. This is done by processing, in turn, each character in the ﬁle being examined, until the ﬁle is exhausted. Processing each character requires one iteration of a loop. We will use the sentinel method, using the ASCII code for EOT (End of Transmission) (00000100) as the sentinel. A table of ASCII codes is in Appendix E.

120. In each iteration of the loop, the contents of R1 is ﬁrst compared to the ASCII code for EOT. If they are equal, the loop is exited, and the program moves on to the ﬁnal step, displaying on the screen the number of occurrences. If not, there is work to do. R1 (the current character under examination) is compared to R0 (the character input from the keyboard). If they match, R2 is incremented. In either case, we move on to getting the next character. The pointer R3 is incremented, the next character is loaded into R1, and the program returns to the test that checks for the sentinel at the end of the ﬁle.

121. When the end of the file is reached, all the characters have been examined, and the count is contained as a binary number in R2. In order to display the count on the monitor, it is first converted to an ASCII code. Since we have assumed the count is less than 10, we can do this by putting a leading 0011 in front of the four-bit binary representation of the count. Note in Figure E.2 the relationship between the binary value of each decimal digit between 0 and 9 and its corresponding ASCII code. Finally, the count is output to the monitor, and the program terminates. Figure 5.17 is a machine language program that implements the ﬂowchart of Figure 5.16.

85. what are the basic components of the data path? the global bus, memory, the alu and the register file, the pc and the pcmux,
and the marmux

86. what does the LC-3 global bus consist of? 16 wires and associated electronics

87. what is the global bus used for? allows one structure to transfer upto 16 bits of information to another structure by making
the necessary electronic connections on the bus. exactly one value can be transferred on the bus at one time. each structure that 
supplies a signal to the bus has a triangle just behind its input arrow to the bus

88. what is this triangle called? the tri-state device

89. what is it used for? allows the computer's control logic to enable exactly one supplier to provide information
to the bus at one time. the structure wishing to obtain the value being supplied can do so by asserting its ld.x (load enable)
signal

90. how does memory work? it contains both instructions and data. it is accessed by loading the MAR with the address of the 
location to be accessed. control signals then read the contents of that memory location and the result of that read
is stored in the MDR

91. how does it work when it comes to storing? value to be stored in loaded into the MDR. the control signals then assert a WE signal
in order to store the value contained in the MDR in the memory location specified by the MAR

92. how does the ALU work? as the processing element, it has two inputs, source 1 from a register and source 2 from either
a register or the sign-extended immediate value provided by the instruction

93. what happens at the start of each instruction cycle? the PC supplies the MAR via the global bus the address of the instruction to be
fetched 

94. the PC is supplied via the three-to-one PCMUX. how is this three-to-one MUX configured? the rightmost input to the PCMUX is used to
increment and write into the PC by one; when a control signal such as BR is executed and the sign-extended bits 8:0 are added to the incremented
PC (addition takes place in a separate adder not the ALU), the output of this adder is the middle input to the PCMUX; the third input to the 
PCMUX is obtained from the global bus

95. what does the MARMUX component do? it controls which source out of two will supply the MAR with the appropriate address during
the execution of a load, a store, or a TRAP instruction. The right input to the MARMUX is obtained by adding either the incremented PC or
a base register to zero or a literal value supplied by the IR. Whether the PC or a base register and what literal value depends on which 
opcode is being processed. The control signal ADDR1MUX speciﬁes the PC or base register. The control signal ADDR2MUX speciﬁes which of four 
values is to be added.

chapter 6

1. what two things will you learn about in this chapter? develop a methodology for constructing
programs to solve problems and develop a methodology for fixing said programs under the 
likely condition that i did not get things right the first time

2. what is structured programming? methodology developed in the 60s to dramatically improve
the ability of average programmers to take a complex description of a problem and systematically
decompose it into smaller manageable units so that they could ultimately write a program 
that executed correctly

3. what is the methodology also called? systematic decomposition

4. define the systematic decomposition/ stepwise refinement process? process of taking A
unit of work and breaking it into smaller units of work such that the collection of smaller
units carries out the same task as the larger unit.

5. what is the essential idea behind systematic decomposition or stepwise refinement?
replace a larger unit of work with a construct that correctly decomposes it

6. how many constructs are there for doing this? 3

7. what are they called? sequential, conditional, and iterative

chapter 7

1. mechanical languages are generally partitioned into two classes. what are they? high level and low level

2. describe high level languaages? are more user friendly, almost resemble statements in natural languge such as english

3. what must happen before a program that is written in a high-level language can be executed? it must be translated into a program
in the ISA of the computer on which it is expected to execute

4. is it often the case that each statement in a high level language specifies several instructions in the ISA of a computer? yes

5. what do you call the small step up from the ISA of a machine? the ISAs assembly language

6. what is assembly language? a low level language which cannot be confused with a statement in english language. it is ISA dependent
i.e. each ISA has only one assembly language 

7. does each assembly language instruction usually specify a single instruction in the ISA? yes

8. what is the purpose of assembly language? to make the programming process more user friendly than programming in machine language
i.e. in the ISA of the computer you are working on while providing the programmer with detailed control over the instructions the
computer can execute

9. what does assembly language do that help the program avoid minutae? let us use mnemonic devices for opcodes and give meaningful 
symbolic names to memory locations (they are called symbolic addresses)

10. what do you lose when you use a high level language like C? whilst it is user friendly you relinquish control over which ISA instruction
are to be used to carry out the work specified by the high level statement

11. what are pseudo ops (assembler directives)? message from the programmer to the translation program to help in the translation 
process (translating a program represented by a string of characters in assembly language to machine language)

12. what is the translation program that is used to translate assembly programs to machine language? assembler

13. what is the translation process called? assembly

14. instead of an instruction being 16 0's and 1's as is the case of the LC-3 ISA, an instruction in assembly language consists of 
four parts. what are they called? label, opcode, operands, comment

15. which two are optional? label and comment

16. in LC-3 assembly language, why are symbolic names called labels are assigned to memory locations? so that we do not have to remember
explicit 16 bit addresses

17. for literal values in assembly language that are used as operands in assembly language, how will know a number is a decimal,
hex, or a binary? decimal numbers will be preceeded by #, binary by b, and hex by X

18. what are the two reasons for explicitly referring to a memory location using a label? the location is a target of a branch 
instruction and the location contains a value that is loaded or stored

19. how many pseudo-ops does the LC-3 assembly language contain? 5

20 what are they? .ORIG, .FILL, .BLKW, .STRINGZ, and .END.

21. what does the .ORIG pseudo-op do? it tells the assembler where in memory to place the LC-3 program i.e. first instruction is placed
in the memory addressed specified, whilst the rest of the instructions in the program are placed in subsequent sequential locations.

22. what does the .FILL pseudo-op do? tells the assembler to set aside the next location in the program and initialise it with the value
of the operand. the value can either be a number or a label

23. what does the .BLKW pseudo op do? tells the assembler to set aside some number of sequential memory locations in the program. The
actual number is the operand of the .BLKW pseudo op

24. what does the .STRINGZ pseudo op do? tells the assembler to initialise a sequence of n+1 memory locations. The argument is a 
sequence of n characters inside double quotation marks. first n words of memory are initialised with zero extended ascii codes of the
corresponding characters in the string. final word of memory is initialised to 0

25. what does the .END pseudo op do? tells the assembler it has reached the end of the program and need not look at anything after it

26. how does the assembly process unfold? it is done in two complete passes (from beginning to .END) through the entire assembly language
program. the objective of the first pass is to identify the actual binary addresses corresponding to the symbolic names (or labels).
this is set of correspondences is known as the symbol table. in pass 2, we translate the individual assembly language instructions
into their corresponding machine language instructions

27. why are labels used in assembly language? to refer to memory locations either because it is a target of a branch instruction or because
it contains data that must be loaded or stored

28. how does the lc-3 assembler keep track of the location assigned to  each instruction? by meaans of the location counter. it is 
initialised to the address specified in .ORIG

29. when a computer begins execution of a program, what is the entity being executed called? executable image

30. what is the executable image created from? from modules created independently by individual pogrammers where each module is 
translated separately into an object file

31. are all modules written by users? no, some are supplied as library routines by the operating system

32. what does each object file consist of? instructions in the ISA of the computer being used along with its associated data

33. what is normally the final step? combining (linking) all of the object modules together into one executable image

34. what happens during the execution of the program? the fetch, decode... instruction cycle is applied to instructions in the executable
image

35. what does the pseudo-op .EXTERNAL do? identifies the symbolic name of an address that is not known at the time a program is 
assembled. it sends a message to the LC-3 assembler that the absence of a symbolic label is not an error in the program i.e. the 
symbolic name is a label in another module that will be translated independently. it allows references by one module to symbolic 
references in another module without a problem

chapter 8

1. how many instructions does the call/return mechanism consist of? two instructions

2. what is the first instruction? JSR(R) is in the caller program

3. what does the JSR(R) instruction do? it loads the PC with the starting address of the subroutine and it loads R7 with the address
immediately after the address of the JSR(R) instruction. the address immediately after the address of the JSR(R) instruction is
the address to come back to after executing the subroutine

4. what name is given to the address we come back to? return linkage

5. what is the second instruction? JMP R7; which happens to be the laast instruction in the subroutine

6. what does this instruction do? it loads the PC with the contents of R7, the address just after the address of the JSR instruction
thereby completing the round trip flow of control from the caller to the callee and back

7. the LC-3 specifies one control instruction for calling subroutines; what is it called? JSR

8. how many addressing modes does the JSR(R) control instruction have for computing the starting address of a subroutine? 2

9. what are the addressing modes? PC-relative addressing and base register addressing

10. what are the two things that the JSR(R) instruction does? loads the PC hence overwriting the incremented PC that was loaded 
during the fetch phase of the JSR(R) instruction and then it saves the return address in R7

11. what is the return address that is saved in R7? it is the incremented PC which is the address of the instruction following the
JSR(R) instruction

12. how many parts does the JSR(R) instruction consist of? 3 parts

13. what are the 3 parts? bits 15:12 specify the opcode; bit 11 specifies what addressing mode will be used (1 for PC-relative); bits
10:0 are the address evaluation bits

14. how does the JSR instruction compute the target address of a subroutine? by sign-extending the 11 bit offset (bits 10:0) of the
instruction to 16 bits and adding it to the incremented PC

15. What other control instruction shares a addressing mode? the BR instruction except in this instruction 9 bits are sign extended 
instead of 11 bits

16. how does the JSRR instruction differ from the JSR instruction? the addressing mode used;  it obtains the starting address of A
subroutine in exactly the same way that the JMP instruction  does i.e. bits 8:6 identify the base register that contains the address 
to be loaded into the PC

17. what problems do  subroutines pose when it comes to registers? they overwrite the contents of a register and therefore if the 
contents that were overwritten are needed by the caller program then you have a major problem at hand

18. how is the above problem aavoided? by having either the subroutine save the contents of the registers it will overwrite and restore
them before it finishes executing or have the caller program save the contents of the registers it is using before it invokes a 
subroutine and restore the contents when the subroutine has finished executing

19. of the two options, which is more efficient? having the callee program (the subroutine) save the contents of the registers is
more efficient because it knows which registers it will overwrite. The caller program has no way of knowing this in advance

20. what is the technique called? callee save

21. is the caller program or the callee program responsible for saving and restoring the contents of R7? the caller is responsible 
since it alone is capable of overwriting the value in R7 after the subroutine returns

22. what do we call the above process? caller save

23. what are some of the uses of the call/return mechanism expanded on above? ability of a user program to call library routines
that are part of the OS

24. can the stack (abstract data type), be implemented in many different ways? yes

25. what makes stack special? LIFO i.e. specification of how it is to be accessed that is the last thing that you stored in a stack 
is the first thing that you remove from it

26. what is the definition of an abstract data type? a storage mechanism that is defined by the operations performed on it and not
by the specific manner in which it is implemented

27. how is a stack implemented in computer memory? consists of a sequence of memory locations along with a mechanism called a stack
pointer which keeps track of the top of the stack

28. how are values inserted into the stack stored? they are stored in memory locations having decreasing addresses i.e. the stack 
grows towards zero.

30. when values are pushed and popped to and from the stack implemented in sequential memory locations, does the data already stored
on the stack physically move? no

31. in the LC-3, what is process of pushing a value onto a stack? first load the value into R0, then decrement the value in R6 
(which contains the memory address of the stack pointer) 

32. in the LC-3, what is the process of popping a value from the stack? the value is read and the stack pointer is incremented

33. what is the fancy name given to the rules that the stack follows? stack protocol

34. what does attempting to pop items that have not been previously pushed result in? underflow

35. What happens when we run out of available space and we try to push a value onto the stack? result in an overflow
