Why read the book?
1. to understand fundamentals as they do not change very often
2. if fundamentals are mastered clearly, there is no limit to how high I will soar provided I continue to put in the
   work
3. a lot of problems arise from memorising technical details without understanding basic underpinnings
4. when it comes to learning, information hiding approach is prevalent. It is a useful productivity enhancer once 
   fundamentals are clearly understood. But until one gets to that point, information hiding gets in the way of
   understanding.
5. understanding not memorising
6. cutting through the layers: Professionals who use computers in systems today but remain ignorant of what is going
   on underneath are likely to discover the hard way that the eﬀectiveness of their solutions is impacted adversely 
   by things other than the actual programs they write. Serious programmers will write more eﬃcient code if they 
   understand what is going on beyond the statements in their high-level language. Consequently, the high-level 
   programming language course, where the compiler protects the student from everything “ugly” underneath, does not 
   serve most engineering students well, and it certainly does not prepare them for the future.

CHAPTER 1

There is no magic to computing. A computer is a determinisstic system. Everytime we hit it over the head in the same
way and in the same place (provided of course we give it the same starting conditions), we get the same response. A 
computer is not an electronic genius, infact it is an electronic idiot doing exactly what we tell it to do.

What appears to be a very complex organism is really just a very large, systematically interconnected collections of
very simple parts. This book will introduce those simple parts in a step by step manner and build the interconnected
structure that is known by the name computer.

Two themes permeate this book:

(a) the notion of abstraction: it is a technique of establishing a simpler way for a person to interact with a system
    removing details that are unnecessary for interaction with the system. To be successful with the notion of abstraction
    I must combine it with the ability to un-abstract (or deconstruct i.e. the ability to go from system to its 
    individual component parts). If we never have to combine a component with others in order to form a larger system
    and if nothing can go wrong with the component, then it is perfectly fine to understand this component at the level
    of its abstraction.
(b) the importance of not separating your mind the notions of hardware and software. Many computer scientists and 
    engineers refer to themselves as hardware people or software people. The implication is that the person knows a 
    whole lot about one of these two things and precious little about the other. Usually, there is the further 
    implication that it is OK to be an expert at one of these (hardware OR software) and clueless about the other.
    The power of abstraction allows us to “usually” operate at a level where we do not have to think about the 
    underlying layers all the time. This is a good thing. It enables us to be more productive. But if we are clueless
    about the underlying layers, then we are not able to take advantage of the nuances of those underlying layers 
    when it is very important to be able to.

    Always aim to work at a higher level of abstraction, but, if you are able to, at the same time, keep in mind the
    underlying levels, you will ﬁnd yourself able to do a much better job. 

    As you approach your study and practice of computing, we urge you to take the approach that hardware and software
    are names for components of two parts of a computing system that work best when they are designed by people who 
    take into account the capabilities and limitations of both.

    The Bottom Line We believe that whether your inclinations are in the direction of a computer hardware career or 
    a computer software career, you will be much more capable if you master both. 

Re-read the below points upon completion of the book and confirm that you understand all the points:

1. When you study data types, a software concept, in C (Chapter 12), you will understand how the ﬁnite word length 
   of the computer, a hardware concept, aﬀects our notion of data types.
2. When you study functions in C (Chapter 14), you will be able to tie the rules of calling a function with the 
   hardware implementation that makes those rules necessary.
3. When you study recursion, a powerful algorithmic device (initially in Chapter 8 and more extensively in Chapter 
   17), you will be able to tie it to the hardware. If you take the time to do that, you will better understand when
   the additional time to execute a procedure recursively is worth it.
4. When you study pointer variables in C (in Chapter 16), your knowledge of computer memory will provide a deeper 
   understanding of what pointers provide, and very importantly, when they should be used and when they should be 
   avoided.
5. When you study data structures in C (in Chapter 19), your knowledge of computer memory will help you better 
   understand what must be done to manipulate the actual structures in memory eﬃciently.

Computers have seen enormous and unparalleled leaps of performance in a relatively short time they have been around. 
After all, it wasn’t until the 1940s that the first computers showed their faces. One of the first computers was the
ENIAC (the Electronic Numerical Integrator and Calculator), a general purpose electronic computer that could be 
reprogrammed for different tasks. It was designed and built in 1943–1945 at the University of Pennsylvania by 
Presper Eckert and his colleagues.

It contained more than 17,000 vacuum tubes. It was approximately 8 feet high, more than 100 feet wide, and about 3 
feet deep (about 300 square feet of floor space). It weighed 30 tons and required 140 kW to operate. Figure 1.1 shows
three operators programming the ENIAC by plugging and unplugging cables and switches.

About 40 years and many computer companies and computers later, in the early 1980s, the Burroughs A series was born.
One of the dozen or so 18-inch boards that comprise that machine is shown in Figure 1.2. Each board contained 50 or 
more integrated circuit packages. Instead of 300 square feet, it took up around 50 to 60 square feet; instead of 30 
tons, it weighed about 1 ton, and instead of 140 kW, it required approximately 25 kW to operate.

Fast forward another 30 or so years and we ﬁnd many of today’s computers on desktops (Figure 1.3), in laptops 
(Figure 1.4), and most recently in smartphones (Figure 1.5). Their relative weights and energy requirements have 
decreased enormously, and the speed at which they process information has also increased enormously. We estimate 
that the computing power in a smartphone today (i.e., how fast we can compute with a smartphone) is more than four 
million times the computing power of the ENIAC!

The integrated circuit packages that comprise modern digital computers have also seen phenomenal improvement. An 
example of one of today’s microprocessors is shown in Figure 1.6. The ﬁrst microprocessor, the Intel 4004 in 1971, 
contained 2300 transistors and operated at 106 KHz. By 1992, those numbers had jumped to 3.1 million transistors at
a frequency of 66 MHz on the Intel Pentium microprocessor, an increase in both parameters of a factor of about 1000. Today’s microprocessors contain upwards of ﬁve billion transistors and can oper- ate at upwards of 4 GHz, another increase in both parameters of about a factor of 1000.

This factor of one million since 1971 in both the number of transistors and the frequency that the microprocessor 
operates at has had very important implications. The fact that each operation can be performed in one millionth of 
the time it took in 1971 means the microprocessor can do one million things today in the time it took to do one 
thing in 1971. The fact that there are more than a million times as many transistors on a chip means we can do a 
lot more things at the same time today than we could in 1971.

The result of all this is we have today computers that seem able to understand the languages people speak – English,
Spanish, Chinese, for example. We have computers that seem able to recognize faces. Many see this as the magic of 
artiﬁcial intelligence. We will see as we get into the details of how a computer works that much of what appears to 
be magic is really due to how blazingly fast very simple mindless operations (many at the same time) can be carried 
out.

Before we leave this ﬁrst chapter, there are two very important ideas that we would like you to understand, ideas 
that are at the core of what computing is all about.

Idea 1: All computers (the biggest and the smallest, the fastest and the slowest, the most expensive and the 
cheapest) are capable of computing exactly the same things if they are given enough time and enough memory. That is,
anything a fast computer can do, a slow computer can do also. The slow computer just does it more slowly. A more 
expensive computer cannot ﬁgure out something that a cheaper computer is unable to ﬁgure out as long as the cheaper 
computer can access enough memory. (You may have to go to the store to buy more memory whenever it runs out of memory
in order to keep increasing memory.) All computers can do exactly the same things. Some computers can do things 
faster, but none can do more than any other.

Idea 2: We describe our problems in English or some other language spoken by people. Yet the problems are solved by
electrons running around inside the computer. It is necessary to transform our problem from the language of humans 
to the voltages that inﬂuence the ﬂow of electrons. This transformation is really a sequence of systematic 
transformations, developed and improved over the last 70 years, which combine to give the computer the ability to 
carry out what appear to be some very complicated tasks. In reality, these tasks are simple and straightforward.

Before modern computers, there were many kinds of calculating machines. Some were analog machines—machines that 
produced an answer by measuring some physical quantity such as distance or voltage. For example, a slide rule is an
analog machine that multiplies numbers by sliding one logarithmically graded ruler next to another. The user can 
read a logarithmic “distance” on the second ruler. Some early analog adding machines worked by dropping weights on 
a scale. The diﬃculty with analog machines is that it is very hard to increase their accuracy.

This is why digital machines—machines that perform computations by manipulating a ﬁxed ﬁnite set of digits or 
letters— came to dominate computing. You are familiar with the distinction between analog and digital watches. An 
analog watch has hour and minute hands, and perhaps a second hand. It gives the time by the positions of its hands,
which are really angular measures. Digital watches give the time in digits. You can increase accuracy just by adding
more digits. For example, if it is important for you to measure time in hundredths of a second, you can buy a watch
that gives a reading like 10:35.16 rather than just 10:35. How would you get an analog watch that would give you an
accurate reading to one one-hundredth of a second? You could do it, but it would take a mighty long second hand! 
When we talk about computers in this book, we will always mean digital machines.

Before modern digital computers, the most common digital machines in the West were adding machines. In other parts 
of the world another digital machine, the abacus, was common. Digital adding machines were mechanical or 
electromechanical devices that could perform a speciﬁc kind of computation: adding integers. There were also digital
machines that could multiply integers. There were digital machines that could put a stack of cards with punched 
names in alphabetical order. The main limitation of all these machines is that they could do only one speciﬁc kind 
of computation. If you owned only an adding machine and wanted to multiply two integers, you had some 
pencil-and-paper work to do.

This is why computers are diﬀerent. You can tell a computer how to add numbers. You can tell it how to multiply. 
You can tell it how to alphabetize a list or perform any computation you like. When you think of a new kind of 
computation, you do not have to buy or design a new computer. You just give the old computer a new set of 
instructions (a program) to carry out the new computation. This is why we say the computer is a universal 
computational device.

Computer scientists believe that anything that can be computed, can be computed by a computer provided it has 
enough time and enough memory. When we study computers, we study the fundamentals of all computing.

The idea of a universal computational device is due to Alan Turing. Turing proposed in 1937 that all computations 
could be carried out by a particular kind of machine, which is now called a Turing machine. He gave a mathematical 
description of this kind of machine, but did not actually build one. Digital computers were not operating until 
several years later. Turing was more interested in solving a philosophical problem: deﬁning computation. 

How Do We Get the Electrons to Do the Work?

Figure 1.9 shows the process we must go through to get the electrons (which actually do the work) to do our bidding. 
We call the steps of this process the “Levels of Transformation.” As we will see, at each level we have choices. If
we ignore any of the levels, our ability to make the best use of our computing system can be very adversely aﬀected.

We describe the problems we wish to solve in a “natural language.” Natural lan- guages are languages that people 
speak, like English, French, Japanese, Italian, and so on. They have evolved over centuries in accordance with their
usage. They are fraught with a lot of things unacceptable for providing instructions to a computer. Most important 
of these unacceptable attributes is ambiguity. Natural language is ﬁlled with ambiguity. 

The ﬁrst step in the sequence of transformations is to transform the natural lan- guage description of the problem 
to an algorithm, and in so doing, get rid of the objectionable characteristics of the natural language. An 
algorithm is a step-by-step procedure that is guaranteed to terminate, such that each step is precisely stated and
can be carried out by the computer. There are terms to describe each of these properties.

1. effective computability means each step can be carried out by a computer.
2. finiteness means that a procedure terminates.
3. definiteness means that each step is precisely stated.

For every problem there are usually many diﬀerent algorithms for solving that problem. One algorithm may require 
the fewest steps. Another algorithm may allow some steps to be performed concurrently.

The next step is to transform the algorithm into a computer program in one of the programming languages that are 
available. Programming languages are “mechanical languages.” That is, unlike natural languages, mechanical languages
did not evolve through human discourse. Rather, they were invented for use in specifying a sequence of instructions
to a computer. 

There are two kinds of programming languages, high-level languages and low-level languages. High-level languages are
at a distance (a high level) from the underlying computer. At their best, they are independent of the computer on 
which the programs will execute. We say the language is “machine independent.”

Low-level languages are tied to the computer on which the programs will execute. There is generally one such 
low-level language for each computer. That language is called the assembly language for that computer.

The next step is to translate the program into the instruction set of the particular computer that will be used to 
carry out the work of the program. ISA specifies the interface between a computer program and the computer hardware
that will be responsible for executing the instructions. Opcode refers to instructions e.g mul and operand
refers to a data value that is operated on.

An analogy that may be helpful in understanding the concept of an ISA is provided by the automobile. Corresponding 
to a computer program, represented as a sequence of 0s and 1s in the case of the computer, is the human sitting in 
the driver’s seat of a car. Corresponding to the microprocessor hardware is the car itself. The “ISA” of the 
automobile is the speciﬁcation of everything the human needs to know to tell the automobile what to do, and 
everything the automobile needs to know to carry out the tasks speciﬁed by the human driver.

The ISA of a computer serves the same purpose as the “ISA” of an automobile, except instead of the driver and the
car, the ISA of a computer speciﬁes the interface between the computer program directing the computer hardware and 
the hardware carrying out those directions.

For example, consider the set of instructions that the computer can carry out—that is, what operations the computer
can perform and where to get the data needed to perform those operations. The term opcode is used to describe the 
operation. The term operand is used to describe individual data values. 

The ISA speciﬁes the acceptable representations for operands. They are called data types. A data type is a 
representation of an operand such that the computer can perform operations on that representation.

The ISA speciﬁes the mechanisms that the computer can use to ﬁgure out where the operands are located. These 
mechanisms are called addressing modes.

There are many ISA's in existence. Each specifies a different number of opcodes, operands and 
addressing modes from the rest.

The ISA also speciﬁes the number of unique locations that comprise the computer’s memory and the number of 
individual 0s and 1s that are contained in each location.

Many ISAs are in use today. The most widely known example is the x86, introduced by Intel Corporation in 1979 and 
currently also manufactured by AMD and other companies. Other ISAs and the companies responsible for them include 
ARM and THUMB (ARM), POWER and z/Architecture (IBM), and SPARC (Oracle).

The translation from a high-level language (such as C) to the ISA of the computer on which the program will execute
(such as x86) is usually done by a translating program called a compiler. To translate from a program written in C 
to the x86 ISA, one would need a C to x86 compiler. For each high-level language and each desired target ISA, one 
must provide a corresponding compiler.

The translation from the unique assembly language of a computer to its ISA is done by an assembler.

The next step is the implementation of the ISA, referred to as its microarchitecture. The microarchitecture (or 
implementation) of an automobile's ISA is about what goes on underneath the hood. 
Here all automobiles brands and models can be different depending on the cost/performance tradeoffs the 
designer made before the car was manufactured.

Previously, we identiﬁed ISAs of several computer manufacturers, including the x86 (Intel), the PowerPC (IBM and 
Motorola), and THUMB (ARM). Each has been implemented by many diﬀerent microarchitectures.

Each microarchitecture is an opportunity for computer designers to make different tradeoffs between the cost 
of the microprocessor, the performance that it will provide, and the energy that it will consume.

The next step is to implement each element of the microarchitecture out of simple logic circuits. Here also there 
are choices, as the logic designer decides how to best make the tradeoﬀs between cost and performance. So, for 
example, even for an operation as simple as addition, there are several choices of logic circuits to perform the 
operation at diﬀering speeds and corresponding costs.

Finally, each basic logic circuit is implemented in accordance with the requirements of the particular device 
technology used. So, CMOS circuits are diﬀerent from NMOS circuits, which are diﬀerent, in turn, from gallium 
arsenide circuits.

At each level of transformation there are choices on how to proceed which determine the resulting cost and 
performance of the computer.

----------------
end of chapter 1
----------------

chapter 2

We noted in Chapter 1 that the computer was organized as a system with several levels of transformation. A problem 
stated in a natural language such as English is actually solved by the electrons moving around inside the components
of the computer.

Inside the computer, millions of very tiny, very fast devices control the movement of those electrons. These 
devices react to the presence or absence of voltages in electronic circuits. They could react to the actual values 
of the voltages, rather than simply to the presence or absence of voltages. However, this would make the control and
detection circuits more complex than they need to be. It is much easier to detect simply whether or not a voltage 
exists at a point in a circuit than it is to measure exactly what that voltage is.

To be perfectly precise, it is not really the case that the computer diﬀerentiates the absolute absence of a voltage
(i.e., 0) from the absolute presence of a voltage (i.e., 1). Actually, the electronic circuits in the computer 
diﬀerentiate voltages close to 0 from voltages far from 0.

With one wire, one can differentiate only two things. One of them can be assigned the value 0, the other can be 
assigned the value 1. But to get useful work done by the computer, it is necessary to be able to differentiate a 
large number of distinct values, and to assign each of them a unique representation. 

We can accomplish this by combining many wires, that is, many bits. For example, if we use eight bits
(corresponding to the voltage present on each of eight wires), we can represent one particular value as 01001110, 
and another value as 11100111. In fact, if we are limited to eight bits we can differentiate at most only 256 values
(2^8) different things.

In general, with k bits, we can distinguish at most 2^k distinct items. Each pattern of these k bits is a code; that
is, it corresponds to a particular item (or value).

We symbolically represent the presence of a voltage as “1” and the absence of a voltage as “0.” We refer to each 0 
and each 1 as a “bit,” which is a shortened form of binary digit.

It is not enough simply to represent values; we must be able to operate on those values. We say a particular 
representation is a data type if there are operations in the computer that can operate on information that is 
encoded in that representation. Each instruction set architecture (ISA) has its own set of data types and its own 
set of instructions that can operate on those data types. 

In this book, we will mainly use two data types: 2’s complement integers for representing positive and negative 
integers that we wish to perform arithmetic on, and ASCII codes for representing characters that we wish to input 
to a computer via the keyboard or output from the computer via a monitor.

An unsigned integer has no sign (plus or minus) associated with it. An unsigned integer just has a magnitude. 
Unsigned.

a computer designer could assign any bit pattern to represent any integer he or she wants. Unfortunately, that 
could complicate matters when we try to build an electronic circuit to add two integers. Infact, signed magnitude and
1'complement data types require unnecessarily cumbersome hardware to do addition. On the other hand, the circuitry 
required to add 2 integers using 2's complement data type is much simpler. Because computer designers knew what it
would take to design a circuit to add two integers, they chose representations that simpliﬁed the circuit. The 
result is the 2’s complement data type, also shown in Figure 2.1. It is used on just about every computer 
manufactured today.

The choice of representations for the negative integers was based, as we said previously, on the wish to keep the 
logic circuits as simple as possible. Almost all computers use the same basic mechanism to perform addition. It is 
called an arithmetic and logic unit, usually known by its acronym ALU. An ALU has two inputs and one output. It 
performs addition by adding the binary bit patterns at its inputs, producing a bit pattern at its output that is 
the sum of the two input bit patterns.

What is particularly relevant is that the binary ALU does not know (and does not care) what the two patterns it is 
adding represent. It simply adds the two binary patterns. Since the binary ALU only ADDs and does not CARE, it would
be nice if our assignment of codes to the integers resulted in the ALU producing correct results when it added two 
integers.

For starters, it would be nice if, when the ALU adds the representation for an arbitrary integer to the 
representation of the integer having the same magnitude but opposite sign, the sum would be 0.

To accomplish that, the 2’s complement data type speciﬁes the representation for each negative integer so that when
the ALU adds it to the representation of the positive integer of the same magnitude, the result will be the 
representation for 0. Moreover, and actually more importantly, as we sequence through representations of −15 to +15,
the ALU is adding 00001 to each successive representation.

Note in particular the representations for −1 and 0, that is, 11111 and 00000. When we add 00001 to the 
representation for −1, we do get 00000, but we also generate a carry. That carry, however, does not inﬂuence the 
result. That is, the correct result of adding 00001 to the representation for −1 is 0, not 100000. Therefore, the 
carry is ignored. In fact, because the carry obtained by adding 00001 to 11111 is ignored, the carry can always be
ignored when dealing with 2’s complement arithmetic.

manual conversion of decimal fraction (e.g. 0.421) to binary requires the following steps:
see decimalFractionToBinary1.png and decimalFractionToBinary2.png

manual conversion of binary fraction to decimal requires the following steps: see binaryFractionToDecimal.png

manual conversion of decimal to 2's complement can be achieved by following these steps see 
decimalTo2scomplement.png

The value of a positive number does not change if we extend the sign bit 0 as many bit positions to the left 
as desired. Similarly, the value of a negative number does not change by extending the sign bit 1 as many bit 
positions to the left as desired. Since in both cases it is the sign bit that is extended, we refer to the 
operation as Sign-EXTension, often abbreviated SEXT. Sign-extension is performed in order to be able to 
operate on representations of diﬀerent lengths. It does not aﬀect the values of the numbers being represented.

overflow in unsigned arithmetic is relatively straightforward because it results in overflow of the msb 
meaning that the result is less than the value of one of the numbers that was added.

overflow in signed negative numbers during arithmetic is easy to test for because the msb overflows and becomes 0  
which means that the number becomes positive. In contrast, when adding two positive signed numbers, overflow occurs 
when the msb is turned on meaning that the result becomes negative.

Suppose we wish to know if two patterns are identical. Since the XOR function produces a 0 only if the corresponding
pair of bits is identical, two patterns are identical if the output of the XOR is all 0s.

An m-bit pattern where each bit has a logical value (0 or 1) independent of the other bits is called a bit vector. 

There are many other representations of information that are used in computers. Two that are among the most useful 
are the ﬂoating point data type and ASCII codes. 

Floating Point Data Type (Greater Range, Less Precision): Most ISAs today specify more than one ﬂoating point 
data type. One of them, usually called ﬂoat, consists of 32 bits, allocated as follows:

1 bit for the sign (positive or negative)
8 bits for the range (the exponent ﬁeld)
23 bits for precision (the fraction ﬁeld)

Normalised Form; the ﬂoating point data type represents numbers expressed in scientiﬁc notation, and mostly in 
normalized form see 32bitFloatingPoint.png: 

N = (−1)^S × 1.fraction × 2^(exponent−127), 1 ≤ exponent ≤ 254

where S, fraction, and exponent are binary numbers.

The computer’s 32-bit ﬂoating point data type consists of (a) a sign bit (positive or negative), (b) 24 binary
digits in normalised form (one non-zero binary digit to the left of the binary point) times (c) the radix 2 
raised to an exponent expressed in eight bits.

The sign bit S is just a single binary digit, 0 for positive numbers, 1 for negative numbers. The 23 fraction 
bits form the 24-bit quantity 1.fraction, where normalized form demands exactly one non-zero binary digit to 
the left of the binary point. Since there exists only one non-zero binary digit (i.e., the value 1), it is 
unnecessary to explicitly store that bit in our 32-bit ﬂoating point format. In fact that is how we get 24 
bits of precision, the 1 to the left of the binary point that is always present in normalised numbers and so 
is unnecessary to store, and the 23 bits of fraction that are actually part of the 32-bit data type.

We say mostly in normalised form because (as noted in the equation) the data type represents a ﬂoating point 
number in normalised form only if the eight-bit exponent is restricted to the 254 unsigned integer values, 
1 (00000001) through 254 (11111110).

As you know, with eight bits, one can represent 256 values uniquely. For the other two integer values 0 
(00000000) and 255 (11111111), the ﬂoating point data type does not represent normalised numbers.

A normalised number is a representation of a real number in scientific notation where:
1. One non-zero digit precedes the decimal point (mantissa).
2. The exponent is adjusted to ensure the mantissa has a specific range

The eight exponent bits are encoded in what we call an excess code, named for the notion that one can get the 
*real* exponent by treating the code as an unsigned integer and subtracting the excess (sometimes called the 
bias). In the case of the IEEE Floating Point that almost everyone uses, that excess (or bias) is 127 for 
32-bit ﬂoating point numbers.

Encoding is the process of converting data into a format required for various information processing needs, 
including transmission, storage, and retrieval. It involves the use of a code to change original data into a 
form that can be used by an external process. 

The exponent ﬁeld gives us numbers as large as 2^(+127) for an 
exponent ﬁeld containing 254 (11111110) and as small as 2^(−126)  for an exponent ﬁeld containing 1 (00000001).

What does the ﬂoating point data type 00111101100000000000000000000000 represent? see floatingPoint1.png 

How is the number −6 5/8  represented in the ﬂoating point data type? 
floatingPoint2.png 


The following three examples provide further illustrations of the interpretation of the 32-bit ﬂoating point 
data type according to the rules of the IEEE standard.

0  10000011  00101000000000000000000 is 1.00101 ⋅ 2^4  = 18.5

The exponent ﬁeld contains the unsigned number 131. Since 131 − 127 is 4, the exponent is +4. Combining a 1 to
the left of the binary point with the fraction ﬁeld to the right of the binary point yields 1.00101. If we 
move the binary point four positions to the right, we get 10010.1, which is 18.5.

1  10000010  00101000000000000000000 is −1 ⋅ 1.00101 ⋅ 2^3  =−9.25

The sign bit is 1, signifying a negative number. The exponent is 130, signifying an exponent of 130 − 127, or 
+3. Combining a 1 to the left of the binary point with the fraction ﬁeld to the right of the binary point 
yields 1.00101. Moving the binary point three positions to the right, we get 1001.01, which is −9.25.

0  11111110  11111111111111111111111

The sign is +. The exponent is 254 − 127, or +127. Combining a 1 to the left of the binary point with the 
fraction ﬁeld to the right of the binary point yields 1.11111111 … 1, which is approximately 2. Therefore, 
the result is approximately 2^128.

Infinities: We noted before that the ﬂoating point data type represented numbers expressed in scientiﬁc notation in 
normalised form provided the exponent ﬁeld does not contain 00000000 or 11111111. If the exponent ﬁeld 
contains 11111111, we use the ﬂoating point data type to represent various things, among them the notion of 
inﬁnity. Inﬁnity is represented by the exponent ﬁeld containing all 1s and the fraction ﬁeld containing all 0s.
We represent positive inﬁnity if the sign bit is 0 and negative inﬁnity if the sign bit is 1.

Subnormal Numbers

The smallest number that can be represented in normalised form is:

N = 1.00000000000000000000000 × 2^(−126)

What about numbers smaller than 2^(−126) but larger than 0? We call such numbers subnormal numbers because 
they cannot be represented in normalised form. The largest subnormal number is

N = 0.11111111111111111111111 × 2^(−126)

The smallest subnormal number is

N = 0.00000000000000000000001 × 2^(−126) i.e., 2^−23 × 2^−126 which is 2^−149 

Note that the largest subnormal number is 2^−126 minus 2^−149 . Do you see why that is the case? nope

Subnormal numbers are numbers of the form

N = (−1)^s × 0.fraction × 2^−126

We represent them with an exponent ﬁeld of 00000000. The fraction ﬁeld is represented in the same way as with 
normalized numbers. That is, if the exponent ﬁeld contains 00000000, the exponent is −126, and the signiﬁcant 
digits are obtained by starting with a leading 0, followed by a binary point, followed by the 23 bits of the 
fraction ﬁeld.

What number corresponds to the following ﬂoating point representation?

0  00000000  00001000000000000000000

Answer: The leading 0 means the number is positive. The next eight bits, a zero exponent, means the exponent 
is −126, and the bit to the left of the binary point is 0. The last 23 bits form the number 
0.00001000000000000000000, which equals 2^−5 . Thus, the number represented is 2^−5 ⋅ 2^−126 ,which is 2^−131.

Including subnormal numbers allows very, very tiny numbers to be represented.

ASCII Codes: Another representation of information is the standard code that almost all computer equipment 
manufacturers have agreed to use for transferring characters between the main computer processing unit and the
input and output devices. It (ASCII) greatly simpliﬁes the interface between a keyboard manufactured by one 
company, a computer made by another company, and a monitor made by a third company.

Each key on the keyboard is identiﬁed by its unique ASCII code. So, for example, the digit 3 is represented as
00110011, the digit 2 is 00110010, the lowercase e is 01100101, and the ENTER key is 00001101. When you type a
key on the keyboard, the corresponding eight-bit code is stored and made available to the computer.

Most keys are associated with more than one code. For example, the ASCII code for the letter E is 01000101, 
and the ASCII code for the letter e is 01100101. Both are associated with the same key, although in one case 
the Shift key is also depressed while in the other case, it is not.

In order to display a particular character on the monitor, the computer must transfer the ASCII code for that 
character to the electronics associated with the monitor. 

Describe what conditions indicate overﬂow has occurred when two unsigned numbers are added? Overflow has occurred in
an unsigned addition when you get a carry out of the leftmost bits.

Why does the sum of a negative 2’s complement number and a positive 2’s complement number never generate an overﬂow?
Because their sum will be a number which if positive, will have a lower magnitude (less positive) than the original
positive number (because a negative number is being added to it), and vice versa.

what are the masks used for? The masks are used to set bits (by ORing a 1) and to clear bits (by ANDing a 0).

If n and m are both four-bit 2’s complement numbers, and s is the four-bit result of adding them together, how can 
we determine, using only the logical operations described in Section 2.6, if an overﬂow occurred during the addition?
Develop a “procedure” for doing so. The inputs to the procedure are n, m, and s, and the output will be a bit 
pattern of all 0s (0000) if no overﬂow occurred and 1000 if an overﬂow did occur.

[(n AND m AND (NOT s)) OR ((NOT n) AND (NOT m) AND s)] AND 1000

Write IEEE ﬂoating point representation of the following decimal numbers.
a.  3.75             0 10000000 11100000000000000000000
b.  −55 23/64        1 10000100 10111010111000000000000
c.  3.1415927        0 10000000 10010010000111111011011
d.  64,000           0 10001110 11110100000000000000000

----------------------
end chapter 2
----------------------


chapter 3

Most processors these days are manufactured out of MOS transistors (metal oxide semiconductors). There are two
types: p type and n type

When the N-type transistor is supplied with 1.2 volts, the connection from source to drain acts like a piece of
wire. We say (in the language of electricity) that we have a short circuit between the source and drain. If the 
gate of the N-type transistor is supplied with 0 volts, the connection between the source and drain is broken. We 
say that between the source and drain we have an open circuit. see figure32

The P-type transistor works in exactly the opposite fashion from the N-type transistor. When the gate is supplied 
with 0 volts, the P-type transistor acts (more or less) like a piece of wire, closing the circuit. When the gate 
is supplied with 1.2 volts, the P-type transistor acts like an open circuit. Because the P-type and N-type
transistors act in this complementary way, we refer to circuits that contain both P-type and N-type transistors as 
CMOS circuits, for complementary metal-oxide semiconductor. see figure33

One step up from the transistor is the logic gate. That is, we construct basic logic structures out of individual 
MOS transistors.

The NOT Gate (Inverter): It is constructed from two MOS transistors, one P-type and one N-type. See Notgate.png 
shows the behavior of the circuit if the input is supplied with 0 volts. Note that the P-type transistor acts
like a short circuit and the N-type transistor acts like an open circuit. The output is, therefore, connected to 
1.2 volts. On the other hand, if the input is supplied with 1.2 volts, the P-type transistor acts like an open 
circuit, but the N-type transistor acts like a short circuit. The output in this case is connected to ground (i.e.,
0 volts). The complete behavior of the circuit can be described by means of a table, as shown in Figure 3.4c. If we 
replace 0 volts with the symbol 0 and 1.2 volts with the symbol 1, we have the truth table 

NOR Gates: a NOR gate contains two P-type and two N-type transistors see NORGate.png
Figure 3.5b shows the behavior of the circuit if A is supplied with 0 volts and B is supplied 
with 1.2 volts. In this case, the lower of the two P-type transistors produces an open circuit, 
and the output C is disconnected from the 1.2-volt power supply. However, the leftmost N-type 
transistor acts like a piece of wire, connecting the output C to 0 volts.

Note that if both A and B are supplied with 0 volts, the two P-type transistors
conduct, and the output C is connected to 1.2 volts. Note further that there is no
ambiguity here, since both N-type transistors act as open circuits, and so C is
disconnected from ground.

If either A or B is supplied with 1.2 volts, the corresponding P-type transistor
results in an open circuit. That is suﬃcient to break the connection from C to
the 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type 
transistors is suﬃcient to cause that transistor to conduct, resulting in C being
connected to ground (i.e., 0 volts).

If we replace the voltages with their logical equivalents, we have the truth
table of Figure 3.5d. 

OR Gate: If we augment the circuit of Figure 3.5a by adding an inverter at 
its output, as shown in Figure 3.6a see OR gate, we have at the output D the logical 
function OR. Figure 3.6a is the circuit for an OR gate. Figure 3.6b describes the behavior 
of this circuit if the input variable A is set to 0 and the input variable B is set to 1. 
Figure 3.6c shows the circuit’s truth table.

AND and NAND Gates: see AND.png if either A or B is supplied with 0 volts,
there is a direct connection from C to the 1.2-volt power supply. The fact that C
is at 1.2 volts means the N-type transistor whose gate is connected to C provides
a path from D to ground. Therefore, if either A or B is supplied with 0 volts, the
output D of the circuit of Figure 3.8 is 0 volts.

Again, we note that there is no ambiguity. The fact that at least one of the two
inputs A or B is supplied with 0 volts means that at least one of the two N-type
transistors whose gates are connected to A or B is open, and that consequently, C
is disconnected from ground. Furthermore, the fact that C is at 1.2 volts means
the P-type transistor whose gate is connected to C is open-circuited. Therefore,
D is not connected to 1.2 volts.

On the other hand, if both A and B are supplied with 1.2 volts, then both
of their corresponding P-type transistors are open. However, their corresponding
N-type transistors act like pieces of wire, providing a direct connection from C
to ground. Because C is at ground, the rightmost P-type transistor acts like a
closed circuit, forcing D to 1.2 volts.

Figure 3.8b is a truth table that summarizes the behavior of the circuit of
Figure 3.8a. Note that the circuit is an AND gate. The circuit shown within the
dashed lines (i.e., having output C) is a NOT-AND gate, which we generally
abbreviate as NAND.

The gates just discussed are very common in digital logic circuits and in
digital computers. There are billions of inverters (NOT gates) in Intel’s Skylake
microprocessor. As a convenience, we can represent each of these gates by standard symbols see symbols.png

The bubble shown in the inverter, NAND, and NOR gates signiﬁes the complement (i.e., NOT) 
function.

Gates with More Than Two Inputs: the notion of AND, OR, NAND, and NOR gates extends to larger numbers of inputs. One
could build a three-input AND gate or a four-input OR gate, for example. An n-input AND gate has an output value of
1 only if ALL n input variables have values of 1. If any of the n inputs has a value of 0, the output of the n-input 
AND gate is 0. An n-input OR gate has an output value of 1 if ANY of the n input variables has a value of 1. That 
is, an n-input OR gate has an output value of 0 only if ALL n input variables have values of 0. see figure310

Now that we understand the workings of the basic logic gates, the next step is to build some of the logic 
structures that are important components of the microarchitecture of a computer.

There are fundamentally two kinds of logic structures, those that include the storage of information and those that
do not.

Here we will deal with structures that do not store information. These structures are sometimes referred to as 
decision elements. Usually, they are referred to as combinational logic structures because their outputs are strictly
dependent on the combination of input values that are being applied to the structure right now. Their outputs are not
at all dependent on any past history of information that is stored internally, since no information can be stored 
internally in a combinational logic circuit.

Decoder: Figure 3.11 shows a logic gate implementation of a two-input decoder. A decoder
has the property that exactly one of its outputs is 1 and all the rest are 0s. The one
output that is logically 1 is the output corresponding to the input pattern that it is
expected to detect. In general, decoders have n inputs and 2^n outputs. We say the
output line that detects the input pattern is asserted. That is, that output line has
the value 1, rather than 0 as is the case for all the other output lines. In Figure 3.11,
note that for each of the four possible combinations of inputs A and B, exactly one
output has the value 1 at any one time. In Figure 3.11b, the input to the decoder
is 10, resulting in the third output line being asserted.

Mux: The function of a mux is to select one of the inputs (A or B) and connect it to the 
output. The select signal (S in Figure 3.12) determines which input is connected to the output.
Figure 3.12a shows a logic gate implementation of a two-input multiplexer, more commonly 
referred to as a mux.

The mux of Figure 3.12 works as follows: Suppose S = 0, as shown in
Figure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out-
put of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is
whatever the input A is. That is, if A = 0, then the output of the leftmost AND gate
is 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output
of the rightmost AND gate is 0, it has no eﬀect on the OR gate. Consequently,
the output at C is exactly the same as the output of the leftmost AND gate. The
net result of all this is that if S = 0, the output C is identical to the input A.
On the other hand, if S = 1, it is B that is ANDed with 1, resulting in the
output of the OR gate having the value of B.

In summary, the output C is always connected to either the input A or the
input B — which one depends on the value of the select line S. We say S selects the
source of the mux (either A or B) to be routed through to the output C. Figure 3.12c
shows the standard representation for a mux.

In general, a mux consists of 2^n inputs and n select lines. Figure 3.13a
shows a gate-level description of a four-input mux. It requires two select lines.
Figure 3.13b shows the standard representation for a four-input mux.
Question: Can you construct the gate-level representation for an eight-input
mux? yes. How many select lines must you have? 3

A One-Bit Adder (a.k.a. a Full Adder):

A simple algorithm for binary addition is to proceed as you have always done in the case of 
decimal addition, from right to left, one column at a time, adding the two digits from the two 
values plus the carry in, and generating a sum digit and a carry to the next column. The
only diﬀerence here (with binary addition) is you get a carry after 1, rather than
after 9.

Figure 3.14 is a truth table that describes the result of binary addition on one
column of bits within two n-bit operands. At each column, there are three values
that must be added: one bit from each of the two operands A and B and the carry
from the previous column. We designate these three bits as Ai , Bi , and Ci . There
are two results, the sum bit (Si) and the carry over to the next column, Ci+1 . Note
that if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e.,
Ci+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If
all three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and
a carry of 1.

Figure 3.15 shows a logic gate implementation of a one-bit adder. Note that each AND gate in Figure 3.15 produces 
an output 1 for exactly one of the eight input combinations of Ai, Bi, and Ci. The output of the OR gate for Ci+1 
must be 1 in exactly those cases where the corresponding input combinations in Figure 3.14 produce an output 1. 
Therefore, the inputs to the OR gate that generates Ci+1 are the outputs of the AND gates corresponding to those 
input combinations. Similarly, the inputs to the OR gate that generates Si are the outputs of the AND gates 
corresponding to the input combinations that require an output 1 for Si in the truth table of Figure 3.14.

Note that since the input combination 000 does not result in an output 1 for either Ci+1 or S, its corresponding 
AND gate is not an input to either of the two OR gates.

Figure 3.16 shows a circuit for adding two 4-bit binary numbers, using four of the one-bit adder circuits of Figure 
3.15. Note that the carry out of column i is an input to the addition performed in column i + 1.

If we wish to implement a logic circuit that adds two 16-bit numbers, we can do so with a circuit of 16 one-bit 
adders.

We should point out that historically the logic circuit of Figure 3.15 that provides three inputs (Ai, Bi, and Ci) 
and two outputs (the sum bit Si and the carry over to the next column Ci+1) has generally been referred to as a 
full adder to diﬀerentiate it from another structure, which is called a half adder. The distinction between the two 
is the carry bit. Note that the carry into the rightmost column in Figure 3.16 is 0. That is, in the rightmost 
circuit, S0 and C1 depend only on two inputs, A0 and B0. Since that circuit depends on only two inputs, it has been
referred to as a half adder. Since the other circuits depend on all three inputs, they are referred to as full 
adders. We prefer the term one-bit adder as a simpler term for describing what is happening in each column.

The Programmable Logic Array (PLA)

Figure 3.17 illustrates a very common building block for implementing any collection of logic functions one wishes 
to implement. The building block is called a programmable logic array (PLA). It consists of an array of AND gates 
(called an AND array) followed by an array of OR gates (called an OR array). The number of AND gates corresponds 
to the number of input combinations (rows) in the truth table. For n-input logic functions, we need a PLA with 2^n
n-input AND gates. In Figure 3.17, we have 2^3 three-input AND gates, corresponding to three logical input variables.
The number of OR gates corresponds to the number of logic functions we wish to implement, that is, the number of 
output columns in the truth table. The implementation algorithm is simply to connect the output of an AND gate to 
the input of an OR gate if the corresponding row of the truth table produces an output 1 for that output column. 
Hence the notion of programmable. That is, we say we program the connections from AND gate outputs to OR gate 
inputs to implement our desired logic functions

Figure 3.15 shows seven AND gates connected to two OR gates since our requirement was to implement two functions 
(sum and carry) of three input variables. Figure 3.17 shows a PLA that can implement any four functions of three 
variables by appropriately connecting AND gate outputs to OR gate inputs. That is, any function of three variables 
can be implemented by connecting the outputs of all AND gates corresponding to input combinations for which the 
output is 1 to inputs of one of the OR gates. Thus, we could implement the one-bit adder by programming the two OR 
gates in Figure 3.17 whose outputs are W and X by connecting or not connecting the outputs of the AND gates to the 
inputs of those two OR gates as speciﬁed by the two output columns of Figure 3.14.

Logical Completeness

We saw that any logic function we wished to implement could be accomplished with a PLA. We saw that the PLA 
consists of only AND gates, OR gates, and inverters. That means that any logic function can be implemented, 
provided that enough AND, OR, and NOT gates are available. We say that the set of gates {AND, OR, NOT} is logically 
complete because we can build a circuit to carry out the speciﬁcation of any truth table we wish without using any 
other kind of gate. That is, the set of gates {AND, OR, and NOT} is logically complete because a barrel of AND 
gates, a barrel of OR gates, and a barrel of NOT gates are suﬃcient to build a logic circuit that carries out the 
speciﬁcation of any desired truth table. The barrels may have to be big, but the point is, we do not need any other 
kind of gate to do the job.

Question: Is there any single two-input logic gate that is logically complete? no. For example, is the NAND gate 
logically complete? no. Hint: Can I implement a NOT gate with a NAND gate? no. If yes, can I then implement an AND gate 
using a NAND gate followed by a NOT gate? no. If yes, can I implement an OR gate using just AND gates and NOT gates? no

If all of the above is true, then the NAND gate is logically complete, and I can implement any desired logic 
function as described by its truth table with a barrel of NAND gates.

Now we are ready to discuss logic structures that do include the storage of information.


The R-S Latch

A simple example of a storage element is the R-S latch. It can store one bit of information, a 0 or a 1. The R-S 
latch can be implemented in many ways, the simplest being the one shown in Figure 3.18. Two 2-input NAND gates are 
connected such that the output of each is connected to one of the inputs of the other. The remaining inputs S and R 
are normally held at a logic level 1. 

The R-S latch gets its name from the old designations for setting the latch to store a 1 and setting the latch to 
store a 0. Setting the latch to store a 1 was referred to as setting the latch, and setting the latch to store a 0 
was referred to as resetting the latch. Ergo, R-S.

The Quiescent State: We describe the quiescent (or quiet) state of a latch as the state when the latch is storing a 
value, either 0 or 1, and nothing is trying to change that value. This is the case when inputs S and R both have the
logic value 1. In Figure 3.18 the letter a designates the value that is currently stored in the latch, which we also
refer to as the output of the latch.

Consider ﬁrst the case where the value stored and therefore the output a is 1. Since that means the value A is 1 
(and since we know the input R is 1 because we are in the quiescent state), the NAND gate’s output b must be 0. 
That, in turn, means B must be 0, which results in the output a equal to 1. As long as the inputs S and R remain 1, 
the state of the circuit will not change. That is, the R-S latch will continue to store the value 1 (the value of 
the output a).

If, on the other hand, we assume the output a is 0, then A must be 0, and the output b must be 1. That, in turn, 
results in B equal to 1, and combined with the input S equal to 1 (again due to quiescence), results in the output 
a equal to 0. Again, as long as the inputs S and R remain 1, the state of the circuit will not change. In this 
case, we say the R-S latch stores the value 0.

Setting the Latch to a 1 or a 0: The latch can be set to 1 by momentarily setting S to 0, provided we keep the value 
of R at 1. Similarly, the latch can be set to 0 by momentarily setting R to 0, provided we keep the value of S at 1. 
In order for the R-S latch to work properly, both S and R must never be allowed to be set to 0 at the same time.

We use the term set to denote setting a variable to 0 or 1, as in “set to 0” or “set to 1.” In addition, we often 
use the term clear to denote the act of setting a variable to 0.

If we set S to 0 for a very brief period of time, this causes a to equal 1, which in turn causes A to equal 1. Since
R is also 1, the output at b must be 0. This causes B to be 0, which in turn makes a equal to 1. If, after that very
brief period of time, we now return S to 1, it does not aﬀect a. Why? Answer: Since B is also 0, and since only one 
input 0 to a NAND gate is enough to guarantee that the output of the NAND gate is 1, the latch will continue to 
store a 1 long after S returns to 1.

In the same way, we can clear the latch (set the latch to 0) by setting R to 0 for a very short period of time. 

We should point out that if both S and R were allowed to be set to 0 at the same time, the outputs a and b would 
both be 1, and the ﬁnal state of the latch would depend on the electrical properties of the transistors making up 
the gates and not on the logic being performed. How the electrical properties of the transistors would determine the
ﬁnal state in this case is a subject we will have to leave for a later semester. :-(

Finally, we should note that when a digital circuit is powered on, the latch can be in either of its two states, 0 
or 1. It does not matter which state since we never use that information until after we have set it to 1 or 0.

The Gated D Latch

To be useful, it is necessary to control when a latch is set and when it is cleared. A simple way to accomplish this
is with the gated latch.

Figure 3.19 shows a logic circuit that implements a gated D latch. It consists of the R-S latch of Figure 3.18, plus
two additional NAND gates that allow the latch to be set to the value of D, but only when WE is asserted (i.e., when
WE equals 1). WE stands for write enable. When WE is not asserted (i.e., when WE equals 0), the outputs S and R are 
both equal to 1. Since S and R are inputs to the R-S latch, if they are kept at 1, the value stored in the latch 
remains unchanged, as explained earlier When WE is momentarily set to 1, exactly one of the outputs S or R is set to
0, depending on the value of D. If D equals 1, then S is set to 0. If D equals 0, then both inputs to the lower NAND 
gate are 1, resulting in R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R is 
set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according to whether D is 1 or 0. When WE 
returns to 0, S and R return to 1, and the value stored in the R-S latch persists.

The Concept of Memory

We now have all the tools we need to describe one of the most important structures in the electronic digital computer,
its memory.

Memory is made up of a (usually large) number of locations, each uniquely identiﬁable and each having the ability to
store a value. We refer to the unique identiﬁer associated with each memory location as its address. We refer to the
number of bits of information stored in each location as its addressability.

Address Space: We refer to the total number of uniquely identiﬁable locations as the memory’s address space. A 2 GB 
memory, for example, refers to a memory that consists of two billion uniquely identiﬁable memory locations.

Actually, the number two billion is only an approximation, due to the way we specify memory locations. Since 
everything else in the computer is represented by sequences of 0s and 1s, it should not be surprising that memory 
locations are identiﬁed by binary addresses as well. With n bits of address, we can uniquely identify 2^n locations.
Ten bits provide 1024 locations, which is approximately 1000. If we have 20 bits to represent each address, we have 
2^20 uniquely identiﬁable locations, which is approximately one million. With 30 bits, we have 2^30 locations, which
is approximately one billion. In the same way we use the preﬁxes “kilo” to represent 2^10 (approximately 1000) and 
“mega” to represent 2^20 (approximately one million), we use the preﬁx “giga” to represent 2^30  (approximately one 
billion). Thus, 2 giga really corresponds to the number of uniquely identiﬁable locations that can be speciﬁed with 
31 address bits. We say the address space is 2^31, which is exactly 2,147,483,648 locations, rather than 
2,000,000,000, although we colloquially refer to it as two billion.

Addressability

The number of bits stored in each memory location is the memory’s addressability. A 2-gigabyte memory (written 2GB)
is a memory consisting of 2,147,483,648 memory locations, each containing one byte (i.e., eight bits) of storage. 
Most memories are byte-addressable.

The reason is historical; most computers got their start processing data, and one character stroke on the keyboard 
corresponds to one 8-bit ASCII code. If the memory is byte-addressable, then each ASCII character occupies one 
location in memory. Uniquely identifying each byte of memory allows individual bytes of stored information to be 
changed easily.

Many computers that have been designed speciﬁcally to perform large scientiﬁc calculations are 64-bit addressable. 
This is due to the fact that numbers used in scientiﬁc calculations are often represented as 64-bit ﬂoating-point 
quantities.

Since scientiﬁc calculations are likely to use numbers that require 64 bits to represent them, it is reasonable to 
design a memory for such a computer that stores one such number in each uniquely identiﬁable memory location.

A 2^2-by-3-Bit Memory

Figure 3.20 illustrates a memory of size 2^2 by 3 bits. That is, the memory has an address space of four locations 
and an addressability of three bits. A memory of size 2^2 requires two bits to specify the address. We describe the 
two-bit address as A[1:0]. A memory of addressability three stores three bits of information in each memory location. 
We describe the three bits of data as D[2:0]. In both cases, our notation A[high:low] and D[high:low] reﬂects the 
fact that we have numbered the bits of address and data from right to left, in order, starting with the rightmost 
bit, which is numbered 0. The notation [high:low] means a sequence of high − low + 1 bits such that “high” is the 
bit number of the leftmost (or high) bit number in the sequence and “low” is the bit number of the rightmost (or low) 
bit number in the sequence.

Accesses of memory require decoding the address bits. Note that the address decoder takes as input the address bits 
A[1:0] and asserts exactly one of its four outputs, corresponding to the word line being addressed. In Figure 3.20, 
each row of the memory corresponds to a unique three-bit word, thus the term word line. Memory can be read by 
applying the address A[1:0], which asserts the word line to be read. Note that each bit of the memory is ANDed with 
its word line and then ORed with the corresponding bits of the other words. Since only one word line can be asserted
at a time, this is eﬀectively a mux with the output of the decoder providing the select function to each bit line. 
Thus, the appropriate word is read at D[2:0].

Figure 3.21 shows the process of reading location 3. The code for 3 is 11. The address A[1:0]=11 is decoded, and the
bottom word line is asserted. Note that the three other decoder outputs are not asserted. That is, they have the 
value 0. The value stored in location 3 is 101. These three bits are each ANDed with their word line producing the 
bits 101, which are supplied to the three output OR gates. Note that all other inputs to the OR gates are 0, since 
they have been produced by ANDing with their unasserted word lines. The result is that D[2:0] = 101. That is, the 
value stored in location 3 is output by the OR gates. Memory can be written in a similar fashion. The address 
speciﬁed by A[1:0] is presented to the address decoder, resulting in the correct word line being asserted. With 
write enable (WE) also asserted, the three bits D[2:0] can be written into the three gated latches corresponding to 
that word line.

Sequential Logic Circuits:

In this section, we discuss digital logic structures that can both process information (i.e., make decisions) and 
store information. That is, these structures base their decisions not only on the input values now present, but also
(and this is very important) on what has happened before. These structures are usually called sequential logic 
circuits. They are distinguishable from combinational logic circuits because, unlike combinational logic circuits,
they contain storage elements that allow them to keep track of prior history information.

Figure 3.22 shows a block diagram of a sequential logic circuit. Note the storage elements. Note also that the 
output can be dependent on both the inputs now and the values stored in the storage elements. The values stored in 
the storage elements reﬂect the history of what has happened before.

Sequential logic circuits are used to implement a very important class of mechanisms called ﬁnite state machines. We
use ﬁnite state machines in essentially all branches of engineering. For example, they are used as controllers of 
electrical systems, mechanical systems, and aeronautical systems. A traﬃc light controller that sets the traﬃc light
to red, yellow, or green depends on the light that is currently on (history information) and input information from 
sensors such as trip wires on the road, a timer keeping track of how long the current light has been on, and perhaps
optical devices that are monitoring traﬃc.

A Simple Example: The Combination Lock

A simple example shows the diﬀerence between combinational logic structures and sequential logic structures. Suppose
one wishes to secure a bicycle with a lock, but does not want to carry a key. A common solution is the combination 
lock. The person memorizes a “combination” and uses it to open the lock. Two common types of locks are shown in 
Figure 3.23.

In Figure 3.23a, the lock consists of a dial, with the numbers from 0 to 30 equally spaced around its circumference.
To open the lock, one needs to know the “combination.” One such combination could be: R13-L22-R3. If this were the 
case, one would open the lock by turning the dial two complete turns to the right (clockwise), and then continuing 
until the dial points to 13, followed by one complete turn to the left (counter clockwise), and then continuing until
the dial points to 22, followed by turning the dial again to the right (clockwise) until it points to 3. At that 
point, the lock opens. What is important here is the sequence of the turns. The lock will not open, for example if 
one performed two turns to the right, and then stopped on 22 (instead of 13), followed by one complete turn to the 
left, ending on 13, followed by one turn to the right, ending on 3. That is, even though the ﬁnal position of the 
dial is 3, and even though R22-L13-R3 uses the same three numbers as the combination R13-L22-R3, the lock would not 
open. Why? Because the lock stores the previous rotations and makes its decision (open or don’t open) on the basis of
the the history of the past operations, that is, on the correct sequence being performed.

Another type of lock is shown in Figure 3.23b. The mechanism consists of (usually) four wheels, each containing the 
digits 0 through 9. When the digits are lined up properly, the lock will open. In this case, the combination is the 
set of four digits. Whether or not this lock opens is totally independent of the past rotations of the four wheels. 
The lock does not care at all about past rotations. The only thing important is the current value of each of the 
four wheels. This is a simple example of a combinational structure.

It is curious that in our everyday speech, both mechanisms are referred to as “combination locks.” In fact, only the
lock of Figure 3.23b is a combinational lock. The lock of Figure 3.23a would be better called a sequential lock!

3.6.2 The Concept of State

For the mechanism of Figure 3.23a to work properly, it has to keep track of the sequence of rotations leading up to 
the opening of the lock. In particular, it has to diﬀerentiate the correct sequence R13-L22-R3 from all other 
sequences. For example, R22-L13-R3 must not be allowed to open the lock. Likewise, R10-L22- R3 must also not be 
allowed to open the lock.

For the lock of Figure 3.23a to work, it must identify several relevant situations, as follows:

A.  The  lock  is  not  open,  and  NO  relevant  operations  have  been performed.
B.  The  lock  is  not  open,  but  the  user  has  just  completed  the R13  operation.
C.  The  lock  is  not  open,  but  the  user  has  just  completed  R13, followed  by  L22.
D.  The  lock  is  open,  since  the  user  has  just  completed  R13, followed  by  L22,  followed  by  R3.

We have labeled these four situations A, B, C, and D. We refer to each of these situations as the state of the lock.
The notion of state is a very important concept in computer engineering, and actually, in just about all branches of
engineering. The state of a mechanism — more generally, the state of a system —is a snapshot of that system in which 
all relevant items are explicitly expressed.

That is: The state of a system is a snapshot of all the relevant elements of the system at the moment the snapshot 
is taken.

In the case of the lock of Figure 3.23a, there are four states A, B, C, and D. Either the lock is open (State D), or
if it is not open, we have already performed either zero (State A), one (State B), or two (State C) correct 
operations. This is the sum total of all possible states that can exist.

Question: Why are there exactly four states needed to describe the combination lock of Figure 3.23a? because there 
are no other variables that are valid in the system e.g. their is no other turn that is valid.

Can you think of a snapshot of the combination lock after an operation (Rn or Ln) that requires a ﬁfth state 
because it is not covered by one of the four states A, B, C, or D? no

There are many examples of systems that you are familiar with that can be easily described by means of states.

3.6.3 The Finite State Machine and Its State Diagram

The behavior of each of the combinational lock can be described by a finite state machine and represented as a state
diagram.

A ﬁnite state machine consists of ﬁve elements:

1. a ﬁnite number of states
2. a ﬁnite number of external inputs
3. a ﬁnite number of external outputs
4. an explicit speciﬁcation of all state transitions
5. an explicit speciﬁcation of what determines each external output value.

The set of states represents all possible situations (or snapshots) that the system can be in. Each state transition
describes what it takes to get from one state to another.

Let’s examine the ﬁnite state machines for the combination lock:

A state diagram is a convenient representation of a ﬁnite state machine. Figure 3.26 is a state diagram for the 
combination lock. Recall, we identiﬁed four states A, B, C, and D. Which state we are in depends on the progress we 
have made in getting from a random initial state to the lock being open. In the state diagram of Figure 3.26, each 
circle corresponds to one of the four states, A, B, C, or D.

The external inputs are R13, L22, R3, and R-other-than-13, L-other-than-22, and R-other-than-3.

The external output is either the lock is open or the lock is not open. (One logical variable will suﬃce to 
describe that!) As shown in the state diagram, in states A, B, and C, the combination lock is locked. In state D, 
the combination lock is open.

The explicit speciﬁcations of all state transitions are shown by the arrows in the state diagram. The more 
sophisticated term for “arrow” is arc. The arrowhead on each arc speciﬁes which state the system is coming from and 
which state it is going to. We refer to the state the system is coming from as the current state, and the state it 
is going to as the next state. The combination lock has eight state transitions. Associated with each transition is 
the input that causes the transition from the current state to the next state. For example, R13 causes the 
transition from state A to state B.

A couple of things are worth noting. First, it is usually the case that from a current state there are multiple 
transitions to next states. The state transition that occurs depends on both the current state and the value of the 
external input. For example, if the combination lock is in state B, and the input is L22, the next state is state C.
If the current state is state B and the input is anything other than L22, the next state is state A. In short, the 
next state is determined by the combination of the current state and the current external input.

The output values of a system can also be determined by the combination of the current state and the value of the 
current external input. However, as is the case for the combination lock, where states A, B, and C specify the lock 
is “locked,” and state D speciﬁes the lock is “unlocked,” the output can also be determined solely by the current 
state of the system. In all the systems we will study in this book, the output values will be speciﬁed solely by the
current state of the system.

3.6.4 The Synchronous Finite State Machine

Up to now a transition from a current state to a next state in our ﬁnite state machine happened when it happened. For
example, a person could insert a nickel into the soft drink machine and then wait 10 seconds or 10 minutes before 
inserting the next coin into the machine. And the soft drink machine would not complain. It would not dispense the 
soft drink until 15 cents was inserted, but it would wait patiently as long as necessary for the 15 cents to be 
inserted. That is, there is no ﬁxed amount of time between successive inputs to the ﬁnite state machine. This is 
true in the case of all four systems we have discussed. We say these systems are asynchronous because there is 
nothing synchronizing when each state transition must occur.

However, almost no computers work that way. On the contrary, we say that computers are synchronous because the state
transitions take place, one after the other, at identical ﬁxed units of time. They are controlled by a synchronous 
ﬁnite state machine.

It is worth pointing out that the two asynchronous ﬁnite state machines discussed above and the synchronous 
ﬁnite state machine that controls a digital computer share an important characteristic: They carry out work, one 
state transition at a time, moving closer to a goal. In the case of the combination lock, as long as you make the 
correct moves, each state transition takes us closer to the lock opening. In the case of the soft drink machine, 
each state transition takes us closer to enjoying the taste of the soft drink. In the case of a computer, each 
state transition takes us closer to solving a problem by processing a computer program that someone has written.

3.6.5 The Clock

A synchronous ﬁnite state machine transitions from its current state to its next state after an identical ﬁxed 
interval of time. Control of that synchronous behavior is in part the responsibility of the clock circuit.

A clock circuit produces a signal, commonly referred to as THE clock, whose value alternates between 0 volts and 
some speciﬁed ﬁxed voltage. In digital logic terms, the clock is a signal whose value alternates between 0 and 1. 
Figure 3.28 shows the value of the clock signal as a function of time. Each of the repeated sequence of identical 
intervals is referred to as a clock cycle. A clock cycle starts when the clock signal transitions from 0 to 1 and 
ends the next time the clock signal transitions from 0 to 1.

In each clock cycle, a computer can perform a piece of useful work. When people say their laptop computers run at a 
frequency of 2 gigahertz, they are saying their laptop computers perform two billion pieces of work each second 
since 2 gigahertz means two billion clock cycles each second, each clock cycle lasting for just one-half of a 
nanosecond. The synchronous ﬁnite state machine makes one state transition each clock cycle.

We will show by means of a traﬃc signal controller how the clock signal controls the transition, ﬁxed clock cycle 
after ﬁxed clock cycle, from one state to the next.
In electronic circuit implementations of a synchronous ﬁnite state machine, the transition from one state to the 
next occurs at the start of each clock cycle.

3.6.6 Example: A Danger Sign

Many electrical, mechanical, and aeronautical systems are controlled by a synchronous ﬁnite state machine. Figure 
3.29 shows the danger sign as it will be placed on the highway. Note the sign says, “Danger, Move Right.” The sign 
contains ﬁve lights (labeled 1 through 5 in the ﬁgure).

The purpose of our synchronous ﬁnite state machine (a.k.a. a controller) is to direct the behavior of our system. In
our case, the system is the set of lights on the traﬃc danger sign. The controller’s job is to have the ﬁve lights 
ﬂash on and oﬀ to warn automobile drivers to move to the right. The controller is equipped with a switch. When the 
switch is in the ON position, the controller directs the lights as follows: During one unit of time, all lights will
be oﬀ. In the next unit of time, lights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on.
Then all ﬁve lights will be on. Then the sequence repeats: no lights on, followed by 1 and 2 on, followed by 1, 2, 3,
and 4 on, and so forth. Each unit of time lasts one second. To an automobile driver approaching the sign, the ﬁve 
lights clearly direct the driver to move to the right. The lights continue to sequence through these four states as 
long as the switch is on. If the switch is turned oﬀ, all the lights are turned oﬀ and remain oﬀ.

The State Diagram for the Danger Sign Controller 

Figure 3.30 is a state diagram for the synchronous ﬁnite state machine that controls the lights. There are four 
states, one for each of the four conditions corresponding to which lights are on. Note that the outputs (whether each
light is on or oﬀ) are determined by the current state of the system.

If the switch is on (input = 1), the transition from each state to the next state happens at one-second intervals, 
causing the lights to ﬂash in the sequence described. If the switch is turned oﬀ (input = 0), the state always 
transitions to state A, the “all oﬀ” state.

The Sequential Logic Circuit for the Danger Sign Controller

Recall that Figure 3.22 shows a generic block diagram for a sequential logic circuit. Figure 3.31 is a block diagram 
of the speciﬁc sequential logic circuit we need to control the lights. Several things are important to note in this 
ﬁgure.

First, the two external inputs: the switch and the clock. The switch determines whether the ﬁnite state machine will
transition through the four states or whether it will transition to state A, where all lights are oﬀ. The other 
input (the clock) controls the transition from state A to B, B to C, C to D, and D to A by controlling the state of 
the storage elements. 

Second, there are two storage elements for storing state information. Since there are four states, and since each 
storage element can store one bit of information, the four states are identiﬁed by the contents of the two storage 
elements: A (00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage element 1 contains 
the low bit. For example, the danger sign controller is in state B when storage element 2 is 0 and storage element 
1 is 1.

Third, combinational logic circuit 1 shows that the on/oﬀ behavior of the lights is controlled by the storage 
elements. That is, the input to the combinational logic circuit is from the two storage elements, that is, the
current state of the ﬁnite state machine.

Finally, combinational logic circuit 2 shows that the transition from the current state to the next state depends on
the two storage elements and the switch. If the switch is on, the output of combinational logic circuit 2 depends on
the state of the two storage elements.

The Combinational Logic: Figure 3.32 shows the logic that implements combinational logic circuits 1 and 2.

Two sets of outputs are required for the controller to work properly: a set of external outputs for the lights and a
set of internal outputs for the inputs to the two storage elements that keep track of the state.

First, let us look at the outputs that control the lights. As we have said, there are only three outputs necessary 
to control the lights. Light 5 is controlled by the output of the AND gate labeled V, since the only time light 5 
is on is when the controller is in state 11. Lights 3 and 4 are controlled by the output of the OR gate labeled X, 
since there are two states in which those lights are on, those labeled 10 and 11. Why are lights 1 and 2 controlled 
by the output of the OR gate labeled W? because the only time they are on is when the controller is in state 01 and
11

Next, let us look at the internal outputs that control the storage elements, which specify the next state of the 
controller. Storage element 2 should be set to 1 for the next clock cycle if the next state is 10 or 11. This is 
true only if the switch is on and the current state is either 01 or 10. Therefore, the output signal that will make 
storage element 2 be 1 in the next clock cycle is the output of the OR gate labeled Y. Why is the next state of 
storage element 1 controlled by the output of the OR gate labeled Z? storage element 1 should be set to 1 for the
next clock cyle if the state of the controller is currently 00 or 10 and the switch is on. This only occurs if
the output is produced by gate Z.

The Two Storage Elements: In order for the danger sign controller to work, the state transitions must occur once 
per second when the switch is on.

A Problem with Gated Latches as Storage Elements: What would happen if the storage elements were gated D latches? If 
the two storage elements were gated D latches, when the write enable signal (the clock) is 1, the output of OR gates
Y and Z would immediately change the bits stored in the two gated D latches. This would produce new input values to 
the three AND gates that are input to OR gates Y and Z, producing new outputs that would be applied to the inputs of
the gated latches, which would in turn change the bits stored in the gated latches, which would in turn mean new 
inputs to the three AND gates and new outputs for OR gates Y and Z. This would happen again and again, continually 
changing the bits stored in the two storage elements as long as the Write Enable signal to the gated D latches was 
asserted. The result: We have no idea what the state of the ﬁnite state machine would be for the next clock cycle. 
And, even in the current clock cycle, the state of the storage elements would change so fast that the ﬁve lights 
would behave erratically.

The problem is the gated D latch. We want the output of OR gates Y and Z to transition to the next state at the end 
of the current clock cycle and allow the current state to remain unchanged until then. That is, we do not want the 
input to the storage elements to take eﬀect until the end of the current clock cycle. Since the output of a gated D 
latch changes immediately in response to its input if the Write Enable signal is asserted, it cannot be the storage 
element for our synchronous ﬁnite state machine. We need storage elements that allow us to read the current state 
throughout the current clock cycle, and not write the next state values into the storage elements until the 
beginning of the next clock cycle.

The Flip-Flop to the Rescue: It is worth repeating; To prevent the above from happening, we need storage elements 
that allow us to read the current state throughout the current clock cycle, and not write the next state values into
the storage elements until the beginning of the next clock cycle. That is, the function to be performed during a 
single clock cycle involves reading and writing a particular variable. Reading must be allowed throughout the 
clock cycle, and writing must occur at the end of the clock cycle.

A ﬂip-ﬂop can accomplish that. One example of a ﬂip-ﬂop is the master/slave ﬂip-ﬂop shown in Figure 3.33. The 
master/slave ﬂip-ﬂop can be constructed out of two gated D latches, one referred to as the master, the other referred
to as the slave. Note that the write enable signal of the master is 1 when the clock is 0, and the write enable 
signal of the slave is 1 when the clock is 1.

Figure 3.34 is a timing diagram for the master/slave ﬂip-ﬂop, which shows how and why the master/slave ﬂip-ﬂop solves
the problem. A timing diagram shows time passing from left to right. Note that clock cycle n starts at the time 
labeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time labeled 4.

Consider clock cycle n, which we will discuss in terms of its ﬁrst half A, its second half B, and the four time 
points labeled 1, 2, 3, and 4.

At the start of each clock cycle, the outputs of the storage elements are the outputs of the two slave latches. 
These outputs (starting at time 1) are input to the AND gates, resulting in OR gates Y and Z producing the next 
state values for the storage elements (at time 2). The timing diagram shows the propagation delay of the 
combinational logic, that is, the time it takes for the combinational logic to produce outputs of OR gates Y and Z. 
Although OR gates Y and Z produce the Next State value sometime during half-cycle A, the write enable signal to the 
master latches is 0, so the next state cannot be written into the master latches.

At the start of half-cycle B (at time 3), the clock signal is 0, which means the write enable signal to the master 
latches is 1, and the master latches can be written. However, during the half-cycle B, the write enable to the slave
latches is 0, so the slave latches cannot write the new information now stored in the master latches.

At the start of clock cycle n+1 (at time 4), the write enable signal to the slave latches is 1, so the slave latches
can store the next state value that was created by the combinational logic during clock cycle n. This becomes the 
current state for clock cycle n+1.

Since the write enable signal to the master latches is now 0, the state of the master latches cannot change. Thus, 
although the write enable signal to the slave latches is 1, those latches do not change because the master latches 
cannot change.

In short, the output of the slave latches contains the current state of the system for the duration of the clock 
cycle and produces the inputs to the six AND gates in the combinational logic circuits. Their state changes at the 
start of the clock cycle by storing the next state information created by the combinational logic during the 
previous cycle but does not change again during the clock cycle. The reason they do not change again during the 
clock cycle is as follows: During half-cycle A, the master latches cannot change, so the slave latches continue to 
see the state information that is the current state for the new clock cycle. During half-cycle B, the slave latches 
cannot change because the clock signal is 0.

Meanwhile, during half-cycle B, the master latches can store the next state information produced by the combinational
logic, but they cannot write it into the slave latches until the start of the next clock cycle, when it becomes the 
state information for the next clock cycle.

3.7 Preview of Coming Attractions: The Data Path of the LC-3

We close out Chapter 3 with a discussion of Figure 3.35, the data path of the LC-3 computer. The data path consists 
of all the logic structures that combine to process information in the core of the computer. Right now, Figure 3.35 
is undoubtedly more than a little intimidating, but you should not be concerned by that. You are not ready to 
analyze it yet. That will come in Chapter 5. We have included it here, however, to show you that you are already 
familiar with many of the basic structures that make up a computer.

For example, you see ﬁve MUXes in the data path, and you already know how they work. Also, an adder (shown as the 
ALU symbol with a + sign inside) and an ALU. You know how those elements are constructed from gates.

One element that we have not identified explicitly yet is a register. A register is simply a set of n flip-flops that
collectively are used to store one n-bit value. In Figure 3.35, PC, IR, MAR, and MDR are all 16-bit registers that 
store 16 bits of information each. The block labeled REG FILE consists of eight registers that each store 16 bits of
information. As you know, one bit of information can be stored in one flip-flop. Therefore, each of these registers 
consists of 16 flip-flops. The data path also shows three 1-bit registers, N, Z, and P. Those registers require only
one flip-flop each. In fact, a register can be any size that we need. The size depends only on the number of bits we
need to represent the value we wish to store.

One way to implement registers is with master/slave ﬂip-ﬂops. Figure 3.36 shows a four-bit register made up of four 
master/slave ﬂip-ﬂops. We usually need ﬂip-ﬂops, rather than latches, because it is usually important to be able to 
both read the contents of a register throughout a clock cycle and also store a new value in the register at the end 
of that same clock cycle. As shown in Figure 3.36, the four-bit value stored in the register during a clock cycle is
Q₃, Q₂, Q₁, Q₀. At the end of that clock cycle, the value D₃, D₂, D₁, D₀ is written into the register.

The arrows in Figure 3.35 represent wires that transmit values from one structure to another. Most of the arrows 
include a cross-hatch with a number next to it. The number represents the number of wires, corresponding to the 
number of bits being transmitted. Thus, for example, the arrow from the register labeled PC to one of the inputs of 
the MUX labeled ADDR1MUX indicates that 16 bits are transmitted from PC to an input of ADDR1MUX.

A two-input AND and a two-input OR are both examples of two-input logic functions. How many diﬀerent two-input 
logic functions are possible? There can be 16 different two input logic functions.

(chapter 3 questions were mostly about logic gates which I don't have sufficient knowledge on to attempt now)
----

chapter 4

To get a task done by a computer, we need two things: (a) a computer program that speciﬁes what the computer must 
do to perform the task, and (b) the computer that is to carry out the task.

A computer program consists of a set of instructions, each specifying a well-deﬁned piece of work for the computer 
to carry out. The instruction is the smallest piece of work speciﬁed in a computer program. That is, the computer 
either carries out the work speciﬁed by an instruction or it does not. The computer does not have the luxury of 
carrying out only a piece of an instruction.

John von Neumann proposed a fundamental model of a computer for processing computer programs in 1946. Figure 4.1 
shows its basic components. We have taken a little poetic license and added a few of our own minor embellishments 
to von Neumann’s original diagram. The von Neumann model consists of ﬁve parts: memory, a processing unit, input, 
output, and a control unit. The computer program is contained in the computer’s memory. The data the program needs 
to carry out the work of the program is either contained in the program’s memory or is obtained from the input 
devices. The results of the program’s execution are provided by the output devices. The order in which the 
instructions are carried out is performed by the control unit.

We will describe each of the ﬁve parts of the von Neumann model in greater detail.

Memory

Previously you saw a simple 2^2-by-3-bit memory that was constructed out of gates and latches. A more realistic 
memory for one of today’s computer systems is 2^34 by 8 bits. That is, a typical memory in today’s world of 
computers consists of 2^34 distinct memory locations, each of which is capable of storing eight bits of information.
We say that such a memory has an address space of 2^34 uniquely identiﬁable locations, and an addressability of 
eight bits.

We refer to such a memory as a 16-gigabyte memory (abbreviated, 16 GB). The “16 giga” refers to the 2^34 locations, 
and the “byte” refers to the eight bits stored in each location. The term is 16 giga because 16 is 2^4 and giga is 
the term we use to represent 2^30, which is approximately one billion; 2^4 times 2^30 = 2^34.A byte is the word we 
use to describe eight bits, much the way we use the word gallon to describe four quarts.

We note (as we will note again and again) that with k bits, we can represent uniquely 2^k items. Thus, to uniquely 
identify 2^34 memory locations, each location must have its own 34-bit address. 

To read the contents of a memory location, we ﬁrst place the address of that location in the memory’s address 
register (MAR) and then interrogate the computer’s memory. The information stored in the location having that 
address will be placed in the memory’s data register (MDR). To write (or store) a value in a memory location, we 
ﬁrst write the address of the memory location in the MAR, and the value to be stored in the MDR. We then 
interrogate the computer’s memory with the write enable signal asserted. The information contained in the MDR will 
be written into the memory location whose address is in the MAR. 

Before we leave the notion of memory for the moment, let us again emphasize the two characteristics of a memory 
location: its address and what is stored there. Figure 4.2 shows a representation of a memory consisting of eight 
locations. Its addresses are shown at the left, numbered in binary from 0 to 7. Each location contains eight bits 
of information. Note that the value 6 is stored in the memory location whose address is 4, and the value 4 is 
stored in the memory location whose address is 6. These represent two very diﬀerent situations.

Finally, an analogy: the post oﬃce boxes in your local post oﬃce. The box number is like the memory location’s 
address. Each box number is unique. The information stored in the memory location is like the letters contained in 
the post oﬃce box. As time goes by, what is contained in the post oﬃce box at any particular moment can change. 
But the box number remains the same. So, too, with each memory location. The value stored in that location can be 
changed, but the location’s memory address remains unchanged.

Processing Unit

The actual processing of information in the computer is carried out by the processing unit. The processing unit in 
a modern computer can consist of many sophisticated complex functional units, each performing one particular 
operation (divide, square root, etc.). The simplest processing unit, and the one normally thought of when 
discussing the basic von Neumann model, is the ALU. ALU is the abbreviation for Arithmetic and Logic Unit, so called
because it is usually capable of performing basic arithmetic functions (like ADD and SUBTRACT) and basic logic 
operations (like bit-wise AND, OR, and NOT) . 

The ALU normally processes data elements of a ﬁxed size referred to as the word length of the computer. The data 
elements are called words. For example, to perform ADD, the ALU receives two words as inputs and produces a single 
word (the sum) as output. Each ISA has its own word length, depending on the intended use of the computer.

Most microprocessors today that are used in PCs or workstations have a word length of 64 bits or 32 bits. Even most
microprocessors now used in cell phones have 64-bit word lengths, However, the microprocessors used in very 
inexpensive applications often have word lengths of as little as 16 or even 8 bits.

It is almost always the case that a computer provides some small amount of storage very close to the ALU to allow 
results to be temporarily stored if they will be needed to produce additional results in the near future. For 
example, if a computer is to calculate (A+B)⋅C, it could store the result of A+B in memory, and then subsequently 
read it in order to multiply that result by C. However, the time it takes to access memory is long compared to the 
time it takes to perform the ADD or MULTIPLY. Almost all computers, therefore, have temporary storage for storing 
the result of A + B in order to avoid the much longer access time that would be necessary when it came time to 
multiply. The most common form of temporary storage is a set of registers. Typically, the size of each register is 
identical to the size of values processed by the ALU; that is, they each contain one word.

Current microprocessors typically contain 32 registers, each consisting of 32 or 64 bits, depending on the 
architecture. These serve the same purpose as the eight 16-bit registers in the LC-3. However, the importance of 
temporary storage for values that most modern computers will need shortly means many computers today have an 
additional set of special-purpose registers consisting of 128 bits of information to handle special needs. 

Input and Output

In order for a computer to process information, the information must get into the computer. In order to use the 
results of that processing, those results must be displayed in some fashion outside the computer. Many devices 
exist for the purposes of input and output. They are generically referred to in computer jargon as peripherals 
because they are in some sense accessories to the processing function. Nonetheless, they are no less important.

Control Unit

The control unit is like the conductor of an orchestra; it is in charge of making all the other parts of the 
computer play together. As we will see when we describe the step-by-step process of executing a computer program, 
it is the control unit that keeps track of both where we are within the process of executing the program and where 
we are in the process of executing each instruction.

To keep track of which instruction is being executed, the control unit has an instruction register to contain that 
instruction. To keep track of which instruction is to be processed next, the control unit has a register that 
contains the next instruction’s address. For historical reasons, that register is called the program counter 
(abbreviated PC), although a better name for it would be the instruction pointer, since the contents of this 
register is, in some sense, “pointing” to the next instruction to be processed. Curiously, Intel does in fact call 
that register the instruction pointer, but the simple elegance of that name has not caught on.

The LC-3: An Example von Neumann Machine

We have already shown you its data path in Chapter 3 (Figure 3.35) and identiﬁed several of its structures in above.
In this section, we will pull together all the parts of the LC-3 we need to describe it as a von Neumann computer 
(see Figure 4.3).

We constructed Figure 4.3 by starting with the LC-3’s full data path (Figure 3.35) and removing all elements that 
are not essential to pointing out the ﬁve basic components of the von Neumann model.

Note that there are two kinds of arrowheads in Figure 4.3: ﬁlled-in and not-ﬁlled-in. Filled-in arrowheads denote 
data elements that ﬂow along the corresponding paths. Not-ﬁlled-in arrowheads denote control signals that control 
the processing of the data elements. For example, the box labeled ALU in the processing unit processes two 16-bit 
values and produces a 16-bit result. The two sources and the result are all data, and are designated by ﬁlled-in 
arrowheads. The operation performed on those two 16-bit data elements (it is labeled ALUK) is part of the control—
therefore, a not-ﬁlled-in arrowhead.

MEMORY consists of the storage elements, along with the Memory Address Register (MAR) for addressing individual 
locations and the Memory Data Register (MDR) for holding the contents of a memory location on its way to/from the 
storage. Note that the MAR contains 16 bits, reﬂecting the fact that the memory address space of the LC-3 is 2^16 
memory locations. The MDR contains 16 bits, reﬂecting the fact that each memory location contains 16 bits—that is, 
the LC-3 is 16-bit addressable.

INPUT/OUTPUT

consists of a keyboard and a monitor. The simplest keyboard requires two registers: a keyboard data register (KBDR)
for holding the ASCII codes of keys struck and a keyboard status register (KBSR) for maintaining status information
about the keys struck. The simplest monitor also requires two registers: a display data register (DDR) for holding 
the ASCII code of something to be displayed on the screen and a display status register (DSR) for maintaining 
associated status information.

THE PROCESSING UNIT

consists of a functional unit (ALU) that performs arithmetic and logic operations and eight registers (R0, … R7) 
for storing temporary values that will be needed in the near future as operands for subsequent instructions. The 
LC-3 ALU can perform one arithmetic operation (addition) and two logical operations (bitwise AND and bitwise NOT).

THE CONTROL UNIT 

consists of all the structures needed to manage the processing that is carried out by the computer. Its most 
important structure is the ﬁnite state machine, which directs all the activity. Processing is carried out step by 
step, or rather, clock cycle by clock cycle. Note the CLK input to the ﬁnite state machine in Figure 4.3. It 
speciﬁes how long each clock cycle lasts. The instruction register (IR) is also an input to the ﬁnite state machine
since the LC-3 instruction being processed determines what activities must be carried out. The program counter (PC)
is also a part of the control unit; it keeps track of the next instruction to be executed after the current 
instruction ﬁnishes.

Note that all the external outputs of the ﬁnite state machine in Figure 4.3 have arrowheads that are not ﬁlled in. 
These outputs control the processing through-out the computer. For example, one of these outputs (two bits) is 
ALUK, which controls the operation performed in the ALU (ADD, AND, or NOT) during the current clock cycle. Another 
output is GateALU, which determines whether or not the output of the ALU is provided to the processor bus during 
the current clock cycle.

The complete description of the data path, control, and ﬁnite state machine for one implementation of the LC-3 is 
the subject of Appendix C.

Instruction Processing

The central idea in the von Neumann model of computer processing is that the program and data are both stored as 
sequences of bits in the computer’s memory, and the program is executed one instruction at a time under the 
direction of the control unit.

The Instruction

The most basic unit of computer processing is the instruction. It is made up of two parts, the opcode (what the 
instruction does) and the operands (who it does it to!).

There are fundamentally three kinds of instructions: operates, data movement, and control, although many ISAs have 
some special instructions that are necessary for those ISAs. Operate instructions operate on data. The LC-3 has 
three operate instructions: one arithmetic (ADD) and two logicals (AND and NOT). Data movement instructions move 
information from the processing unit to and from memory and to and from input/output devices. The LC-3 has six 
data movement instructions. Control instructions are necessary for altering the sequential processing of 
instructions.

An LC-3 instruction consists of 16 bits (one word), numbered from left to right, bit [15] to bit [0]. Bits [15:12] 
contain the opcode. This means there are at most 2^4 distinct opcodes. Actually, we use only 15 of the possible 
four-bit codes. One is reserved for some future use. Bits [11:0] are used to ﬁgure out where the operands are.

In this chapter, we will introduce ﬁve of the LC-3’s 15 instructions: two operates (ADD and AND), one data movement
(LD), and two control (BR and TRAP).

The ADD instruction is an operate instruction that requires three operands: two source operands
(the numbers to be added) and one destination operand (where the sum is to be stored after the addition is 
performed). We said that the processing unit of the LC-3 contained eight registers for purposes of storing data 
that may be needed later. In fact, the ADD instruction requires that at least one of the two source operands is 
contained in one of these registers, and that the result of the ADD is put into one of these eight registers. 
Since there are eight registers, three bits are necessary to identify each register. The 16-bit LC-3 ADD 
instruction has one of the following two forms (we say formats):

15 14 13 12  11 10 9   8  7  6   5   4  3   2  1  0
0  0  0  1 | 1  1  0 | 0  1  0 | 0 | 0  0 | 1  1  0
  ADD           R6        R2                  R6


15 14 13 12  11 10 9   8  7  6   5   4  3  2  1  0
 0  0  0 1 | 1  1  0 | 0  1  0 | 1 | 0  0  1  1  0
  ADD           R6        R2             imm

Both formats show the four-bit opcode for ADD, contained in bits [15:12]: 0001. Bits [11:9] identify the location
to be used for storing the result, in this case register 6 (R6). Bits [8:6] identify the register that contains one
of the two source operands, in this case R2. The only diﬀerence in the two formats is the 1 or 0 stored in bit 5, 
and what that means. In the ﬁrst case, bit 5 is 0, signifying that the second source operand is in the register 
speciﬁed by bits [2:0], in this case R6. In the second case, bit 5 is 1, signifying that the second source operand 
is formed by sign-extending the integer in bits [4:0] to 16 bits. In this case, the second source operand is the 
positive integer 6.

Thus, the instruction we have just encoded is interpreted, depending on whether bit 5 is a 0 or a 1 as either “Add 
the contents of register 2 (R2) to the contents of register 6 (R6) and store the result back into register 6 (R6),” 
or “Add the contents of register 2 (R2) to the positive integer 6 and store the result into register 6.”

The AND Instruction; The AND instruction is also an operate instruction, and its behavior is essentially identical to
the ADD instruction, except for one thing. Instead of ADDing the two source operands, the AND instruction performs a
bit-wise AND of the corresponding bits of the two source operands. For example, the instruction shown below

15    14     13     12     11     10     9      8     7     6      5      4     3     2     1     0
0     1      0      1   |   0      1     0   |  0     1     1  |   1   |  0     0     0     0     0
         AND                     R2                   R3                         imm

is an AND instruction since bits [15:12] = 0101. The two sources are R3 and the immediate value 0. The instruction 
loads R2 with the value 0 since the AND instruction performs a bit-wise AND where the bit of the second operand is 
always 0. As we shall see, this instruction is a convenient technique for making sure a particular register contains 
0 at the start of processing. We refer to this technique as initializing R2 to 0.

The LD Instruction; The LD instruction requires two operands. LD stands for load, which is computerese for “go to a 
particular memory location, read the value that is contained there, and store that value in one of the registers.” 
The two operands that are required are the value to be read from memory and the destination register that will 
contain that value after the instruction has completed processing. There are many formulas that can be used for 
calculating the address of the memory location to be read. Each formula is called an addressing mode. The 
particular addressing mode identiﬁed by the use of the opcode LD is called PC+oﬀset. The 16-bit LC-3 LD instruction 
has the following format:

15 14 13 12  11 10 9   8  7  6  5  4  3  2  1  0
0  0  1  0 | 0  1  0 | 0  1  1  0  0  0  1  1  0
   LD           R2                 198

The four-bit opcode for LD is 0010. Bits [11:9] identify the register that will contain the value read from memory 
after the instruction is executed. Bits [8:0] are used to calculate the address of the location to be read. Since the
addressing mode for LD is PC+oﬀset, this address is computed by sign-extending the 2’s complement integer contained 
in bits [8:0] to 16 bits and adding it to the current contents of the program counter. In summary, the instruction 
tells the computer to add 198 to the contents of the PC to form the address of a memory location and to load the 
contents of that memory location into R2. If bits [8:0] had been 111111001, the instruction would have been 
interpreted: “A d d −7 to the contents of the PC to form the address of a memory location.”

The Instruction Cycle (NOT the Clock Cycle!)

Instructions are processed under the direction of the control unit in a very systematic, step-by-step manner. The 
entire sequence of steps needed to process an instruction is called the instruction cycle. The instruction cycle 
consists of six sequential phases, each phase requiring zero or more steps. We say zero steps to indicate that most 
computers have been designed such that not all instructions require all six phases. We will discuss this momentarily. 
But ﬁrst, we will examine the six phases of the instruction cycle:

FETCH
DECODE
EVALUATE  ADDRESS
FETCH  OPERANDS
EXECUTE
STORE  RESULT

FETCH

The FETCH phase obtains the next instruction from memory and loads it into the instruction register (IR) of the 
control unit. Recall that a computer program consists of a number of instructions, that each instruction is 
represented by a sequence of bits, and that the entire program (in the von Neumann model) is stored in the computer’s 
memory. In order to carry out the work of an instruction, we must ﬁrst identify where it is. The program counter (PC) 
contains the address of the next instruction to be processed. Thus, the FETCH phase takes the following steps:

First the MAR is loaded with the contents of the PC.
Next, the memory is interrogated, which results in the next instruction being placed by the memory into the MDR.
Finally, the IR is loaded with the contents of the MDR.

We are now ready for the next phase, decoding the instruction. However, when the instruction ﬁnishes execution, and 
we wish to fetch the next instruction, we would like the PC to contain the address of the next instruction. This is 
accomplished by having the FETCH phase perform one more task: incrementing the PC. In that way, after the current 
instruction ﬁnishes, the FETCH phase of the next instruction will load into the IR the contents of the next memory 
location, provided the execution of the current instruction does not involve changing the value in the PC.

The complete description of the FETCH phase is as follows:

Step 1: Load the MAR with the contents of the PC, and simultaneously increment the PC.
Step 2: Interrogate memory, resulting in the instruction being placed in the MDR.
Step 3: Load the IR with the contents of the MDR.

Each of these steps is under the direction of the control unit, much like, as we said previously, the instruments in 
an orchestra are under the control of a conductor’s baton. Each stroke of the conductor’s baton corresponds to one 
machine cycle. The amount of time taken by each machine cycle is one clock cycle. In fact, we often use the two terms 
interchangeably. Step 1 takes one clock cycle. Step 2 could take one clock cycle or many clock cycles, depending on 
how long it takes to access the computer’s memory. Step 3 takes one clock cycle. In a modern digital computer, a 
clock cycle takes a very small fraction of a second.

Indeed, a 3.1 GHz Intel Core i7 completes 3.1 billion clock cycles in one second. Said another way, one clock cycle 
takes 0.322 billionths of a second (0.322 nanoseconds). Recall that the light bulb that is helping you read this text 
is switching on and oﬀ at the rate of 60 times a second. Thus, in the time it takes a light bulb to switch on and oﬀ 
once, today’s computers can complete more than 51 million clock cycles!

DECODE

The DECODE phase examines the instruction in order to ﬁgure out what the microarchitecture is being asked to do. 
Recall the decoders we studied in Chapter 3. In the LC-3, a 4-to-16 decoder identiﬁes which of the 16 opcodes is to 
be processed (even though one of the 16 is not used!). Input is the four-bit opcode IR [15:12]. The output line 
asserted is the one corresponding to the opcode at the input. Depending on which output of the decoder is asserted, 
the remaining 12 bits identify what else is needed to process that instruction.

EVALUATE ADDRESS

This phase computes the address of the memory location that is needed to process the instruction. Recall the example 
of the LD instruction: The LD instruction causes a value stored in memory to be loaded into a register. In that 
example, the address was obtained by sign-extending bits [8:0] of the instruction to 16 bits and adding that value to
the current contents of the PC. This calculation was performed during the EVALUATE ADDRESS phase. It is worth noting 
that not all instructions access memory to load or store data. For example, we have already seen that the ADD and AND
instructions in the LC-3 obtain their source operands from registers or from the instruction itself and store the 
result of the ADD or AND instruction in a register. For those instructions, the EVALUATE ADDRESS phase is not needed.

FETCH OPERANDS

This phase obtains the source operands needed to process the instruction. In the LD example, this phase took two steps: loading 
MAR with the address calculated in the EVALUATE ADDRESS phase and reading memory that resulted in the source operand being placed 
in MDR.

In the ADD example, this phase consisted of obtaining the source operands from R2 and R6. In most current microprocessors, this phase 
(for the ADD instruction) can be done at the same time the instruction is being executed (the ﬁfth phase of the instruction cycle). 

EXECUTE

This phase carries out the execution of the instruction. In the ADD example, this phase consisted of the step of performing the addition 
in the ALU.

STORE RESULT

The ﬁnal phase of an instruction’s execution. The result is written to its designated destination. In the case of the ADD instruction, in 
many computers this action is performed during the EXECUTE phase. That is, in many computers, including the LC-3, an ADD instruction can 
fetch its source operands, perform the ADD in the ALU, and store the result in the destination register all in a single clock cycle. A 
separate STORE RESULT phase is not needed.

Once the instruction cycle has been completed, the control unit begins the instruction cycle for the next instruction, starting from the 
top with the FETCH phase. Since the PC was updated during the previous instruction cycle, it contains at this point the address of the 
instruction stored in the next sequential memory location. Thus, the next sequential instruction is fetched next. Processing continues in 
this way until something breaks this sequential ﬂow, or the program ﬁnishes execution.

ADD [eax], edx This is an example of an Intel x86 instruction that requires all six phases of the instruction cycle. All instructions 
require the ﬁrst two phases, FETCH and DECODE. This instruction uses the eax register to calculate the address of a memory location 
(EVALUATE ADDRESS). The contents of that memory location is then read (FETCH OPERAND), added to the contents of the edx register (EXECUTE), 
and the result written into the memory location that originally contained the ﬁrst source operand (STORE RESULT).

Changing the Sequence of Execution

There is a third type of instruction, the control instruction, whose purpose is to change the sequence of instruction execution. For 
example, there are times, as we shall see very soon, when it is desirable to ﬁrst execute the ﬁrst instruction, then the second, then the 
third, then the ﬁrst again, the second again, then the third again, then the ﬁrst for the third time, the second for the third time, and 
so on. As we know, each instruction cycle starts with loading the MAR with the PC. Thus, if we wish to change the sequence of instructions 
executed, we must change the contents of the PC between the time it is incremented (during the FETCH phase of one instruction) and the 
start of the FETCH phase of the next instruction.

Control instructions perform that function by loading the PC during the EXECUTE phase, which wipes out the incremented PC that was loaded 
during the FETCH phase. The result is that, at the start of the next instruction cycle, when the computer accesses the PC to obtain the 
address of an instruction to fetch, it will get the address loaded during the previous instruction’s EXECUTE phase, rather than the next 
sequential instruction in the computer’s program.

The most common control instruction is the conditional branch (BR), which either changes the contents of the PC or does not change the 
contents of the PC, depending on the result of a previous instruction (usually the instruction that is executed immediately before the 
conditional branch instruction).

The BR Instruction 

The BR instruction consists of three parts, the opcode (bits [15:12] = 0000), the condition to be tested (bits [11:9]), 
and the addressing mode bits (bits [8:0]) that are used to form the address to be loaded into the PC if the result of the previous 
instruction agrees with the test speciﬁed by bits [11:9]. The addressing mode, i.e., the mechanism used to determine the actual address, 
is the same one we used in the LD instruction. Bits [8:0] are sign-extended to 16 bits and then added to the current contents of the PC.

Suppose the BR instruction shown below is located in memory location x36C9.

15 14 13 12  11 10 9   8  7  6  5  4  3  2  1  0
0  0  0  0 | 1  0  1 | 1  1  1  1  1  1  0  1  0
   BR       condition              -6

The opcode 0000 identiﬁes the instruction as a conditional branch. Bits [11:9] = 101 speciﬁes that the test to be performed on the most 
recent result is whether or not that result is something other than 0. In Chapter 5 we will describe in detail all the tests that can 
be performed on the most recent result. For now, we will just use one test: Is the result not zero? Bits [8:0] is the value −6.

Assume the previous instruction executed (in memory location x36C8) was an ADD instruction and the result of the ADD was 0. Since the 
test “not-zero” failed, the BR instruction would do nothing during its EXECUTE phase, and so the next instruction executed would be the 
instruction at M[x36CA], the address formed by incrementing the PC during the FETCH phase of the BR instruction’s instruction cycle.

On the other hand, if the result of the ADD instruction is not 0, then the test succeeds, causing the BR instruction to load PC with 
x36C4, the address formed by sign-extending bits [8:0] to 16 bits and adding that value (-6) to the incremented PC (x36CA).

Thus, the next instruction executed after the BR instruction at x36C9 is either the instruction at x36CA or the one at x36C4, depending 
on whether the result of the ADD instruction was zero or not zero.

Control of the Instruction Cycle

The instruction cycle is controlled by a synchronous ﬁnite state machine. An abbreviated version of its state diagram, highlighting a 
few of the LC-3 instructions discussed in this chapter, is shown in Figure 4.4. As is the case with the ﬁnite state machines studied 
in Section 3.6, each state corresponds to one machine cycle of activity that takes one clock cycle to perform. The processing 
controlled by each state is described within the node representing that state. The arcs show the next state transitions.

Processing starts with State 1. The FETCH phase takes three clock cycles, corresponding to the three steps described earlier. In the 
ﬁrst clock cycle, the MAR is loaded with the contents of the PC, and the PC is incremented. In order for the contents of the PC to be 
loaded into the MAR (see Figure 4.3), the ﬁnite state machine must assert GatePC and LD.MAR. GatePC connects the PC to the processor 
bus. LD.MAR, the write enable signal of the MAR register, loads the contents of the bus into the MAR at the end of the current clock
cycle. (Registers are loaded at the end of the clock cycle if the corresponding control signal is asserted.)

In order for the PC to be incremented (again, see Figure 4.3), the ﬁnite state machine must assert the PCMUX select lines to choose 
the output of the box labeled +1 and must also assert the LD.PC signal to load the output of the PCMUX into the PC at the end of the 
current cycle.

The ﬁnite state machine then goes to State 2. Here, the MDR is loaded with the instruction, which is read from memory.

In State 3, the instruction is transferred from the MDR to the instruction register (IR). This requires the ﬁnite state machine to 
assert GateMDR and LD.IR, which causes the IR to be loaded at the end of the clock cycle, concluding the FETCH phase of the 
instruction cycle.

The DECODE phase takes one clock cycle. In State 4, using the external input IR, and in particular the opcode bits of the instruction, 
the ﬁnite state machine can go to the appropriate next state for processing instructions depend- ing on the particular opcode in IR 
[15:12]. Three of the 15 paths out of State 4 are shown. Processing continues clock cycle by clock cycle until the instruction 
completes execution, and the next state logic returns the ﬁnite state machine to State 1.

As has already been discussed, it is sometimes necessary not to execute the next sequential instruction but rather to access another 
location to ﬁnd the next instruction to execute. As we have said, instructions that change the ﬂow of instruction processing in this 
way are called control instructions. In the case of the conditional branch instruction (BR), at the end of its instruction cycle, the 
PC contains one of two addresses: either the incremented PC that was loaded in State 1 or the new address computed from sign-extending 
bits [8:0] of the BR instruction and adding it to the PC, which was loaded in State 63. Which address gets loaded into the PC depends 
on the test of the most recent result.

Halting the Computer (the TRAP Instruction)

From everything we have said, it appears that the computer will continue processing instructions, carrying out the instruction cycle 
again and again, ad nauseum. Since the computer does not have the capacity to be bored, must this continue until someone pulls the 
plug and disconnects power to the computer?

Usually, user programs execute under the control of an operating system. Linux, DOS, MacOS, and Windows are all examples of operating 
systems. Operating systems are just computer programs themselves. As far as the com- puter is concerned, the instruction cycle 
continues whether a user program is being processed or the operating system is being processed. This is ﬁne as far as user programs 
are concerned since each user program terminates with a control instruction that changes the PC to again start processing the operating 
system—often to initiate the execution of another user program.

But what if we actually want to stop this potentially inﬁnite sequence of instruction cycles? Recall our analogy to the conductor’s 
baton, beating at the rate of billions of clock cycles per second. Stopping the instruction sequencing requires stopping the 
conductor’s baton. We have pointed out many times that there is inside the computer a component that corresponds very closely to the 
conductor’s baton. It is called the clock, and it deﬁnes the amount of time each machine cycle takes. We saw in Chapter 3 that the 
clock enables the synchronous ﬁnite state machine to continue on to the next clock cycle. In Chapter 3 the next clock cycle 
corresponded to the next state of the danger sign we designed. Here the next clock cycle corresponds to the next state of the 
instruction cycle, which is either the next state of the current phase of the instruction cycle or the ﬁrst state of the next phase of 
the instruction cycle. Stopping the instruction cycle requires stopping the clock.

Figure 4.5a shows a block diagram of the clock circuit, consisting primarily of a clock generator and a RUN latch. The clock generator 
is a crystal oscillator, a piezoelectric device that you may have studied in your physics or chemistry class. For our purposes, the 
crystal oscillator is a black box that produces the oscillating voltage shown in Figure 4.5b. Note the resemblance of that voltage to the 
conductor’s baton. Every clock cycle, the voltage rises to 1.2 volts and then drops back to 0 volts.

If the RUN latch is in the 1 state (i.e., Q = 1), the output of the clock circuit is the same as the output of the clock generator. If the 
RUN latch is in the 0 state (i.e., Q = 0), the output of the clock circuit is 0.

Thus, stopping the instruction cycle requires only clearing the RUN latch. Every computer has some mechanism for doing that. In some older 
machines, it is done by executing a HALT instruction. In the LC-3, as in many other machines, it is done under control of the operating 
system, as we will see in Chapter 9. For now it is enough to know that if a user program requires help from the operating system, it 
requests that help with the TRAP instruction (opcode = 1111) and an eight-bit code called a trap vector, which identiﬁes the help that 
the user program needs. The eight-bit code x25 tells the operating system that the program has ﬁnished executing and the computer can stop 
processing instructions.

Question: If a HALT instruction can clear the RUN latch, thereby stopping the instruction cycle, what instruction is needed to set the 
RUN latch, thereby reinitiating the instruction cycle? Hint: This is a trick question!

Our First Program: A Multiplication Algorithm

We now have all that we need to write our ﬁrst program. We have a data movement instruction LD to load data from memory into a register, 
and we have two operate instructions, ADD for performing arithmetic and AND for performing a bit-wise logical operation. We have a 
control instruction BR for loading the PC with an address diﬀerent from the incremented PC so the instruction to be executed next will 
NOT be the instruction in the next sequential location in memory. And we have the TRAP instruction (a.k.a. system call) that allows us to 
ask the operating system for help, in this case to stop the computer. With all that under our belt, we can write our ﬁrst program.

Suppose the computer does not know how to multiply two positive integers. In the old days, that was true for a lot of computers! They had 
ADD instructions, but they did not have multiply instructions. What to do? Suppose we wanted to multiply 5 times 4. Even if we do not 
know how to multiply, if we know that 5 times 4 is 5+5+5+5, and the computer has an ADD instruction, we can write a program that can 
multiply. All we have to do is add 5 to itself four times.

Figure 4.6 illustrates the process.

Let us assume that memory location x3007, abbreviated M[x3007], contains the value 5, and M[x3008] contains the value 4. We start by 
copying the two values from memory to the two registers R1 and R2. We are going to accumulate the results of the additions in R3, so we 
initialize R3 to 0. Then we add 5 to R3, and subtract 1 from R2 so we will know how many more times we will need to add 5 to R3. We keep 
doing this (adding 5 to R3 and subtracting 1 from R2) until R2 contains the value 0. That tells us that we have added 5 to R3 four times 
and we are done, so we HALT the computer. R3 contains the value 20, the result of our multiplication.

Figure 4.7 shows the actual LC-3 program, stored in memory locations x3000 to x3008.

The program counter, which keeps track of the next instruction to be executed, initially contains the address x3000.

To move the data from memory locations M[x3007] and M[x3008] to R1 and R2, we use the data movement instruction LD. The LC-3 computer 
executes the LD instruction in M[x3000] by sign-extending the oﬀset (in this case 6) to 16 bits, adding it to the incremented PC (in this 
case x3001 since we incremented the PC during the FETCH phase of this instruction), fetching the data from M[x3007], and loading it in 
R1. The LD instruction in M[x3001] is executed in the same way.

R3 is initialized to 0 by performing a bit-wise AND of the contents of R3 with the sign-extended immediate value 0 and loading the result 
into R3.

Next  the  computer  executes  the  ADD  instructions  at  M[x3003]  and M[x3004]. The ADD instruction at M[x3003] adds the contents of R1 
to the contents of R3 and loads the result into R3. The ADD instruction at M[x3004] adds −1 to the contents of R2, which keeps track of 
how many times the value 5 has been added to R3.

At this point, the PC contains the address x3005. The BR instruction in M[x3005] loads the PC with the address x3003 if the result of the 
previous instruction (the one in M[x3004]) is not 0. If the result of the previous instruction is 0, the BR instruction does nothing, and 
so the next instruction to be executed is the instruction at M[x3006], i.e., the incremented PC is x3006.

Thus, the two ADD instructions execute again and again, until the result of executing the instruction in M[x3004] produces the value 0, 
indicating that the value 5 has been added four times. Finally, the TRAP instruction in M[x3006] is executed, which is a call to the 
operating system to halt the computer.

chapter 5

The ISA: Overview

The ISA speciﬁes all the information about the computer that the software has to be aware of. In other words, the ISA speciﬁes everything 
in the computer that is available to a programmer when he/she writes programs in the computer’s own machine language. Most people, however, 
do not write programs in the computer’s own machine language, but rather opt for writing programs in a high-level language like C++ or 
Python (or Fortran or COBOL, which have been around for more than 50 years). Thus, the ISA also speciﬁes everything in the computer that 
is needed by someone (a compiler writer) who wishes to translate programs written in a high-level language into the machine language of 
the computer.

The ISA speciﬁes the memory organization, register set, and instruction set, including the opcodes, data types, and addressing modes of 
the instructions in the instruction set.

Memory Organization

The LC-3 memory has an address space of 2^(16) (i.e., 65,536) locations, and an addressability of 16 bits. Not all 65,536 addresses are 
actually used for memory locations. Since the normal unit of data that is processed in the LC-3 is 16 bits, we refer to 16 bits as one 
word, and we say the LC-3 is word-addressable.

Registers

Since it usually takes far more than one clock cycle to obtain data from memory, the LC-3 provides (like almost all computers) 
additional temporary storage locations that can be accessed in a single clock cycle.

The most common type of temporary storage locations, and the one used in the LC-3, is a set of registers. Each register in the set is 
called a general purpose register (GPR). Like memory locations, registers store information that can be operated on later. The number of 
bits stored in each register is usually one word. In the LC-3, this means 16 bits.

Registers must be uniquely identiﬁable. The LC-3 speciﬁes eight GPRs, each identiﬁed by a three-bit register number. They are referred to 
as R0, R1, … R7. Figure 5.1 shows a snapshot of the LC-3’s register set, sometimes called a register ﬁle, with the eight values 1, 3, 5, 
7, −2, −4, −6, and −8 stored in R0, … R7, respectively.

Recall from Chapter 4 that the instruction to ADD the contents of R0 to R1 and store the result in R2 is speciﬁed as

15 14 13 12  11 10 9   8  7  6   5   4  3   2  1  0
0  0  0  1 | 0  1  0 | 0  0  0 | 0 | 0  0 | 0  0  1
  ADD           R2        R0                  R1


where the two sources of the ADD instruction are speciﬁed in bits [8:6] and bits [2:0]. The destination of the ADD result is speciﬁed in 
bits [11:9]. Figure 5.2 shows the contents of the register ﬁle of Figure 5.1 AFTER the instruction "ADD  R2,  R1,  R0." is executed.

The Instruction Set

The instruction set is deﬁned by its set of opcodes, data types, and addressing modes. The addressing modes determine where the operands 
are located. The data type is the representation of the operands in 0s and 1s.

The instruction ADD R2, R0, R1 has an opcode ADD, one addressing mode (register mode), and one data type (2’s complement integer). The 
instruction directs the computer to perform a 2’s complement integer addition and speci- ﬁes the locations (GPRs) where the computer is 
expected to ﬁnd the operands and the location (a GPR) where the computer is to write the result.

We saw in Chapter 4 that the ADD instruction can also have two addressing modes (register mode and immediate mode), where one of the two 
operands is literally contained in bits [4:0] of the instruction.

Figure 5.3 lists all the instructions of the LC-3, the bit encoding [15:12] for each opcode, and the format of each instruction. Some of 
them you will recognize from Chapter 4. Many others will be explained in Sections 5.2, 5.3, and 5.4.

Opcodes

Some ISAs have a very large number of opcodes, one for each of a very large number of tasks that a program may wish to carry out. The x86 
ISA has more than 200 opcodes. Other ISAs have a very small set of opcodes. Some ISAs have specific opcodes to help with processing 
scientific calculations. For example, the Hewlett Packard Precision Architecture can specify the compound operation (A ⋅ B) + C with one 
opcode; that is, a multiply, followed by an add on three source operands A, B, and C. Other ISAs have instructions that process video 
images obtained from the World Wide Web. The Intel x86 ISA added a number of instructions which they originally called MMX instructions 
because they eXtended the ISA to assist with MultiMedia applications that use the web. Still other ISAs have specific opcodes to help with 
handling the tasks of the operating system. For example, the VAX ISA, popular in the 1980s, used a single opcode instead of a long 
sequence of instructions that other computers used, to save the information associated with a program that was in the middle of executing 
prior to switching to another program. The decision as to which instructions to include or leave out of an ISA is usually a hotly debated 
topic in a company when a new ISA is being specified.

The LC-3 ISA has 15 instructions, each identiﬁed by its unique opcode. The opcode is speciﬁed in bits [15:12] of the instruction. Since 
four bits are used to specify the opcode, 16 distinct opcodes are possible. However, the LC-3 ISA speciﬁes only 15 opcodes. The code 1101 
has been left unspeciﬁed, reserved for some future need that we are not able to anticipate today.

As we already discussed brieﬂy in Chapter 4, there are three diﬀerent types of instructions, which means three diﬀerent types of opcodes: 
operates, data movement, and control. Operate instructions process information. Data movement instructions move information between 
memory and the registers and between registers/memory and input/output devices. Control instructions change the sequence of instructions 
that will be executed. That is, they enable the execution of an instruction other than the one that is stored in the next sequential 
location in memory.

Data Types

A data type is a representation of information such that the ISA has opcodes that operate on that representation. There are many ways to 
represent the same information in a computer. That should not surprise us, since in our daily lives, we regularly represent the same 
information in many diﬀerent ways. For example, a child, when asked how old he is, might hold up three ﬁngers, signifying that he is 3 
years old. If the child is particularly precocious, he might write the decimal digit 3 to indicate his age. Or, if the child is a CS or 
CE major at the university, he might write 0000000000000011, the 16-bit binary representation for 3. If he is a chemistry major, he might 
write 3.0 ⋅ 10^0 . All four represent the same value: 3.

In addition to the representation of a single number by diﬀerent bit patterns in diﬀerent data types, it is also the case that the same 
bit pattern can correspond to diﬀerent numbers, depending on the data type. For example, the 16 bits 0011000100110000 represent the 2’s 
complement integer 12,592, the ASCII code for 10, and a bit vector such that b₁3, b₁2, b₇, b₄, and b₃ have the relevant property of the 
bit vector.

That should also not surprise us, since in our daily lives, the same representation can correspond to multiple interpretations, as is 
the case with a red light. When you see it on the roadway while you are driving, it means you should stop. When you see it at Centre Bell 
where the Montreal Canadiens play hockey, it means someone has just scored a goal.

Every opcode will interpret the bit patterns of its operands according to the data type it is designed to support. In the case of the ADD 
opcode, for example, the hardware will interpret the bit patterns of its operands as 2’s complement integers. Therefore, if a programmer 
stored the bit pattern 0011000100110000 in R3, thinking that the bit pattern represented the integer 10, the instruction ADD R4, R3, #10 
would write the integer 12,602 into R4, and not the ASCII code for the integer 20. Why? Because the opcode ADD interprets the bit patterns 
of its operands as 2’s complement integers, and not ASCII codes, regardless what the person creating those numbers intended.

Addressing Modes

An addressing mode is a mechanism for specifying where the operand is located. An operand can generally be found in one of three places: 
in memory, in a register, or as a part of the instruction. If the operand is a part of the instruction, we refer to it as a literal or as 
an immediate operand. The term literal comes from the fact that the bits of the instruction literally form the operand. The term 
immediate comes from the fact that we can obtain the operand immediately from the instruction, that is, we don’t have to look elsewhere 
for it.

The LC-3 supports ﬁve addressing modes: immediate (or literal), register, and three memory addressing modes: PC-relative, indirect, and 
Base+oﬀset. We will see in Section 5.2 that operate instructions use two addressing modes: register and immediate. We will see in Section 
5.3 that data movement instructions use four of the ﬁve addressing modes.

Condition Codes

One ﬁnal item will complete our overview of the ISA of the LC-3: condition codes. The LC-3 has three single-bit registers that are 
individually set (set to 1) or cleared (set to 0) each time one of the eight general purpose registers is written into as a result of 
execution of one of the operate instructions or one of the load instructions. Each operate instruction performs a computation and writes 
the result into a general purpose register. Each load instruction reads the contents of a memory location and writes the value found 
there into a general purpose register. We will discuss all the operate instructions in Section 5.2 and all the load instructions in 
Section 5.3.

The three single-bit registers are called N, Z, and P, corresponding to their meaning: negative, zero, and positive. Each time a GPR is 
written by an operate or a load instruction, the N, Z, and P one-bit registers are individually set to 0 or 1, corresponding to whether 
the result written to the GPR is negative, zero, or positive. That is, if the result is negative, the N register is set, and Z and P are 
cleared. If the result is zero, Z is set and N and P are cleared. If the result is positive, P is set and N and Z are cleared.

The set of three single-bit registers are referred to as condition codes because the condition of those bits are used to change the 
sequence of execution of the instructions in a computer program. Many ISAs use condition codes to change the execution sequence. SPARC 
and x86 are two examples. 

Operate Instructions

ADD, AND, and NOT

Operate instructions process data. Arithmetic operations (like ADD, SUB, MUL, and DIV) and logical operations (like AND, OR, NOT, XOR) 
are common examples. The LC-3 has three operate instructions: ADD, AND, and NOT.

The NOT (opcode = 1001) instruction is the only operate instruction that performs a unary operation, that is, the operation requires one 
source operand. The NOT instruction bit-wise complements a 16-bit source operand and stores the result in a destination register. NOT 
uses the register addressing mode for both its source and destination. Bits [8:6] specify the source register and bits [11:9] specify the 
destination register. Bits [5:0] must contain all 1s.

If R5 initially contains 0101000011110000, after executing the following instruction:

15    14    13    12     11    10    9     8    7    6     5    4    3    2    1    0
1     0     0     1  |   0     1     1  |  1    0    1  |  1    1    1    1    1    1
      NOT                     R3                R5

R3 will contain 1010111100001111.

Figure 5.4 shows the key parts of the data path that are used to perform the NOT instruction shown here. Since NOT is a unary operation, 
only the A input of the ALU is relevant. It is sourced from R5. The control signal to the ALU directs the ALU to perform the bit-wise 
complement operation. The output of the ALU (the result of the operation) is stored in R3 and the condition codes are set, completing the 
execution of the NOT instruction.

Recall from Chapter 4 that the ADD (opcode = 0001) and AND (opcode = 0101) instructions both perform binary operations; they require two 
16-bit source operands. The ADD instruction performs a 2’s complement addition of its two source operands. The AND instruction performs a 
bit-wise AND of each pair of bits of its two 16-bit operands. Like the NOT, the ADD and AND use the register addressing mode for one of 
the source operands and for the destination operand. Bits [8:6] specify the source register, and bits [11:9] specify the destination 
register (where the result will be written).

Immediates

The second source operand for both ADD and AND instructions (as also discussed in Chapter 4) can be speciﬁed by either register mode or 
as an immediate operand. Bit [5] determines which. If bit [5] is 0, then the second source operand uses a register, and bits [2:0] 
specify which register. In that case, bits [4:3] are set to 0 to complete the speciﬁcation of the instruction.

In the ADD instruction shown below, if R4 contains the value 6 and R5 contains the value −18, then R1 will contain the value −12 after 
the instruction is executed.

15    14    13    12     11    10    9  |  8    7    6     5     4    3     2    1    0
0     0      0     1  |  0     0     1  |  1    0    0  |  0  |  0    0  |  1    0    1
        ADD                    R1               R4                            R5

If bit [5] is 1, the second source operand is contained within the instruction. In that case the second source operand is obtained by 
sign-extending bits [4:0] to 16 bits before performing the ADD or AND. The result of the ADD (or AND) instruction is written to the 
destination register and the condition codes are set, completing the execution of the ADD (or AND) instruction. Figure 5.5 shows the key 
parts of the data path that are used to perform the instruction "ADD  R1,  R4,  #-2."

Since the immediate operand in an ADD or AND instruction must ﬁt in bits [4:0] of the instruction, not all 2’s complement integers can be 
immediate operands. Question: Which integers are OK (i.e., which integers can be used as immediate operands)? range of -32 to 16

What does the following instruction do?

15    14    13    12     11    10    9  |  8    7    6     5     4    3    2    1    0
0     1      0     1  |  0     1     0  |  0    1    0  |  1  |  0    0    0    0    0

ANSWER:    Register 2 is cleared (i.e., set to all 0s).

What does the following instruction do?

15    14    13    12     11    10    9  |  8    7    6     5     4    3    2    1    0
0     0      0     1  |  1     1     0  |  1    1    0  |  1  |  0    0    0    0    1

ANSWER:    Register 6 is incremented (i.e., R6 ← R6 + 1).

Note that a register can be used as a source and also as a destination in the same instruction. This is true for all instructions in 
the LC-3.

Recall that the negative of an integer represented in 2’s complement can be obtained by complementing the number and adding 1. Therefore, 
assuming the values A and B are in R0 and R1, what sequence of three instructions performs “A minus B” and writes the result into R2?

ANSWER:

15    14    13    12     11    10    9     8    7    6     5    4    3    2    1    0
1     0     0     1  |   0     0     1  |  0    0    1  |  1    1    1    1    1    1     R1  ←  NOT(B)
      NOT                     R1                R1


15    14    13    12     11    10    9  |  8    7    6     5     4    3    2    1    0    R2  ←  -B + 1
0     0      0     1  |  0     1     0  |  0    0    1  |  1  |  0    0    0    0    1
         ADD                  R2                R1


15    14    13    12     11    10    9  |  8    7    6     5     4    3     2    1    0    R2  ←  A  +(-B)
0     0      0     1  |  0     1     0  |  0    0    0  |  0  |  0    0  |  0    1    0
         ADD                  R2               R0                               R2


Question: What distasteful result is also produced by this sequence? How can it easily be avoided?


The LEA Instruction (Although Not Really an Operate)

Where to put the LEA instruction is a matter for debate (when you have nothing more important to do!). It does not really operate on data, 
it simply loads a register with an address. It clearly does not move data from memory to a register, nor is it a control instruction. We 
had to put it somewhere, so we chose to discuss it here!

LEA (opcode  =  1110) loads the register speciﬁed by bits [11:9] of the instruction with the value formed by adding the incremented 
program counter to the sign-extended bits [8:0] of the instruction. We saw this method of constructing an address in Chapter 4 with the 
LD instruction. However, in this case, the instruction does not access memory, it simply loads the computed address into a register. 
Perhaps a better name for this opcode would be CEA (for Compute Eﬀective Address). However, since many microprocessors in industry that 
have this instruction in their ISAs call it LEA (for Load Eﬀective Address), we have chosen to use the same acronym.

We shall see shortly that the LEA instruction is useful to initialize a register with an address that is very close to the address of the 
instruction doing the initializing.

If memory location x4018 contains the instruction LEA R5, #−3, and the PC contains x4018,

15    14    13    12     11    10    9  |  8    7    6     5     4    3    2    1    0
1     1      1     0  |  1     0     1  |  1    1    1     1     1    1    1    0    1
        LEA                    R5                        -3

R5 will contain x4016 after the instruction at x4018 is executed. Question: Why will R5 not contain the address x4015? because the PC will
be incremented during the fetch phase

Figure 5.6 shows the relevant parts of the data path required to execute the LEA instruction. Note that the value to be loaded into the 
register does not involve any access to memory. ...nor does it have any eﬀect on the condition codes.

Data Movement Instructions

Data movement instructions move information between the general purpose registers and memory and between the registers and the 
input/output devices. We will ignore for now the business of moving information from input devices to registers and from registers to 
output devices. This will be an important part of Chapter 9. In this chapter, we will conﬁne ourselves to moving information between 
memory and the general purpose registers.

The process of moving information from memory to a register is called a load, and the process of moving information from a register to 
memory is called a store. In both cases, the information in the location containing the source operand remains unchanged. In both cases, 
the location of the destination operand is over-written with the source operand, destroying in the process the previous value that was in 
the destination location.

The LC-3 contains six instructions that move information: LD, LDR, LDI, ST, STR, and STI.

The format of the load and store instructions is as follows:

15    14    13    12    11    10    9    8     7    6     5     4     3     2     1     0
         opcode       |     DR or SR   |                Addr Gen bits

Data movement instructions require two operands, a source and a destination. The source is the data to be moved; the destination is the 
location where it is moved to. One of these locations is a register, the other is a memory location or an input/output device. In this 
chapter we will assume the second operand is in memory. In Chapter 9 we will study the cases where the second operand is an input or 
output device.

Bits [11:9] specify one of these operands, the register. If the instruction is a load, DR refers to the destination general purpose 
register that will contain the value after it is read from memory (at the completion of the instruction cycle). If the instruction is a 
store, SR refers to the register that contains the value that will be written to memory.

Bits [8:0] contain the address generation bits. That is, bits [8:0] contain information that is used to compute the 16-bit address of 
the second operand. In the case of the LC-3’s data movement instructions, there are three ways to interpret bits [8:0]. They are 
collectively called addressing modes. The opcode speciﬁes how to interpret bits [8:0]. That is, the LC-3’s opcode speciﬁes which of the 
three addressing modes should be used to obtain the address of the operand from bits [8:0] of the instruction.

PC-Relative Mode

LD (opcode = 0010) and ST (opcode = 0011) specify the PC-relative addressing mode. It is so named because bits [8:0] of the instruction 
specify an oﬀset relative to the PC. The memory address is computed by sign-extending bits [8:0] to 16 bits and adding the result to the 
incremented PC. The incremented PC is the contents of the program counter after the FETCH phase, that is, after the PC has been 
incremented. If the instruction is LD, the computed address (PC + oﬀset) speciﬁes the memory location to be accessed. Its contents is 
loaded into the register speciﬁed by bits [11:9] of the instruction. If the instruction is ST, the contents of the register speciﬁed by 
bits [11:9] of the instruction is written into the memory location whose address is PC + oﬀset. ...and the N, Z, and P one-bit condition 
codes are set depending on whether the value loaded is negative, positive, or zero.

If the following instruction is located at x4018, it will cause the contents of x3FC8 to be loaded into R2.

15    14    13    12     11    10    9     8    7    6    5    4    3    2    1    0
0     0     1     0   |  0     1     0  |  1    1    0    1    0    1    1    1    1
         LD                    R2                       x1AF

Figure 5.7 shows the relevant parts of the data path required to execute this instruction. The three steps of the LD instruction are 
identiﬁed. In step 1, the incremented PC (x4019) is added to the sign-extended value contained in IR [8:0] (xFFAF), and the result (x3FC8) 
is loaded into the MAR. In step 2, memory is read and the contents of x3FC8 is loaded into the MDR. Suppose the value stored in x3FC8 is 
5. In step 3, the value 5 is loaded into R2, and the NZP condition codes are set, completing the instruction cycle.

Note that the address of the memory operand is limited to a small range of the total memory. That is, the address can only be within +256 
or −255 locations of the LD or ST instruction. This is the range provided by the sign-extended value contained in bits [8:0] of the 
instruction. If a load instruction needs to access a memory location further away from the load instruction, one of the other two 
addressing modes must be used.

Indirect Mode

LDI (opcode = 1010) and STI (opcode = 1011) specify the indirect addressing mode. An address is ﬁrst formed exactly the same way as with 
LD and ST. However, instead of this address being the address of the operand to be loaded or stored, it is the address of the address of 
the operand to be loaded or stored. Hence the name indirect. Note that the address of the operand can be anywhere in the computer’s memory, 
not just within the range provided by bits [8:0] of the instruction as is the case for LD and ST. The destination register for the LDI 
and the source register for STI, like all the other loads and stores, are speciﬁed in bits [11:9] of the instruction.

If the instruction

15    14    13    12     11    10    9     8    7    6    5    4    3    2    1    0
1     0     1     0   |  0     1     1  |  1    1    1    0    0    1    1    0    0
         LDI                   R3                       x1CC

is in x4A1B, and the contents of x49E8 is x2110, execution of this instruction results in the contents of x2110 being loaded into R3.

Figure 5.8 shows the relevant parts of the data path required to execute this instruction. As is the case with the LD and ST instructions, 
the ﬁrst step consists of adding the incremented PC (x4A1C) to the sign-extended value contained in IR [8:0] (xFFCC), and the result 
(x49E8) loaded into the MAR. In step 2, memory is in x4A1B and x2110 is in x49E8, and execution of this instruction results in the 
contents of x2110 being loaded into R3. In step 3, since x2110 is not the operand, but the address of the operand, it is loaded into the 
MAR. In step 4, memory is again read, and the MDR again loaded. This time the MDR is loaded with the contents of x2110. Suppose the value 
−1 is stored in memory location x2110. In step 5, the contents of the MDR (i.e., −1) is loaded into R3 and the NZP condition codes are set, 
completing the instruction cycle.

Base+offset Mode

LDR (opcode = 0110) and STR (opcode = 0111) specify the Base+oﬀset addressing mode. The Base+oﬀset mode is so named because the address 
of the operand is obtained by adding a sign-extended six-bit oﬀset to a base register. The six-bit oﬀset is obtained from the instruction, 
bits [5:0]. The base register is speciﬁed by bits [8:6] of the instruction.

If R2 contains the 16-bit quantity x2345, the following instruction loads R1 with the contents of x2362.

15    14    13    12     11    10    9     8    7    6     5    4    3    2    1    0
0     1     1     0   |  0     0     1  |  0    1    0  |  0    1    1    1    0    1
         LDR                   R1               R2                    x1D

Figure 5.9 shows the relevant parts of the data path required to execute this instruction. First the contents of R2 (x2345) is added to 
the sign-extended value contained in IR [5:0] (x001D), and the result (x2362) is loaded into the MAR. Second, memory is read, and the 
contents of x2362 is loaded into the MDR. Suppose the value stored in memory location x2362 is x0F0F. Third, and ﬁnally, the contents of 
the MDR (in this case, x0F0F) is loaded into R1 and the NZP condition codes are set, completing the execution of the LDR instruction.

Note that the Base+oﬀset addressing mode also allows the address of the operand to be anywhere in the computer’s memory.

An Example

We conclude our study of addressing modes with a comprehensive example. Assume the contents of memory locations x30F6 through x30FC are 
as shown in Figure 5.10, and the PC contains x30F6. We will examine the eﬀects of carrying out the seven instructions starting at 
location x30FC.

Since the PC points initially to location x30F6, the ﬁrst instruction to be executed is the one stored in location x30F6. The opcode of 
that instruction is 1110, load eﬀective address (LEA). LEA loads the register speciﬁed by bits [11:9] with the address formed by 
sign-extending bits [8:0] of the instruction and adding the result to the incremented PC. The 16-bit value obtained by sign-extending bits 
[8:0] of the instruction is xFFFD. The incremented PC is x30F7. Therefore, at the end of execution of the LEA instruction, R1 contains 
x30F4, and the PC contains x30F7.

Next, the instruction stored in location x30F7 is executed. Since the opcode 0001 speciﬁes ADD, the sign-extended immediate in bits [4:0] 
(since bit [5] is 1) is added to the contents of the register speciﬁed in bits [8:6], and the result is written to the register speciﬁed 
by bits [11:9]. Since the previous instruction wrote x30F4 into R1, and the sign-extended immediate value is x000E, the sum is x3102. At 
the end of execution of this instruction, R2 contains x3102, and the PC contains x30F8. R1 still contains x30F4.

Next, the instruction stored in x30F8. The opcode 0011 speciﬁes the ST instruction, which stores the contents of the register speciﬁed by 
bits [11:9] (R2) into the memory location whose address is computed using the PC-relative addressing mode. That is, the address is 
computed by adding the incremented PC (x30F9) to the 16-bit value obtained by sign-extending bits [8:0] of the instruction (xFFFB). 
Therefore, at the end of execution of the ST instruction, memory location x30F4 (i.e., x30F9 + xFFFB) contains the value stored in R2 
(x3102) and the PC contains x30F9.

Next the instruction at x30F9. The AND instruction, with an immediate operand x0000. At the end of execution, R2 contains the value 0, 
and the PC contains x30FA.

At x30FA, the opcode 0001 speciﬁes the ADD instruction. After execution, R2 contains the value 5, and the PC contains x30FB.

At x30FB, the opcode 0111 signiﬁes the STR instruction. STR (like LDR) uses the Base+oﬀset addressing mode. The memory address is obtained 
by adding the contents of the BASE register (speciﬁed by bits [8:6]) to the sign-extended oﬀset contained in bits [5:0]. In this case, 
bits [8:6] specify R1, which contains x30F4. The 16-bit sign-extended oﬀset is x000E. Since x30F4 + x000E is x3102, the memory address is 
x3102. The STR instruction stores into x3102 the contents of the register speciﬁed by bits [11:9], in this case R2. Since R2 contains the 
value 5, at the end of execution of this instruction, M[x3102] contains the value 5, and the PC contains x30FC.

Finally the instruction at x30FC. The opcode 1010 speciﬁes LDI. LDI (like STI) uses the indirect addressing mode. The memory address is 
obtained by ﬁrst forming an address as is done in the PC-relative addressing mode. Bits [8:0] are sign-extended to 16 bits (xFFF7) and 
added to the incremented PC (x30FD). Their sum (x30F4) is the address of the operand address. Since M[x30F4] contains x3102, x3102 is 
the operand address. The LDI instruction loads the value found at this address (in this case 5) into the register identiﬁed by bits [11:9] 
of the instruction (in this case R3). At the end of execution of this instruction, R3 contains the value 5 and the PC contains x30FD.

Control Instructions

Control instructions change the sequence of instructions to be executed. If there were no control instructions, the next instruction 
fetched after the current instruction ﬁnishes would always be the instruction located in the next sequential memory location. As you know, 
this is because the PC is incremented in the FETCH phase of each instruction cycle. 

The LC-3 has ﬁve opcodes that enable the sequential execution ﬂow to be broken: conditional branch, unconditional jump, subroutine call 
(sometimes called function), TRAP, and RTI (Return from Trap or Interrupt). In this section, we will deal almost entirely with the most 
common control instruction, the conditional branch. We will also discuss the unconditional jump and the TRAP instruction. The TRAP 
instruction, often called service call, is useful because it allows a programmer to get help from the operating system to do things that
the typical programmer does not fully understand how to do. Typical examples: getting information into the computer from input devices, 
displaying information to output devices, and stopping the computer. The TRAP instruction breaks the sequential execution of a user 
program to start a sequence of instructions in the operating system. How the TRAP instruction does this, and in fact, most of the 
discussion of the TRAP instruction and all of the discussion of the subroutine call and the return from interrupt we will leave for 
Chapters 8 and 9.

Conditional Branches

Of the ﬁve instructions which change the execution ﬂow from the next sequential instruction to an instruction located someplace else in 
the program, only one of them decides each time it is executed whether to execute the next instruction in sequence or whether to execute 
an instruction from outside that sequence. The instruction that makes that decision each time it is executed is the conditional branch 
instruction BR (opcode = 0000).

Like all instructions in the LC-3, the PC is incremented during the FETCH phase of its instruction cycle. Based on the execution of 
previous instructions in the program, the conditional branch’s EXECUTE phase either does nothing or it loads the PC with the address of 
the instruction it wishes to execute next. If the conditional branch instruction does nothing during the EXECUTE phase, then the 
incremented PC will remain unchanged, and the next instruction executed will be the next instruction in sequence.

That decision, whether to do nothing to the incremented PC or whether to change it, is based on previous results computed by the program, 
which are reﬂected in the condition codes. We will explain.

The format of the conditional branch instruction is as follows:

15    14    13    12     11    10     9    8    7    6    5    4    3    2    1    0
0     0     0     0   |  n   |  z  |  p  |               PCoﬀset

Bits [11], [10], and [9] are associated with the three condition codes, N, Z, and P.

As you know, the three operate instructions (ADD, AND, and NOT) and the three load instructions (LD, LDI, and LDR) in the LC-3 write 
values into general purpose registers, and also set the three condition codes in accordance with whether the value written is negative, 
zero, or positive.

The conditional branch instruction uses that information to determine whether or not to depart from the usual sequential execution of 
instructions that we get as a result of incrementing the PC during the FETCH phase of each instruction.

We said (without explanation) in the computer program we studied in Section 4.4 that if bits [11:9] of the conditional branch instruction 
are 101, we will depart from the usual sequential execution if the last value written into a register by one of the six instructions 
listed above is not 0. We are now ready to see exactly what causes that to happen.

During the EXECUTE phase of the BR instruction cycle, the processor examines the condition codes whose associated bits in the instruction, 
bits [11:9], are 1. Note the lower case n, z, and p in bits [11:9] of the BR instruction format shown above. If bit [11] is 1, 
condition code N is examined. If bit [10] is 1, condition code Z is examined. If bit [9] is 1, condition code P is examined. If any of 
bits [11:9] are 0, the associated condition codes are not examined. If any of the condition codes that are examined are set (i.e., equal 
to 1), then the PC is loaded with the address obtained in the EVALUATE ADDRESS phase. If none of the condition codes that are examined 
are set, the incremented PC is left unchanged, and the next sequential instruction will be fetched at the start of the next instruction 
cycle.

The address obtained during the EVALUATE ADDRESS phase of the instruction cycle is generated using the PC-relative addressing mode.

In our example in Section 4.4, the ADD instruction in memory location x3004 subtracted 1 from R2, wrote the result to R2, and set the 
condition codes. The BR instruction in memory location x3005 shows bits [11:9] = 101. Since bit [11] is 1, if the N bit is set, the 
result of the ADD must have been negative. Since bit [9] is also 1, if the P bit is set, the result must have been positive. Since bit 
[10] is 0, we do not examine the Z bit. Thus if the previous result is positive or negative (i.e., not 0), the PC is loaded with x3003, 
the address calculated in the EVALUATE ADDRESS phase of the branch instruction.

Recall that the program of Figure 4.7 used R2 to keep track of the number of times the number 5 was added to R3. As long as we were not 
done with all our additions, the result of subtracting 1 from R2 was not zero. When we were done with our additions, subtracting 1 from 
R2 produced the result 0, so Z was set to 1, N and P were set to 0. At that point, bits [11:9] checked the N and P condition codes which 
were 0, so the incremented PC was not changed, and the instruction at location x3006, a trap to the operating system to halt the computer, 
was executed next.

Let’s Look at Another Example. Suppose the following instruction is located at x4027, and the last value loaded into a general purpose 
register was 0.

15    14    13    12    11   10   9    8    7    6    5    4    3    2    1    0
0     0     0     0  |  0    1    0  | 0    1    1    0    1    1    0    0    1
         BR             n    z    p                   x0D9

Figure 5.11 shows the data path elements that are required to execute this instruction. Note the logic required to determine whether the 
sequential instruction ﬂow should be broken. Each of the three AND gates corresponds to one of the three condition codes. The output of 
the AND gate is 1 if the corresponding condition code is 1 and if the associated bit in the instruction directs the hardware to check that 
condition code. If any of the three AND gates have an output 1, the OR gate has an output 1, indicating that the sequential instruction 
ﬂow should be broken, and the PC should be loaded with the address evaluated during the EVALUATE ADDRESS phase of the instruction cycle.

In the case of the conditional branch instruction at x4027, the answer is yes, and the PC is loaded with x4101, replacing x4028, which 
had been loaded into the PC during the FETCH phase of the BR instruction.

Another Example. If all three bits [11:9] are 1, then all three condition codes are examined. In this case, since the last result stored 
into a register had to be either negative, zero, or positive (there are no other choices!), one of the three condition codes must be in 
state 1. Since all three are examined, the PC is loaded with the address obtained in the EVALUATE ADDRESS phase. We call this an 
unconditional branch since the instruction ﬂow is changed unconditionally, that is, independent of the data.

For example, if the following instruction, located at x507B, is executed, the PC is loaded with x5001.

15    14    13    12    11   10   9    8    7    6    5    4    3    2    1    0
0     0     0     0  |  1    1    1  | 1    1    0    0    0    0    1    0    1
         BR             n    z    p                   x185

Question: What happens if all three bits [11:9] in the BR instruction are 0?

Two Methods of Loop Control

We saw in Section 4.4 in our multiplication program that we repeatedly executed a sequence of instructions until the value in a register 
was zero. We call that sequence a loop body, and each time the loop body is executed we call it one iteration of the loop body. The BR 
instruction at the end of the sequence controls the number of times the loop body is executed. There are two common ways to control the 
number of iterations.

Loop Control with a Counter Suppose we know that the 12 locations x3100 to x310B contain integers, and we wish to compute the sum of 
these 12 integers. A ﬂowchart for an algorithm to solve the problem is shown in Figure 5.12.

First, as in all algorithms, we must initialize our variables. That is, we must set up the initial values of the variables that the 
computer will use in executing the program that solves the problem. There are three such variables: the address of the next integer to 
be added (assigned to R1), the running sum (assigned to R3), and the number of integers left to be added (assigned to R2). The three 
variables are initialized as follows: The address of the ﬁrst integer to be added is put in R1. R3, which will keep track of the 
running sum, is initialized to 0. R2, which will keep track of the number of integers left to be added, is initialized to 12. Then the 
process of adding begins.

The program repeats the process of loading into R4 one of the 12 integers and adding it to R3. Each time we perform the ADD, we 
increment R1 so it will point to (i.e., contain the address of) the next number to be added and decrement R2 so we will know how many 
numbers still need to be added. When R2 becomes zero, the Z condition code is set, and we can detect that we are done.

The 10-instruction program shown in Figure 5.13 accomplishes the task. The details of the program execution are as follows: The program 
starts with PC = x3000. The ﬁrst instruction (at location x3000) initializes R1 with the address x3100. (The incremented PC is x3001; 
the sign-extended PCoﬀset is x00FF.)

The instruction at x3001 clears R3. R3 will keep track of the running sum, so it must start with the value 0. As we said previously, 
this is called initializing the SUM to zero.

The instructions at x3002 and x3003 initialize R2 to 12, the number of inte- gers to be added. R2 will keep track of how many numbers 
have already been added. This will be done (by the instruction in x3008) by decrementing R2 after each addition takes place.

The instruction at x3004 is a conditional branch instruction. Note that bit [10] is a 1. That means that the Z condition code will be 
examined. If it is set, we know R2 must have just been decremented to 0. That means there are no more numbers to be added, and we are 
done. If it is clear, we know we still have work to do, and we continue with another iteration of the loop body.

The instruction at x3005 loads the next integer into R4, and the instruction at x3006 adds it to R3.

The instructions at x3007 and x3008 perform the necessary bookkeeping. The instruction at x3007 increments R1, so R1 will point to the 
next location in memory containing an integer to be added. The instruction at x3008 decrements R2, which is keeping track of the number 
of integers still to be added, and sets the condition codes.

The instruction at x3009 is an unconditional branch, since bits [11:9] are all 1. It loads the PC with x3004. It also does not aﬀect 
the condition codes, so the next instruction to be executed (the conditional branch at x3004) will be based on the instruction executed 
at x3008.

This is worth saying again. The conditional branch instruction at x3004 follows the instruction at x3009, which does not aﬀect 
condition codes, which in turn follows the instruction at x3008. Thus, the conditional branch instruction at x3004 will be based on the 
condition codes set by the instruction at x3008. The instruction at x3008 sets the condition codes based on the value produced by 
decrementing R2. As long as there are still integers to be added, the ADD instruction at x3008 will produce a value greater than zero 
and therefore clear the Z condition code. The conditional branch instruction at x3004 examines the Z condition code. As long as Z is 
clear, the PC will not be aﬀected, and the next iteration of the loop body will begin. That is, the next instruction cycle will start 
with an instruction fetch from x3005.

The conditional branch instruction causes the execution sequence to follow: x3000, x3001, x3002, x3003, x3004, x3005, x3006, x3007, 
x3008, x3009, x3004, x3005, x3006, x3007, x3008, x3009, x3004, x3005, and so on. The loop body consists of the instructions at x3005 
to x3009. When the value in R2 becomes 0, the PC is loaded with x300A, and the program continues at x300A with its next activity.

You may have noticed that we can remove the branch instruction at x3004 if we replace the unconditional branch instruction at x3009 
with a conditional branch that tests for not 0 (i.e., bits [11:9]=101), and branches to the instruction currently located in x3005. 
It is tempting to do that since it decreases the loop body by one instruction. BUT, we admonish you not to do that! The program as 
shown obeys the rules of structured programming that we will discuss in Chapter 6. The shortcut does work for this simple example, but 
it breaks the methodology of structured programming. You do not want to get in the habit of taking such shortcuts, since for larger 
programs it is a clear invitation to disaster. More on this in Chapter 6.

Finally, it is worth noting that we could have written a program to add these 12 integers without any control instructions. We still 
would have needed the LEA instruction in x3000 to initialize R1. We would not have needed the instruction at x3001 to initialize the 
running sum, nor the instructions at x3002 and x3003 to initialize the number of integers left to be added. We could have loaded the 
contents of x3100 directly into R3, and then repeatedly (by incrementing R1), loaded subsequent integers into R4 and adding R4 to the 
running sum in R3 11 more times! After the addition of the twelfth integer, we would go on to the next task, as does the example of 
Figure 5.13 with the branch instruction in x3004.

Unfortunately, instead of a 10-instruction program, we would have a 35-instruction program. Moreover, if we had wished to add 100 
integers without any control instructions instead of 12, we would have had a 299-instruction program instead of 10. The control 
instructions in the example of Figure 5.13 permit the reuse of sequences of code (the loop body) by breaking the sequential instruction 
execution ﬂow.

Loop Control with a Sentinel 

The example above controls the number of times the loop body executes by means of a counter. We knew we wanted to execute the loop 12 
times, so we simply set a counter to 12, and then after each execution of the loop, we decremented the counter and checked to see if it 
was zero. If it was not zero, we set the PC to the start of the loop and continued with another iteration.

A second method for controlling the number of executions of a loop is to use a sentinel. This method is particularly eﬀective if we do 
not know ahead of time how many iterations we will want to perform. Each iteration is usually based on processing a value. We append to 
our sequence of values to be processed a value that we know ahead of time can never occur (i.e., the sentinel). For example, if we are 
adding a sequence of numbers, a sentinel could be a letter A or a *, that is, something that is not a number. Our loop test is simply 
a test for the occurrence of the sentinel. When we ﬁnd it, we know we are done.

Suppose we know the values stored in locations x3100 to x310B are all positive. Then we could use any negative number as a sentinel. 
Let’s say the sentinel stored at memory address x310C is −1. The resulting ﬂowchart for this solution is shown in Figure 5.14, and the 
resulting program is shown in Figure 5.15.

As before, the instruction at x3000 loads R1 with the address of the ﬁrst value to be added, and the instruction at x3001 initializes 
R3 (which keeps track of the sum) to 0.

At x3002, we load the contents of the next memory location into R4. If the sentinel is loaded, the N condition code is set.

The conditional branch at x3003 examines the N condition code. If N=1, PC is loaded with x3008 and onto the next task. If N=0, R4 must 
contain a valid number to be added. In this case, the number is added to R3 (x3004), R1 is incremented to point to the next memory 
location (x3005), R4 is loaded with the contents of the next memory location (x3006), and the PC is loaded with x3003 to begin the next 
iteration (x3007).

The JMP Instruction

The conditional branch instruction, for all its capability, does have one unfortunate limitation. The next instruction executed must 
be within the range of addresses that can be computed by adding the incremented PC to the sign- extended oﬀset obtained from bits [8:0] 
of the instruction. Since bits [8:0] specify a 2’s complement integer, the next instruction executed after the conditional branch can 
be at most +256 or −255 locations from the branch instruction itself.

What if we would like to execute next an instruction that is 2000 locations from the current instruction? We cannot ﬁt the value 2000 
into the nine-bit ﬁeld; ergo, the conditional branch instruction does not work.

The LC-3 ISA does provide an instruction JMP (opcode = 1100) that can do the job.

The JMP instruction loads the PC with the contents of the register speciﬁed by bits [8:6] of the instruction. If the following JMP 
instruction is located at address x4000,

15    14    13    12    11    10    9    8    7    6    5    4    3    2    1    0
1     1     0     0  |  0     0     0  | 0    1    0  | 0    0    0    0    0    0
      JMP                                    BaseR

R2 contains the value x6600, and the PC contains x4000, then the instruction at x4000 (the JMP instruction) will be executed, followed 
by the instruction located at x6600. Since registers contain 16 bits (the full address space of memory), the JMP instruction has no 
limitation on where the next instruction to be executed must reside.

The TRAP Instruction

We will discuss the details of how the TRAP instruction works in Chapter 9. However, because it will be useful long before that to get 
data into and out of the computer, we discuss the TRAP instruction here. The TRAP (opcode = 1111) instruction changes the PC to a 
memory address that is part of the operating system so that the operating system will perform some task on behalf of the program that 
is executing. In the language of operating system jargon, we say the TRAP instruction invokes an operating system service call. Bits 
[7:0] of the TRAP instruction form the trapvector, an eight-bit code that identiﬁes the service call that the program wishes the 
operating system to perform on its behalf. Table A.2 contains the trapvectors for all the service calls that we will use with the LC-3 
in this book.

15    14    13    12    11    10    9    8    7    6    5    4    3    2    1    0
1      1     1     1  |  0     0    0    0  |          trapvector

Once the operating system is ﬁnished performing the service call, the program counter is set to the address of the instruction 
following the TRAP instruction, and the program continues. In this way, a program can, during its execution, request services from the 
operating system and continue processing after each such service is performed. The services we will require for now are:

*  Input a character from the keyboard (trapvector = x23).
*  Output a character to the monitor (trapvector = x21).
*  Halt the program (trapvector = x25).

Another Example: Counting Occurrences of a Character

We will ﬁnish our introduction to the ISA of the LC-3 with another example program. Suppose we would like to be able to input a 
character from the keyboard, then count the number of occurrences of that character in a ﬁle, and ﬁnally display that count on the 
monitor. We will simplify the problem by assuming that the number of occurrences of any character that we would be interested in is 
small enough that it can be expressed with a single decimal digit. That is, there will be at most nine occurrences. This simpliﬁcation 
allows us to not have to worry about complex conversion routines between the binary count and the ASCII display on the monitor—a 
subject we will get into in Chapter 10, but not today.

Figure 5.16 is a ﬂowchart of the algorithm that solves this problem. Note that each step is expressed both in English and also (in 
parentheses) in terms of an LC-3 implementation.

The ﬁrst step is (as always) to initialize all the variables. This means providing starting values (called initial values) for R0, 
R1, R2, and R3, the four registers the computer will use to execute the program that will solve the problem. R2 will keep track of the 
number of occurrences; in Figure 5.16, it is referred to as Count. It is initialized to zero. R3 will point to the next character in 
the ﬁle that is being examined. We refer to it as a pointer since it points to (i.e., contains the address of) the location where the 
next character of the ﬁle that we wish to examine resides. The pointer is initialized with the address of the ﬁrst character in the ﬁle. 
R0 will hold the character that is being counted; we will input that character from the keyboard and put it in R0. R1 will hold, in 
turn, each character that we get from the ﬁle being examined.

We should also note that there is no requirement that the ﬁle we are examining be close to or far away from the program we are 
developing. For example, it is perfectly reasonable for the program we are developing to start at x3000 and the ﬁle we are examining to 
start at x9000. If that were the case, in the initialization process, R3 would be initialized to x9000.

The next step is to count the number of occurrences of the input character. This is done by processing, in turn, each character in the 
ﬁle being examined, until the ﬁle is exhausted. Processing each character requires one iteration of a loop. Recall from Section 5.4.3 
that there are two common methods for keeping track of iterations of a loop. We will use the sentinel method, using the ASCII code for 
EOT (End of Transmission) (00000100) as the sentinel. A table of ASCII codes is in Appendix E.

In each iteration of the loop, the contents of R1 is ﬁrst compared to the ASCII code for EOT. If they are equal, the loop is exited, 
and the program moves on to the ﬁnal step, displaying on the screen the number of occurrences. If not, there is work to do. R1 (the 
current character under examination) is compared to R0 (the character input from the keyboard). If they match, R2 is incremented. In 
either case, we move on to getting the next character. The pointer R3 is incremented, the next character is loaded into R1, and the 
program returns to the test that checks for the sentinel at the end of the ﬁle.

When the end of the file is reached, all the characters have been examined, and the count is contained as a binary number in R2. In 
order to display the count on the monitor, it is first converted to an ASCII code. Since we have assumed the count is less than 10, 
we can do this by putting a leading 0011 in front of the four-bit binary representation of the count. Note in Figure E.2 the 
relationship between the binary value of each decimal digit between 0 and 9 and its corresponding ASCII code. Finally, the count is 
output to the monitor, and the program terminates.

Figure 5.17 is a machine language program that implements the ﬂowchart of Figure 5.16.

First the initialization steps. The instruction at x3000 clears R2 by ANDing it with x0000. The instruction at x3001 loads the starting 
address of the ﬁle to be examined into R3. Again, we note that this ﬁle can be anywhere in memory. Prior to starting execution at 
x3000, some sequence of instructions must have stored the ﬁrst address of this ﬁle in x3012. Location x3002 contains the TRAP 
instruction, which requests the operating system to perform a service call on behalf of this program. The function requested, as 
identiﬁed by the eight-bit trapvector 00100011 (i.e., x23), is to load into R0 the ASCII code of the next character typed on the 
keyboard. Table A.2 lists trapvectors for all operating system service calls that can be performed on behalf of a user program. The 
instruction at x3003 loads the character pointed to by R3 into R1.

Then the process of examining characters begins. We start (x3004) by subtracting 4 (the ASCII 
code for EOT) from R1 and storing it in R4. If the result is zero, the end of the ﬁle has been 
reached, and it is time to output the count. The instruction at x3005 conditionally branches 
to x300E, where the process of outputting the count begins.

If R4 is not equal to zero, the character in R1 is legitimate and must be examined. The 
sequence of instructions at locations x3006, x3007, and x3008 determines whether the contents 
of R1 and R0 are identical. Taken together, the three instructions compute
                   
                                 R0 − R1

This produces all zeros only if the bit patterns of R1 and R0 are identical. If the bit 
patterns are not identical, the conditional branch at x3009 branches to x300B; that is, it 
skips the instruction at x300A, which increments the counter (R2).

The instruction at x300B increments R3, so it will point to the next character in the ﬁle being 
examined, the instruction at x300C loads that character into R1, and the instruction at x300D 
unconditionally takes us back to x3004 to start processing that character.

When the sentinel (EOT) is ﬁnally detected, the process of outputting the count begins (at 
x300E). The instruction at x300E loads 00110000 into R0, and the instruction at x300F adds the 
count to R0. This converts the binary representation of the count (in R2) to the ASCII 
representation of the count (in R0). The instruction at x3010 invokes a TRAP to the operating 
system to output the contents of R0 to the monitor. When that is done and the program resumes 
execution, the instruction at x3011 invokes a TRAP instruction to terminate the program.

Question: Can you improve the execution of the above program? Hint: How many times are the 
instructions at x3006 and x3007 executed. What small change will decrease the total number of 
instructions that have to be executed.

The Data Path Revisited

Before we leave Chapter 5, let us revisit the data path diagram that we ﬁrst encountered in 
Chapter 3 (Figure 3.35). Many of the structures we have seen earlier in this chapter in Figures 
5.4, 5.5, 5.6, 5.7, 5.8, 5.9, and 5.11. We reproduce the data path diagram as Figure 5.18. 
Note at the outset that there are two kinds of arrows in the data path, those with arrowheads 
ﬁlled in and those with arrowheads not ﬁlled in. Filled-in arrowheads designate information 
that is processed. Unﬁlled-in arrowheads designate control signals. Control signals emanate 
from the block labeled “Finite State Machine.” The connections from the ﬁnite state machine to 
most control signals have been left oﬀ Figure 5.18 to reduce unnecessary clutter in the 
diagram.

Basic Components of the Data Path

The Global Bus

The most obvious item on the data path diagram is the heavy black structure with arrowheads at 
both ends. This represents the data path’s global bus. The LC-3 global bus consists of 16 wires 
and associated electronics. It allows one structure to transfer up to 16 bits of information to 
another structure by making the necessary electronic connections on the bus. Exactly one value 
can be transferred on the bus at one time. Note that each structure that supplies values to the 
bus has a triangle just behind its input arrow to the bus. This triangle (called a tri-state device) 
allows the computer’s control logic to enable exactly one supplier to provide information to the bus 
at any one time. The structure wishing to obtain the value being supplied can do so by asserting 
its LD.x (load enable) signal (recall our discussion of gated latches in Section 3.4.2). Not 
all computers have a single global bus. The pros and cons of a single global bus is yet another topic 
that will have to wait for later in your education.

Memory

One of the most important parts of any computer is the memory that contains both instructions 
and data. Memory is accessed by loading the memory address register (MAR) with the address of 
the location to be accessed. To perform a load, control signals then read the contents of that 
memory location, and the result of that read is delivered by the memory to the memory data 
register (MDR). On the other hand, to perform a store, what is to be stored is loaded into the 
MDR. Then the control signals assert a write enable (WE) signal in order to store the value
contained in MDR in the memory location speciﬁed by MAR.

The ALU and the Register File

The ALU is the processing element. It has two inputs, source 1 from a register and source 2 
from either a register or the sign-extended immediate value provided by the instruction. The 
registers (R0 through R7) can provide two values: source 1, which is controlled by the three-bit 
register number SR1, and source 2, which is controlled by the three-bit register number SR2. 
SR1 and SR2 are ﬁelds in the LC-3 operate instructions. The selection of a second register 
operand or a sign-extended immediate operand is determined by bit [5] of the LC-3 instruction.
Note the mux that provides source 2 to the ALU. The select line of that mux is bit [5] of the 
LC-3 operate instruction.

The results of an ALU operation are (a) a result that is stored in one of the registers, and 
(b) the three single-bit condition codes. Note that the ALU can supply 16 bits to the bus, and 
that value can then be written into the register specified by the three-bit register number DR. 
Also, note that the 16 bits supplied to the bus are also input to logic that determines whether 
that 16-bit value is negative, zero, or positive. The three one-bit condition code registers N, 
Z, and P are set accordingly.

The PC and the PCMUX

At the start of each instruction cycle, the PC supplies to the MAR over the global bus the 
address of the instruction to be fetched. In addition, the PC, in turn, is supplied via the 
three-to-one PCMUX. During the FETCH phase of the instruction cycle, the PC is incremented and 
written into the PC. That is shown as the rightmost input to the PCMUX.

If the current instruction is a control instruction, then the relevant source of the PCMUX 
depends on which control instruction is currently being processed. If the current instruction 
is a conditional branch and the branch is taken, then the PC is loaded with the incremented PC 
+ PCoﬀset (the 16-bit value obtained by sign-extending IR [8:0]). Note that this addition takes 
place in the special adder and not in the ALU. The output of the adder is the middle input to 
PCMUX. The third input to PCMUX is obtained from the global bus. Its use will become clear after 
we discuss other control instructions in Chapters 9.

The MARMUX

As you know, memory is accessed by supplying the address to the MAR. The MARMUX controls which 
of two sources will supply the MAR with the appropriate address during the execution of a load, 
a store, or a TRAP instruction. The right input to the MARMUX is obtained by adding either the 
incremented PC or a base register to zero or a literal value supplied by the IR. Whether the PC 
or a base register and what literal value depends on which opcode is being processed. The control 
signal ADDR1MUX speciﬁes the PC or base register. The control signal ADDR2MUX speciﬁes which of 
four values is to be added. The left input to MARMUX provides the zero-extended trapvector, which 
is needed to invoke service calls, and will be discussed in detail in Chapter 9.

The Instruction Cycle Speciﬁc to the LC-3

We complete our tour of the LC-3 data path by following the ﬂow through an instruction cycle. 
Suppose the content of the PC is x3456 and the content of location x3456 is

15  14  13  12   11  10  9    8   7  6    5   4   3   2   1   0
0    1   1   0 |  0   1  1  | 0   1  0  | 0   0   0   1   0   0
    LDR              R3           R2               4

Suppose the LC-3 has just completed processing the instruction at x3455, which happened to be 
an ADD instruction.

FETCH

As you know, the instruction cycle starts with the FETCH phase. That is, the
instruction is obtained by accessing memory with the address contained in the PC.
In the ﬁrst cycle, the contents of the PC is loaded via the global bus into the MAR,
and the PC is incremented and loaded into the PC. At the end of this cycle, the
PC contains x3457. In the next cycle (if memory can provide information in one
cycle), the memory is read, and the instruction 0110011010000100 is loaded into
the MDR. In the next cycle, the contents of the MDR is loaded into the instruction
register (IR), completing the FETCH phase.

DECODE

In the next cycle, the contents of the IR is decoded, resulting in the control
logic providing the correct control signals (unﬁlled arrowheads) to control the processing of 
the rest of this instruction. The opcode is 0110, identifying the LDR instruction. This means 
that the Base+oﬀset addressing mode is to be used to determine the address of data to be 
loaded into the destination register R3.) and the sign-extended
bits [5:0] of the IR are added and supplihe register to be read to obtain the base address.
ADDR1MUX seleced via the MARMUX to the MA

EVALUATE ADDRESS

In the next cycle, the contents of R2 (the base registerR.
The SR1 ﬁeld speciﬁes 010, tts SR1OUT, and ADDR2MUX selects the second from the
right source.

OPERAND FETCH

In the next cycle (or more than one, if memory access takes more than one cycle),
the value at that address is loaded into the MDR.

EXECUTE

The LDR instruction does not require an EXECUTE phase, so this phase takes
zero cycles.

STORE RESULT

In the last cycle, the contents of the MDR is gated onto the global bus, from which
it is loaded into R3 and supplied to the condition code logic in order to set the
NZP condition codes.

exercises

Given instructions ADD, JMP, LEA, and NOT, identify whether the instructions are operate 
instructions, data movement instructions, or control instructions. For each instruction, list 
the addressing modes that can be used with the instruction.

ADD -> 0001 -> operate instruction -> register and immediate addressing modes
JMP -> 1100 -> control instruction -> register mode
LEA -> 1110 -> data movement instruction -> immediate addressing
NOT -> 1001 -> operate instruction -> register mode

A memory’s addressability is 64 bits. What does that tell you about the
size of the MAR and MDR? both contain 64 bits each

There are two common ways to terminate a loop. One way uses a counter
to keep track of the number of iterations. The other way uses an element
called a sentinel. What is the distinguishing characteristic of this element? when encountered
the loop is broken; they are used when we don't know how many iterations will be performed

Say we have a memory consisting of 256 locations, and each location contains 16 bits.
a. How many bits are required for the address? 8 bits
b. If we use the PC-relative addressing mode, and want to allow control transfer between 
instructions 20 locations away, how many bits of a branch instruction are needed to specify 
the PC-relative oﬀset? 5
c. If a control instruction is in location 3, what is the PC-relative oﬀset of address 10? 13
Assume that the control transfer instructions work the same way as in the LC-3.

a. What is an addressing mode? method of finding source operands for an instruction
b. Name three places an instruction’s operands might be located? in a register, in an instruction,
and in memory
c. List the ﬁve addressing modes of the LC-3, and for each one state where the operand is 
located (from part b). register, literal or immediate (in the instruction), pc-relative (bits 8:0 are sign
extended and then added to PC), base+offset (base register specified by bits 8:6 added to sign extended bits
5:0), indirect addressing (use pc-relative addressing mode except it points to the address of an address of the operand)

What is the largest positive number we can represent literally (i.e., as an immediate value) 
within an LC-3 ADD instruction? 15

We want to increase the number of registers that we can specify in the LC-3 ADD instruction to 
32. Do you see any problem with that? Explain.

We would like to have an instruction that does nothing. Many ISAs
actually have an opcode devoted to doing nothing. It is usually called
NOP, for NO OPERATION. The instruction is fetched, decoded, and
executed. The execution phase is to do nothing! Which of the following
three instructions could be used for NOP and have the program still work
correctly?
a. 0001 001 001 1 00000 Add R1, R1, #0 =>differs from a NOP in that it sets the CC’s.
b. 0000 111 000000001   BRnzp#1=> Unconditionally branches to one afte the next address in the PC. Therefore
no, this instruction is not the same as NOP.
c. 0000 000 000000000 Branch that is never taken. Yes same as NOP.

We wish to execute a single LC-3 instruction that will subtract the decimal number 20 from 
register 1 and put the result into register 2. Can we do it? If yes, do it. If not, explain why not. No. We cannot do it in a single instruction as the 
smallest representable integer with the 5 bits available for the immediate field in the ADD instruction 
is -16. However this could be done in two instructions.

How might one use a single LC-3 instruction to move the value in R2 into R3?
0001 011 010 1 00000 (ADD R3, R2, #0)

The LC-3 has no subtract instruction. How could one perform the following operation using only 
three LC-3 instructions: R1 ← R2 − R3
1001 011 011 111111 (NOT R3, R3 )
0001 011 011 1 00001 (ADD R3, R3, #1 )
0001 001 010 0 00011 (ADD R1, R2, R3 )

Using only one LC-3 instruction and without changing the contents of any register, how might 
one set the condition codes based on the value that resides in R1?
0001 001 001 1 00000 (ADD R1, R1, #0)
or
0101 001 001 1 11111 (AND R1, R1, #-1)

Is there a sequence of LC-3 instructions that will cause the condition codes at the end of the 
sequence to be N = 1, Z = 1, and P = 0? Explain. Can’t happen. The condition where N=1, Z=1 and P=0 
would require the contents of a register to be both negative and zero.

Write an LC-3 instruction that clears the contents of R2. 
0101 010 010 1 00000 (AND R2, R2, #0)

How many times does the LC-3 make a read or write request to memory during the processing of 
the LD instruction? two, once to fetch the instruction, once to fetch thedata. 
How many times during the processing of the LDI instruction? three, once to fetch the 
instruction, once to fetch the data address, and once to fetch the data.
How many times during the processing of the LEA instruction? once, only to fetch the instruction. 
Processing includes all phases of the instruction cycle.

The LC-3 Instruction Register (IR) is made up of 16 bits, of which the least signiﬁcant nine 
bits [8:0] represent the PC-relative oﬀset for the LD instruction. If we change the ISA so 
that bits [6:0] represent the PC-relative oﬀset, what is the new range of addresses we can 
load data from using the LD instruction? PC-64 to PC+63. The PC value used here is the incremented PC value.

What is the maximum number of TRAP service routines that the LC-3 ISA can support? Explain.
The Trap instruction provides 8 bits for a trap vector. That means there could be 2^8 = 256 trap routines.

Suppose the following LC-3 program is loaded into memory starting at location x30FF:

x30FF 1110 0010 0000 0001     --> reg 1 0x3101
x3100 0110 0100 0100 0010     --> reg 2 0x3103
x3101 1111 0000 0010 0101
x3102 0001 0100 0100 0001
x3103 0001 0100 1000 0010

If the program is executed, what is the value in R2 at the end of execution? 0x3103 but the answers
claim it is R2 <- 0x1482. why?

The LC-3 ISA contains the instruction LDR DR, BaseR, oﬀset. After the instruction is decoded, 
the following operations (called microinstructions) are carried out to complete the processing 
of the LDR instruction:

MAR ← BaseR + SEXT(Offset6) ; set up the memory address
MDR ← Memory[MAR] ; read mem at BaseR + offset
DR ← MDR ; load DR

Suppose that the architect of the LC-3 wanted to include an instruction MOVE DR, SR that would 
copy the memory location with address given by SR and store it into the memory location whose 
address is in DR.

The MOVE instruction is not really necessary since it can be accomplished with a sequence of 
existing LC-3 instructions. What sequence of existing LC-3 instructions implements (also called
“emulates”) MOVE R0, R1? LDR R2, R1, #0 ; load R2 with contents of location pointed to by R1
STR R2, R0, #0; store those contents into location pointed to by R0

If the MOVE instruction were added to the LC-3 ISA, what sequence of microinstructions, 
following the decode operation, would emulate MOVE DR,SR?

The constituent micro-ops are:
MAR <− SR
MDR<− Mem[MAR]
MAR <− DR
Mem[MAR]<−MDR

Using the overall data path in Figure 5.18, identify the elements that implement the ADD 
instruction of Figure 5.5. The IR, SEXT unit, SR2 MUX, Reg File and ALU implement the ADD 
instruction, along with NZP and the logic which goes with it.

Using the overall data path in Figure 5.18, identify the elements that
implement the LDI instruction of Figure 5.8. Memory, MDR, MAR, IR, PC, RegFile, the SEXT unit 
connected to IR[8:0], ADDR2MUX, ADDR1MUX set to PC, along with the ADDER they connect to, and 
MAXMUX and GateMARMUX implement the LDI instruction, along with NZP and the logic which goes
with it.

Using the overall data path in Figure 5.18, identify the elements that implement the LEA 
instruction of Figure 5.6. IR, PC, Reg File, the SEXT unit connected to IR[8:0], ADDR2MUX, ADDR1MUX set to
PC, along with the ADDER they connect to, and MAXMUX and GateMARMUX implement the
LEA instruction, along with NZP and the logic which goes with it.

When a computer executes an instruction, the state of the computer is changed as a result of 
that execution. Is there any diﬀerence in the state of the LC-3 computer as a result of 
executing instruction 1 below vs. executing instruction 2 below? Explain. We can assume the 
state of the LC-3 computer before execution is the same in both cases.

instruction 1: 0001 000 000 1 00000    register 0 <-- register 0 + \#0
instruction 2: 0000 111 000000000      branch to PC' + \#0 if any of N,Z,orP is set

Yes, there is difference. Instruction 1 (ADD) sets the Condition Codes while Instruction 2
(BR) doesn’t.


In class we showed the ﬁrst few states of the ﬁnite state machine that is
required for processing instructions of a computer program written for
LC-3. In the ﬁrst state, the computer does two things, represented as:

MAR <-- PC
PC <-- PC+1

Why does the microarchitecture put the contents of the PC into the
MAR? Why does the microarchitecture increment the PC? PC is put into MAR to fetch the next 
instruction that needs to be executed. PC is incremented to move to the next instruction to be fetched.

We wish to know if R0 is being used as the Base Register for computing
the address in an LDR instruction. Since the instruction is in memory,
we can load it into R4. And, since the Base Register is identiﬁed in bits
8:6 of the instruction, we can load R5 with 0000000111000000 and then
execute AND R6,R5,R4. We would know that R0 is the base register if
what condition is met? If R6 is zero, then R0 is used as a base register

chapter 6

In this chapter we attempt to do two things: ﬁrst, we develop a methodology for 
constructing programs to solve problems and second, we develop a methodology for ﬁxing 
those programs under the likely condition that we did not get everything right the 
ﬁrst time.

Problem Solving: Systematic Decomposition

Recall from Chapter 1 that in order for electrons to solve a problem, we need to go
through several levels of transformation to get from a natural language description
of the problem (in our case English, to something electrons can deal with. Once
we have a natural language description of the problem, the next step is to trans-
form the problem statement into an algorithm. That is, the next step is to transform
the problem statement into a step-by-step procedure that has the properties of def-
initeness (each step is precisely stated), eﬀective computability (each step can be
carried out by a computer), and ﬁniteness (the procedure terminates).

In the late 1960s, the concept of structured programming emerged as a way
to dramatically improve the ability of average programmers to take a complex
description of a problem and systematically decompose it into smaller and smaller
manageable units so that they could ultimately write a program that executed cor-
rectly. The methodology has also been called systematic decomposition because
the larger tasks are systematically broken down into smaller ones.

We will ﬁnd the systematic decomposition model a useful technique for
designing computer programs to carry out complex tasks.

The Three Constructs: Sequential, Conditional, Iterative

Systematic decomposition is the process of taking a task, that is, a unit of work
(see Figure 6.1a), and breaking it into smaller units of work such that the collec-
tion of smaller units carries out the same task as the one larger unit. The idea is
that if one starts with a large, complex task and applies this process again and
again, one will end up with very small units of work and consequently be able to
easily write a program to carry out each of these small units of work. The process
is also referred to as stepwise reﬁnement, because the process is applied one step
at a time, and each step reﬁnes one of the tasks that is still too complex into a
collection of simpler subtasks.

The idea is to replace each larger unit of work with a construct that correctly
decomposes it. There are basically three constructs for doing this: sequential,
conditional, and iterative.

The sequential construct (Figure 6.1b) is the one to use if the designated
task can be broken down into two subtasks, one following the other. That is, the
computer is to carry out the ﬁrst subtask completely, then go on and carry out the
second subtask completely— never going back to the ﬁrst subtask after starting
the second subtask.

The conditional construct (Figure 6.1c) is the one to use if the task consists
of doing one of two subtasks but not both, depending on some condition. If the 
condition is true, the computer is to carry out one subtask. If the condition is not 
true, the computer is to carry out a different subtask. Either subtask may be vacuous; 
that is, it may do nothing. Regardless, after the correct subtask is completed, the 
program moves onward. The program never goes back and retests the condition.

The iterative construct (Figure 6.1d) is the one to use if the task consists of
doing a subtask a number of times, but only as long as some condition is true. If
the condition is true, do the subtask. After the subtask is ﬁnished, go back and
test the condition again. As long as the result of the condition tested is true, the
program continues to carry out the same subtask again and again. The ﬁrst time
the test is not true, the program proceeds onward.

Note in Figure 6.1 that whatever the task of Figure 6.1a, work starts with the
arrow into the top of the ‘‘box’’ representing the task and finishes with the arrow out
of the bottom of the box. There is no mention of what goes on inside the box. In each
of the three possible decompositions of Figure 6.1a (i.e., Figure 6.1b, c, and d), there
is exactly one entrance into the construct and exactly one exit out of the construct.
Thus, it is easy to replace any task of the form of Figure 6.1a with whichever of its
three decompositions apply. We will see how with several examples.

LC-3 Control Instructions to Implement the Three Constructs

Before we move on to an example, we illustrate in Figure 6.2 the use of LC-3
control instructions to direct the program counter to carry out each of the three 
decomposition constructs. That is, Figure 6.2b, c, and d corresponds respectively
to the three constructs shown in Figure 6.1b, c, and d.

We use the letters A, B, C, and D to represent addresses in memory containing
LC-3 instructions. The letter A, for example, represents the address of the ﬁrst
LC-3 instruction to be executed in all three cases, since it is the starting address
of the task to be decomposed (shown in Figure 6.2a).

Figure 6.2b illustrates the control ﬂow of the sequential decomposition. Note
that no control instructions are needed since the PC is incremented from Address
B1 to Address B1+1. The program continues to execute instructions through
address D1. It does not return to the ﬁrst subtask.

Figure 6.2c illustrates the control ﬂow of the conditional decomposition.
First, a condition is generated, resulting in the setting of one of the condition
codes. This condition is tested by the conditional branch instruction at Address
B2. If the condition is true, the PC is set to Address C2+1, and subtask 1 is
executed. (Note: x equals 1 + the number of instructions in subtask 2.) If the
condition is false, the PC (which had been incremented during the FETCH phase
of the branch instruction) fetches the instruction at Address B2+1, and subtask
2 is executed. Subtask 2 terminates in a branch instruction that at address C2
unconditionally branches to D2+1. (Note: y equals the number of instructions in
subtask 1.)

Figure 6.2d illustrates the control ﬂow of the iterative decomposition. As
in the case of the conditional construct, ﬁrst a condition is generated, a condi-
tion code is set, and a conditional branch instruction is executed. In this case,
the condition bits of the instruction at address B3 are set to cause a conditional
branch if the condition generated is false. If the condition is false, the PC is set
to address D3+1. (Note: z equals 1 + the number of instructions in the subtask
in Figure 6.2d.) On the other hand, as long as the condition is true, the PC will
be incremented to B3 +1, and the subtask will be executed. The subtask termi-
nates in an unconditional branch instruction at address D3, which sets the PC to
A to again generate and test the condition. (Note: w equals the total number of
instructions in the decomposition shown as Figure 6.2d.)

The Character Count Example from Chapter 5, Revisited

Recall the example of Section 5.5. The statement of the problem is as follows:
“We wish to input a character from the keyboard, count the number of occurrences
of that character in a ﬁle, and display that count on the monitor.”

The systematic decomposition of this English language statement of the
problem to the ﬁnal LC-3 implementation is shown in Figure 6.3. Figure 6.3a
is a brief statement of the problem.

In order to solve the problem, it is always a good idea ﬁrst to examine exactly
what is being asked for, and what is available to help solve the problem. In this
case, the statement of the problem says that we will get the character of inter-
est from the keyboard, and that we must examine all the characters in a ﬁle and
determine how many are identical to the character obtained from the keyboard.
Finally, we must output the result.

