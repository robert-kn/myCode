Why read the book?
1. to understand fundamentals as they do not change very often
2. if fundamentals are mastered clearly, there is no limit to how high I will soar provided I continue to put in the
   work
3. a lot of problems arise from memorising technical details without understanding basic underpinnings
4. when it comes to learning, information hiding approach is prevalent. It is a useful productivity enhancer once 
   fundamentals are clearly understood. But until one gets to that point, information hiding gets in the way of
   understanding.
5. understanding not memorising
6. cutting through the layers: Professionals who use computers in systems today but remain ignorant of what is going
   on underneath are likely to discover the hard way that the eﬀectiveness of their solutions is impacted adversely 
   by things other than the actual programs they write. Serious programmers will write more eﬃcient code if they 
   understand what is going on beyond the statements in their high-level language. Consequently, the high-level 
   programming language course, where the compiler protects the student from everything “ugly” underneath, does not 
   serve most engineering students well, and it certainly does not prepare them for the future.

CHAPTER 1

There is no magic to computing. A computer is a determinisstic system. Everytime we hit it over the head in the same
way and in the same place (provided of course we give it the same starting conditions), we get the same response. A 
computer is not an electronic genius, infact it is an electronic idiot doing exactly what we tell it to do.

What appears to be a very complex organism is really just a very large, systematically interconnected collections of
very simple parts. This book will introduce those simple parts in a step by step manner and build the interconnected
structure that is known by the name computer.

Two themes permeate this book:

(a) the notion of abstraction: it is a technique of establishing a simpler way for a person to interact with a system
    removing details that are unnecessary for interaction with the system. To be successful with the notion of abstraction
    I must combine it with the ability to un-abstract (or deconstruct i.e. the ability to go from system to its 
    individual component parts). If we never have to combine a component with others in order to form a larger system
    and if nothing can go wrong with the component, then it is perfectly fine to understand this component at the level
    of its abstraction.
(b) the importance of not separating your mind the notions of hardware and software. Many computer scientists and 
    engineers refer to themselves as hardware people or software people. The implication is that the person knows a 
    whole lot about one of these two things and precious little about the other. Usually, there is the further 
    implication that it is OK to be an expert at one of these (hardware OR software) and clueless about the other.
    The power of abstraction allows us to “usually” operate at a level where we do not have to think about the 
    underlying layers all the time. This is a good thing. It enables us to be more productive. But if we are clueless
    about the underlying layers, then we are not able to take advantage of the nuances of those underlying layers 
    when it is very important to be able to.

    Always aim to work at a higher level of abstraction, but, if you are able to, at the same time, keep in mind the
    underlying levels, you will ﬁnd yourself able to do a much better job. 

    As you approach your study and practice of computing, we urge you to take the approach that hardware and software
    are names for components of two parts of a computing system that work best when they are designed by people who 
    take into account the capabilities and limitations of both.

    The Bottom Line We believe that whether your inclinations are in the direction of a computer hardware career or 
    a computer software career, you will be much more capable if you master both. 

Re-read the below points upon completion of the book and confirm that you understand all the points:

1. When you study data types, a software concept, in C (Chapter 12), you will understand how the ﬁnite word length 
   of the computer, a hardware concept, aﬀects our notion of data types.
2. When you study functions in C (Chapter 14), you will be able to tie the rules of calling a function with the 
   hardware implementation that makes those rules necessary.
3. When you study recursion, a powerful algorithmic device (initially in Chapter 8 and more extensively in Chapter 
   17), you will be able to tie it to the hardware. If you take the time to do that, you will better understand when
   the additional time to execute a procedure recursively is worth it.
4. When you study pointer variables in C (in Chapter 16), your knowledge of computer memory will provide a deeper 
   understanding of what pointers provide, and very importantly, when they should be used and when they should be 
   avoided.
5. When you study data structures in C (in Chapter 19), your knowledge of computer memory will help you better 
   understand what must be done to manipulate the actual structures in memory eﬃciently.

Computers have seen enormous and unparalleled leaps of performance in a relatively short time they have been around. 
After all, it wasn’t until the 1940s that the first computers showed their faces. One of the first computers was the
ENIAC (the Electronic Numerical Integrator and Calculator), a general purpose electronic computer that could be 
reprogrammed for different tasks. It was designed and built in 1943–1945 at the University of Pennsylvania by 
Presper Eckert and his colleagues.

It contained more than 17,000 vacuum tubes. It was approximately 8 feet high, more than 100 feet wide, and about 3 
feet deep (about 300 square feet of floor space). It weighed 30 tons and required 140 kW to operate. Figure 1.1 shows
three operators programming the ENIAC by plugging and unplugging cables and switches.

About 40 years and many computer companies and computers later, in the early 1980s, the Burroughs A series was born.
One of the dozen or so 18-inch boards that comprise that machine is shown in Figure 1.2. Each board contained 50 or 
more integrated circuit packages. Instead of 300 square feet, it took up around 50 to 60 square feet; instead of 30 
tons, it weighed about 1 ton, and instead of 140 kW, it required approximately 25 kW to operate.

Fast forward another 30 or so years and we ﬁnd many of today’s computers on desktops (Figure 1.3), in laptops 
(Figure 1.4), and most recently in smartphones (Figure 1.5). Their relative weights and energy requirements have 
decreased enormously, and the speed at which they process information has also increased enormously. We estimate 
that the computing power in a smartphone today (i.e., how fast we can compute with a smartphone) is more than four 
million times the computing power of the ENIAC!

The integrated circuit packages that comprise modern digital computers have also seen phenomenal improvement. An 
example of one of today’s microprocessors is shown in Figure 1.6. The ﬁrst microprocessor, the Intel 4004 in 1971, 
contained 2300 transistors and operated at 106 KHz. By 1992, those numbers had jumped to 3.1 million transistors at
a frequency of 66 MHz on the Intel Pentium microprocessor, an increase in both parameters of a factor of about 1000. Today’s microprocessors contain upwards of ﬁve billion transistors and can oper- ate at upwards of 4 GHz, another increase in both parameters of about a factor of 1000.

This factor of one million since 1971 in both the number of transistors and the frequency that the microprocessor 
operates at has had very important implications. The fact that each operation can be performed in one millionth of 
the time it took in 1971 means the microprocessor can do one million things today in the time it took to do one 
thing in 1971. The fact that there are more than a million times as many transistors on a chip means we can do a 
lot more things at the same time today than we could in 1971.

The result of all this is we have today computers that seem able to understand the languages people speak – English,
Spanish, Chinese, for example. We have computers that seem able to recognize faces. Many see this as the magic of 
artiﬁcial intelligence. We will see as we get into the details of how a computer works that much of what appears to 
be magic is really due to how blazingly fast very simple mindless operations (many at the same time) can be carried 
out.

Before we leave this ﬁrst chapter, there are two very important ideas that we would like you to understand, ideas 
that are at the core of what computing is all about.

Idea 1: All computers (the biggest and the smallest, the fastest and the slowest, the most expensive and the 
cheapest) are capable of computing exactly the same things if they are given enough time and enough memory. That is,
anything a fast computer can do, a slow computer can do also. The slow computer just does it more slowly. A more 
expensive computer cannot ﬁgure out something that a cheaper computer is unable to ﬁgure out as long as the cheaper 
computer can access enough memory. (You may have to go to the store to buy more memory whenever it runs out of memory
in order to keep increasing memory.) All computers can do exactly the same things. Some computers can do things 
faster, but none can do more than any other.

Idea 2: We describe our problems in English or some other language spoken by people. Yet the problems are solved by
electrons running around inside the computer. It is necessary to transform our problem from the language of humans 
to the voltages that inﬂuence the ﬂow of electrons. This transformation is really a sequence of systematic 
transformations, developed and improved over the last 70 years, which combine to give the computer the ability to 
carry out what appear to be some very complicated tasks. In reality, these tasks are simple and straightforward.

Before modern computers, there were many kinds of calculating machines. Some were analog machines—machines that 
produced an answer by measuring some physical quantity such as distance or voltage. For example, a slide rule is an
analog machine that multiplies numbers by sliding one logarithmically graded ruler next to another. The user can 
read a logarithmic “distance” on the second ruler. Some early analog adding machines worked by dropping weights on 
a scale. The diﬃculty with analog machines is that it is very hard to increase their accuracy.

This is why digital machines—machines that perform computations by manipulating a ﬁxed ﬁnite set of digits or 
letters— came to dominate computing. You are familiar with the distinction between analog and digital watches. An 
analog watch has hour and minute hands, and perhaps a second hand. It gives the time by the positions of its hands,
which are really angular measures. Digital watches give the time in digits. You can increase accuracy just by adding
more digits. For example, if it is important for you to measure time in hundredths of a second, you can buy a watch
that gives a reading like 10:35.16 rather than just 10:35. How would you get an analog watch that would give you an
accurate reading to one one-hundredth of a second? You could do it, but it would take a mighty long second hand! 
When we talk about computers in this book, we will always mean digital machines.

Before modern digital computers, the most common digital machines in the West were adding machines. In other parts 
of the world another digital machine, the abacus, was common. Digital adding machines were mechanical or 
electromechanical devices that could perform a speciﬁc kind of computation: adding integers. There were also digital
machines that could multiply integers. There were digital machines that could put a stack of cards with punched 
names in alphabetical order. The main limitation of all these machines is that they could do only one speciﬁc kind 
of computation. If you owned only an adding machine and wanted to multiply two integers, you had some 
pencil-and-paper work to do.

This is why computers are diﬀerent. You can tell a computer how to add numbers. You can tell it how to multiply. 
You can tell it how to alphabetize a list or perform any computation you like. When you think of a new kind of 
computation, you do not have to buy or design a new computer. You just give the old computer a new set of 
instructions (a program) to carry out the new computation. This is why we say the computer is a universal 
computational device.

Computer scientists believe that anything that can be computed, can be computed by a computer provided it has 
enough time and enough memory. When we study computers, we study the fundamentals of all computing.

The idea of a universal computational device is due to Alan Turing. Turing proposed in 1937 that all computations 
could be carried out by a particular kind of machine, which is now called a Turing machine. He gave a mathematical 
description of this kind of machine, but did not actually build one. Digital computers were not operating until 
several years later. Turing was more interested in solving a philosophical problem: deﬁning computation. 

How Do We Get the Electrons to Do the Work?

Figure 1.9 shows the process we must go through to get the electrons (which actually do the work) to do our bidding. 
We call the steps of this process the “Levels of Transformation.” As we will see, at each level we have choices. If
we ignore any of the levels, our ability to make the best use of our computing system can be very adversely aﬀected.

We describe the problems we wish to solve in a “natural language.” Natural lan- guages are languages that people 
speak, like English, French, Japanese, Italian, and so on. They have evolved over centuries in accordance with their
usage. They are fraught with a lot of things unacceptable for providing instructions to a computer. Most important 
of these unacceptable attributes is ambiguity. Natural language is ﬁlled with ambiguity. 

The ﬁrst step in the sequence of transformations is to transform the natural lan- guage description of the problem 
to an algorithm, and in so doing, get rid of the objectionable characteristics of the natural language. An 
algorithm is a step-by- step procedure that is guaranteed to terminate, such that each step is precisely stated and
can be carried out by the computer. There are terms to describe each of these properties.

1. effective computability means each step can be carried out by a computer.
2. finiteness means that a procedure terminates.
3. definiteness means that each step is precisely stated.

For every problem there are usually many diﬀerent algorithms for solving that problem. One algorithm may require 
the fewest steps. Another algorithm may allow some steps to be performed concurrently.

The next step is to transform the algorithm into a computer program in one of the programming languages that are 
available. Programming languages are “mechanical languages.” That is, unlike natural languages, mechanical languages
did not evolve through human discourse. Rather, they were invented for use in specifying a sequence of instructions
to a computer. 

There are two kinds of programming languages, high-level languages and low-level languages. High-level languages are
at a distance (a high level) from the underlying computer. At their best, they are independent of the computer on 
which the programs will execute. We say the language is “machine independent.”

Low-level languages are tied to the computer on which the programs will execute. There is generally one such 
low-level language for each computer. That language is called the assembly language for that computer.

The next step is to translate the program into the instruction set of the particular computer that will be used to 
carry out the work of the program. ISA specifies the interface between a computer program and the computer hardware
that will be responsible for executing the instructions. Opcode refers to instructions e.g mul and operand
refers to a data value that is operated on.

An analogy that may be helpful in understanding the concept of an ISA is provided by the automobile. Corresponding 
to a computer program, represented as a sequence of 0s and 1s in the case of the computer, is the human sitting in 
the driver’s seat of a car. Corresponding to the microprocessor hardware is the car itself. The “ISA” of the 
automobile is the speciﬁcation of everything the human needs to know to tell the automobile what to do, and 
everything the automobile needs to know to carry out the tasks speciﬁed by the human driver.

The ISA of a computer serves the same purpose as the “ISA” of an automobile, except instead of the driver and the
car, the ISA of a computer speciﬁes the interface between the computer program directing the computer hardware and 
the hardware carrying out those directions.

For example, consider the set of instructions that the computer can carry out—that is, what operations the computer
can perform and where to get the data needed to perform those operations. The term opcode is used to describe the 
operation. The term operand is used to describe individual data values. 

The ISA speciﬁes the acceptable representations for operands. They are called data types. A data type is a 
representation of an operand such that the computer can perform operations on that representation.

The ISA speciﬁes the mechanisms that the computer can use to ﬁgure out where the operands are located. These 
mechanisms are called addressing modes.

There are many ISA's in existence. Each specifies a different number of opcodes, operands and 
addressing modes from the rest.

The ISA also speciﬁes the number of unique locations that comprise the computer’s memory and the number of 
individual 0s and 1s that are contained in each location.

Many ISAs are in use today. The most widely known example is the x86, introduced by Intel Corporation in 1979 and 
currently also manufactured by AMD and other companies. Other ISAs and the companies responsible for them include 
ARM and THUMB (ARM), POWER and z/Architecture (IBM), and SPARC (Oracle).

The translation from a high-level language (such as C) to the ISA of the computer on which the program will execute
(such as x86) is usually done by a translating program called a compiler. To translate from a program written in C 
to the x86 ISA, one would need a C to x86 compiler. For each high-level language and each desired target ISA, one 
must provide a corresponding compiler.

The translation from the unique assembly language of a computer to its ISA is done by an assembler.

The next step is the implementation of the ISA, referred to as its microarchitecture. The microarchitecture (or 
implementation) of an automobile's ISA is about what goes on underneath the hood. 
Here all automobiles brands and models can be different depending on the cost/performance tradeoffs the 
designer made before the car was manufactured.

Previously, we identiﬁed ISAs of several computer manufactur- ers, including the x86 (Intel), the PowerPC (IBM and 
Motorola), and THUMB (ARM). Each has been implemented by many diﬀerent microarchitectures.

Each microarchitecture is an opportunity for computer designers to make different tradeoffs between the cost 
of the microprocessor, the performance that it will provide, and the energy that it will consume.

The next step is to implement each element of the microarchitecture out of simple logic circuits. Here also there 
are choices, as the logic designer decides how to best make the tradeoﬀs between cost and performance. So, for 
example, even for an operation as simple as addition, there are several choices of logic circuits to perform the 
operation at diﬀering speeds and corresponding costs.

Finally, each basic logic circuit is implemented in accordance with the requirements of the particular device 
technology used. So, CMOS circuits are diﬀerent from NMOS circuits, which are diﬀerent, in turn, from gallium 
arsenide circuits.

At each level of transformation there are choices on how to proceed which determine the resulting cost and 
performance of the computer.

----------------
end of chapter 1
----------------

chapter 2

We noted in Chapter 1 that the computer was organized as a system with several levels of transformation. A problem 
stated in a natural language such as English is actually solved by the electrons moving around inside the components
of the computer.

Inside the computer, millions of very tiny, very fast devices control the movement of those electrons. These 
devices react to the presence or absence of voltages in electronic circuits. They could react to the actual values 
of the voltages, rather than simply to the presence or absence of voltages. However, this would make the control and
detection circuits more complex than they need to be. It is much easier to detect simply whether or not a voltage 
exists at a point in a circuit than it is to measure exactly what that voltage is.

To be perfectly precise, it is not really the case that the computer diﬀerentiates the absolute absence of a voltage
(i.e., 0) from the absolute presence of a voltage (i.e., 1). Actually, the electronic circuits in the computer 
diﬀerentiate voltages close to 0 from voltages far from 0.

With one wire, one can differentiate only two things. One of them can be assigned the value 0, the other can be 
assigned the value 1. But to get useful work done by the computer, it is necessary to be able to differentiate a 
large number of distinct values, and to assign each of them a unique representation. 

We can accomplish this by combining many wires, that is, many bits. For example, if we use eight bits
(corresponding to the voltage present on each of eight wires), we can represent one particular value as 01001110, 
and another value as 11100111. In fact, if we are limited to eight bits we can differentiate at most only 256 values
(2^8) different things.

In general, with k bits, we can distinguish at most 2k distinct items. Each pattern of these k bits is a code; that
is, it corresponds to a particular item (or value).

We symbolically represent the presence of a voltage as “1” and the absence of a voltage as “0.” We refer to each 0 
and each 1 as a “bit,” which is a shortened form of binary digit.

It is not enough simply to represent values; we must be able to operate on those values. We say a particular 
representation is a data type if there are oper- ations in the computer that can operate on information that is 
encoded in that representation. Each instruction set architecture (ISA) has its own set of data types and its own 
set of instructions that can operate on those data types. 

In this book, we will mainly use two data types: 2’s complement integers for representing positive and negative 
integers that we wish to perform arithmetic on, and ASCII codes for representing characters that we wish to input 
to a computer via the keyboard or output from the computer via a monitor.

An unsigned integer has no sign (plus or minus) associated with it. An unsigned integer just has a magnitude. 
Unsigned.

a computer designer could assign any bit pattern to represent any integer he or she wants. Unfortunately, that 
could complicate matters when we try to build an electronic circuit to add two integers. Infct, signed magnitude and
1'complement data types require unnecessarily cumbersome hardware to do addition. On the other hand, the circuitry 
required to add 2 integers using 2's complement data type is much simpler. Because com- puter designers knew what it
would take to design a circuit to add two integers, they chose representations that simpliﬁed the circuit. The 
result is the 2’s complement data type, also shown in Figure 2.1. It is used on just about every computer 
manufactured today.

The choice of representations for the negative integers was based, as we said previously, on the wish to keep the 
logic circuits as simple as possible. Almost all computers use the same basic mechanism to perform addition. It is 
called an arithmetic and logic unit, usually known by its acronym ALU. An ALU has two inputs and one output. It 
performs addition by adding the binary bit patterns at its inputs, producing a bit pattern at its output that is 
the sum of the two input bit patterns.

What is particularly relevant is that the binary ALU does not know (and does not care) what the two patterns it is 
adding represent. It simply adds the two binary patterns. Since the binary ALU only ADDs and does not CARE, it would
be nice if our assignment of codes to the integers resulted in the ALU producing correct results when it added two 
integers.

For starters, it would be nice if, when the ALU adds the representation for an arbitrary integer to the 
representation of the integer having the same magnitude but opposite sign, the sum would be 0.

To accomplish that, the 2’s complement data type speciﬁes the representation for each negative integer so that when
the ALU adds it to the representation of the positive integer of the same magnitude, the result will be the 
representation for 0. Moreover, and actually more importantly, as we sequence through representations of −15 to +15,
the ALU is adding 00001 to each successive representation.

Note in particular the representations for −1 and 0, that is, 11111 and 00000. When we add 00001 to the 
representation for −1, we do get 00000, but we also generate a carry. That carry, however, does not inﬂuence the 
result. That is, the correct result of adding 00001 to the representation for −1 is 0, not 100000. Therefore, the 
carry is ignored. In fact, because the carry obtained by adding 00001 to 11111 is ignored, the carry can always be
ignored when dealing with 2’s complement arithmetic.

manual conversion of decimal fraction (e.g. 0.421) to binary requires the following steps:
see decimalFractionToBinary1.png and decimalFractionToBinary2.png

manual conversion of binary fraction to decimal requires the following steps: see binaryFractionToDecimal.png

manual conversion of decimal to 2's complement can be achieved by following these steps see 
decimalTo2scomplement.png

The value of a positive number does not change if we extend the sign bit 0 as many bit positions to the left 
as desired. Similarly, the value of a negative number does not change by extending the sign bit 1 as many bit 
positions to the left as desired. Since in both cases it is the sign bit that is extended, we refer to the 
operation as Sign-EXTension, often abbreviated SEXT. Sign-extension is performed in order to be able to 
operate on representations of diﬀerent lengths. It does not aﬀect the values of the numbers being represented.

overflow in unsigned arithmetic is relatively straightforward because it results in overflow of the msb 
meaning that the result is less than the value of one of the numbers that was added.

overflow in signed negative numbers during arithmetic is easy to test for because the msb overflows and becomes 0  
which means that the number becomes positive. In contrast, when adding two positive signed numbers, overflow occurs 
when the msb is turned on meaning that the result becomes negative.

Suppose we wish to know if two patterns are identical. Since the XOR function produces a 0 only if the corresponding
pair of bits is identical, two patterns are identical if the output of the XOR is all 0s.

An m-bit pattern where each bit has a logical value (0 or 1) independent of the other bits is called a bit vector. 

There are many other representations of information that are used in computers. Two that are among the most useful 
are the ﬂoating point data type and ASCII codes. 

Floating Point Data Type (Greater Range, Less Precision): Most ISAs today specify more than one ﬂoating point 
data type. One of them, usually called ﬂoat, consists of 32 bits, allocated as follows:

1 bit for the sign (positive or negative)
8 bits for the range (the exponent ﬁeld)
23 bits for precision (the fraction ﬁeld)

Normalised Form; the ﬂoating point data type represents numbers expressed in scientiﬁc notation, and mostly in 
normalized form see 32bitFloatingPoint.png: 

N = (−1)^S × 1.fraction × 2^(exponent−127), 1 ≤ exponent ≤ 254

where S, fraction, and exponent are binary numbers.

The computer’s 32-bit ﬂoating point data type consists of (a) a sign bit (positive or negative), (b) 24 binary
digits in normalised form (one non-zero binary digit to the left of the binary point) times (c) the radix 2 
raised to an exponent expressed in eight bits.

The sign bit S is just a single binary digit, 0 for positive numbers, 1 for negative numbers. The 23 fraction 
bits form the 24-bit quantity 1.fraction, where normalized form demands exactly one non-zero binary digit to 
the left of the binary point. Since there exists only one non-zero binary digit (i.e., the value 1), it is 
unnecessary to explicitly store that bit in our 32-bit ﬂoating point format. In fact that is how we get 24 
bits of precision, the 1 to the left of the binary point that is always present in normalised numbers and so 
is unnecessary to store, and the 23 bits of fraction that are actually part of the 32-bit data type.

We say mostly in normalised form because (as noted in the equation) the data type represents a ﬂoating point 
number in normalised form only if the eight-bit exponent is restricted to the 254 unsigned integer values, 
1 (00000001) through 254 (11111110).

As you know, with eight bits, one can represent 256 values uniquely. For the other two integer values 0 
(00000000) and 255 (11111111), the ﬂoating point data type does not represent normalised numbers.

A normalised number is a representation of a real number in scientific notation where:
1. One non-zero digit precedes the decimal point (mantissa).
2. The exponent is adjusted to ensure the mantissa has a specific range

The eight exponent bits are encoded in what we call an excess code, named for the notion that one can get the 
*real* exponent by treating the code as an unsigned integer and subtracting the excess (sometimes called the 
bias). In the case of the IEEE Floating Point that almost everyone uses, that excess (or bias) is 127 for 
32-bit ﬂoating point numbers.

Encoding is the process of converting data into a format required for various information processing needs, 
including transmission, storage, and retrieval. It involves the use of a code to change original data into a 
form that can be used by an external process. 

The exponent ﬁeld gives us numbers as large as 2^(+127) for an 
exponent ﬁeld containing 254 (11111110) and as small as 2^(−126)  for an exponent ﬁeld containing 1 (00000001).

What does the ﬂoating point data type 00111101100000000000000000000000 represent? see floatingPoint1.png 

How is the number −6 5/8  represented in the ﬂoating point data type? 
floatingPoint2.png 


The following three examples provide further illustrations of the interpretation of the 32-bit ﬂoating point 
data type according to the rules of the IEEE standard.

0  10000011  00101000000000000000000 is 1.00101 ⋅ 2^4  = 18.5

The exponent ﬁeld contains the unsigned number 131. Since 131 − 127 is 4, the exponent is +4. Combining a 1 to
the left of the binary point with the fraction ﬁeld to the right of the binary point yields 1.00101. If we 
move the binary point four positions to the right, we get 10010.1, which is 18.5.

1  10000010  00101000000000000000000 is −1 ⋅ 1.00101 ⋅ 2^3  =−9.25

The sign bit is 1, signifying a negative number. The exponent is 130, signifying an exponent of 130 − 127, or 
+3. Combining a 1 to the left of the binary point with the fraction ﬁeld to the right of the binary point 
yields 1.00101. Moving the binary point three positions to the right, we get 1001.01, which is −9.25.

0  11111110  11111111111111111111111

The sign is +. The exponent is 254 − 127, or +127. Combining a 1 to the left of the binary point with the 
fraction ﬁeld to the right of the binary point yields 1.11111111 … 1, which is approximately 2. Therefore, 
the result is approximately 2^128.

Infinities: We noted before that the ﬂoating point data type represented numbers expressed in scientiﬁc notation in 
normalised form provided the exponent ﬁeld does not contain 00000000 or 11111111. If the exponent ﬁeld 
contains 11111111, we use the ﬂoating point data type to represent various things, among them the notion of 
inﬁnity. Inﬁnity is represented by the exponent ﬁeld containing all 1s and the fraction ﬁeld containing all 0s.
We represent positive inﬁnity if the sign bit is 0 and negative inﬁnity if the sign bit is 1.

Subnormal Numbers

The smallest number that can be represented in normalised form is:

N = 1.00000000000000000000000 × 2^(−126)

What about numbers smaller than 2^(−126) but larger than 0? We call such numbers subnormal numbers because 
they cannot be represented in normalised form. The largest subnormal number is

N = 0.11111111111111111111111 × 2^(−126)

The smallest subnormal number is

N = 0.00000000000000000000001 × 2^(−126) i.e., 2^−23 × 2^−126 which is 2^−149 

Note that the largest subnormal number is 2^−126 minus 2^−149 . Do you see why that is the case? nope

Subnormal numbers are numbers of the form

N = (−1)^s × 0.fraction × 2^−126

We represent them with an exponent ﬁeld of 00000000. The fraction ﬁeld is represented in the same way as with 
normalized numbers. That is, if the exponent ﬁeld contains 00000000, the exponent is −126, and the signiﬁcant 
digits are obtained by starting with a leading 0, followed by a binary point, followed by the 23 bits of the 
fraction ﬁeld.

What number corresponds to the following ﬂoating point representation?

0  00000000  00001000000000000000000

Answer: The leading 0 means the number is positive. The next eight bits, a zero exponent, means the exponent 
is −126, and the bit to the left of the binary point is 0. The last 23 bits form the number 
0.00001000000000000000000, which equals 2^−5 . Thus, the number represented is 2^−5 ⋅ 2^−126 ,which is 2^−131.

Including subnormal numbers allows very, very tiny numbers to be represented.

ASCII Codes: Another representation of information is the standard code that almost all computer equipment 
manufacturers have agreed to use for transferring characters between the main computer processing unit and the
input and output devices. It (ASCII) greatly simpliﬁes the interface between a keyboard manufactured by one 
company, a computer made by another company, and a monitor made by a third company.

Each key on the keyboard is identiﬁed by its unique ASCII code. So, for example, the digit 3 is represented as
00110011, the digit 2 is 00110010, the lowercase e is 01100101, and the ENTER key is 00001101. When you type a
key on the keyboard, the corresponding eight-bit code is stored and made available to the computer.

Most keys are associated with more than one code. For example, the ASCII code for the letter E is 01000101, 
and the ASCII code for the letter e is 01100101. Both are associated with the same key, although in one case 
the Shift key is also depressed while in the other case, it is not.

In order to display a particular character on the monitor, the computer must transfer the ASCII code for that 
character to the electronics associated with the monitor. 

Describe what conditions indicate overﬂow has occurred when two unsigned numbers are added? Overflow has occurred in
an unsigned addition when you get a carry out of the leftmost bits.

Why does the sum of a negative 2’s complement number and a positive 2’s complement number never generate an overﬂow?
Because their sum will be a number which if positive, will have a lower magnitude (less positive) than the original
positive number (because a negative number is being added to it), and vice versa.

what are the masks used for? The masks are used to set bits (by ORing a 1) and to clear bits (by ANDing a 0).

If n and m are both four-bit 2’s complement numbers, and s is the four-bit result of adding them together, how can 
we determine, using only the logical operations described in Section 2.6, if an overﬂow occurred during the addition?
Develop a “procedure” for doing so. The inputs to the procedure are n, m, and s, and the output will be a bit 
pattern of all 0s (0000) if no overﬂow occurred and 1000 if an overﬂow did occur.

[(n AND m AND (NOT s)) OR ((NOT n) AND (NOT m) AND s)] AND 1000

Write IEEE ﬂoating point representation of the following decimal numbers.
a.  3.75             0 10000000 11100000000000000000000
b.  −55 23/64        1 10000100 10111010111000000000000
c.  3.1415927        0 10000000 10010010000111111011011
d.  64,000           0 10001110 11110100000000000000000

----------------------
end chapter 2
----------------------


chapter 3

Most processors these days are manufactured out of MOS transistors (metal oxide semiconductors). There are two
types: p type and n type

When the N-type transistor is supplied with 1.2 volts, the connection from source to drain acts like a piece of
wire. We say (in the language of electricity) that we have a short circuit between the source and drain. If the 
gate of the N-type transistor is supplied with 0 volts, the connection between the source and drain is broken. We 
say that between the source and drain we have an open circuit. see figure32

The P-type transistor works in exactly the opposite fashion from the N-type transistor. When the gate is supplied 
with 0 volts, the P-type transistor acts (more or less) like a piece of wire, closing the circuit. When the gate 
is supplied with 1.2 volts, the P-type transistor acts like an open circuit. Because the P-type and N-type
transistors act in this complementary way, we refer to circuits that contain both P-type and N-type transistors as 
CMOS circuits, for complementary metal-oxide semiconductor. see figure33

One step up from the transistor is the logic gate. That is, we construct basic logic structures out of individual 
MOS transistors.

The NOT Gate (Inverter): It is constructed from two MOS transistors, one P-type and one N-type. See Notgate.png 
shows the behavior of the circuit if the input is supplied with 0 volts. Note that the P-type transistor acts
like a short circuit and the N-type transistor acts like an open circuit. The output is, therefore, connected to 
1.2 volts. On the other hand, if the input is supplied with 1.2 volts, the P-type transistor acts like an open 
circuit, but the N-type transistor acts like a short circuit. The output in this case is connected to ground (i.e.,
0 volts). The complete behavior of the circuit can be described by means of a table, as shown in Figure 3.4c. If we 
replace 0 volts with the symbol 0 and 1.2 volts with the symbol 1, we have the truth table see notgate.png

NOR Gates: a NOR gate contains two P-type and two N-type transistors see NORGate.png
Figure 3.5b shows the behavior of the circuit if A is supplied with 0 volts and B is supplied 
with 1.2 volts. In this case, the lower of the two P-type transistors produces an open circuit, 
and the output C is disconnected from the 1.2-volt power supply. However, the leftmost N-type 
transistor acts like a piece of wire, connecting the output C to 0 volts.

Note that if both A and B are supplied with 0 volts, the two P-type transistors
conduct, and the output C is connected to 1.2 volts. Note further that there is no
ambiguity here, since both N-type transistors act as open circuits, and so C is
disconnected from ground.

If either A or B is supplied with 1.2 volts, the corresponding P-type transistor
results in an open circuit. That is suﬃcient to break the connection from C to
the 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type 
transistors is suﬃcient to cause that transistor to conduct, resulting in C being
connected to ground (i.e., 0 volts).

If we replace the voltages with their logical equivalents, we have the truth
table of Figure 3.5d. 

OR Gate: If we augment the circuit of Figure 3.5a by adding an inverter at 
its output, as shown in Figure 3.6a see OR gate, we have at the output D the logical 
function OR. Figure 3.6a is the circuit for an OR gate. Figure 3.6b describes the behavior 
of this circuit if the input variable A is set to 0 and the input variable B is set to 1. 
Figure 3.6c shows the circuit’s truth table.

AND and NAND Gates: see AND.png if either A or B is supplied with 0 volts,
there is a direct connection from C to the 1.2-volt power supply. The fact that C
is at 1.2 volts means the N-type transistor whose gate is connected to C provides
a path from D to ground. Therefore, if either A or B is supplied with 0 volts, the
output D of the circuit of Figure 3.8 is 0 volts.

Again, we note that there is no ambiguity. The fact that at least one of the two
inputs A or B is supplied with 0 volts means that at least one of the two N-type
transistors whose gates are connected to A or B is open, and that consequently, C
is disconnected from ground. Furthermore, the fact that C is at 1.2 volts means
the P-type transistor whose gate is connected to C is open-circuited. Therefore,
D is not connected to 1.2 volts.

On the other hand, if both A and B are supplied with 1.2 volts, then both
of their corresponding P-type transistors are open. However, their corresponding
N-type transistors act like pieces of wire, providing a direct connection from C
to ground. Because C is at ground, the rightmost P-type transistor acts like a
closed circuit, forcing D to 1.2 volts.

Figure 3.8b is a truth table that summarizes the behavior of the circuit of
Figure 3.8a. Note that the circuit is an AND gate. The circuit shown within the
dashed lines (i.e., having output C) is a NOT-AND gate, which we generally
abbreviate as NAND.

The gates just discussed are very common in digital logic circuits and in
digital computers. There are billions of inverters (NOT gates) in Intel’s Skylake
microprocessor. As a convenience, we can represent each of these gates by standard symbols see symbols.png

The bubble shown in the inverter, NAND, and NOR gates signiﬁes the complement (i.e., NOT) 
function.

Gates with More Than Two Inputs: he notion of AND, OR, NAND, and NOR gates extends to larger numbers of inputs. One
could build a three-input AND gate or a four-input OR gate, for example. An n-input AND gate has an output value of
1 only if ALL n input variables have values of 1. If anyofthen inputs has a value of 0, the output of the n-input 
AND gate is 0. An n-input OR gate has an output value of 1 if ANY of the n input variables has a value of 1. That 
is, an n-input OR gate has an output value of 0 only if ALL n input variables have values of 0. see figure310

Now that we understand the workings of the basic logic gates, the next step is to build some of the logic 
structures that are important components of the microarchitecture of a computer.

There are fundamentally two kinds of logic structures, those that include the storage of information and those that
do not.

Here we will deal with structures that do not store information. These structures are sometimes referred to as 
decision elements. Usually, they are referred to as combinational logic structures because their outputs are strictly
dependent on the combination of input values that are being applied to the structure right now. Their outputs are not
at all dependent on any past history of information that is stored internally, since no information can be stored 
internally in a combinational logic circuit.

Decoder: Figure 3.11 shows a logic gate implementation of a two-input decoder. A decoder
has the property that exactly one of its outputs is 1 and all the rest are 0s. The one
output that is logically 1 is the output corresponding to the input pattern that it is
expected to detect. In general, decoders have n inputs and 2^n outputs. We say the
output line that detects the input pattern is asserted. That is, that output line has
the value 1, rather than 0 as is the case for all the other output lines. In Figure 3.11,
note that for each of the four possible combinations of inputs A and B, exactly one
output has the value 1 at any one time. In Figure 3.11b, the input to the decoder
is 10, resulting in the third output line being asserted.

Mux: The function of a mux is to select one of the inputs (A or B) and connect it to the 
output. The select signal (S in Figure 3.12) determines which input is connected to the output.
Figure 3.12a shows a logic gate implementation of a two-input multiplexer, more commonly 
referred to as a mux.

The mux of Figure 3.12 works as follows: Suppose S = 0, as shown in
Figure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out-
put of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is
whatever the input A is. That is, if A = 0, then the output of the leftmost AND gate
is 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output
of the rightmost AND gate is 0, it has no eﬀect on the OR gate. Consequently,
the output at C is exactly the same as the output of the leftmost AND gate. The
net result of all this is that if S = 0, the output C is identical to the input A.
On the other hand, if S = 1, it is B that is ANDed with 1, resulting in the
output of the OR gate having the value of B.

In summary, the output C is always connected to either the input A or the
input B — which one depends on the value of the select line S. We say S selects the
source of the mux (either A or B) to be routed through to the output C. Figure 3.12c
shows the standard representation for a mux.

In general, a mux consists of 2^n inputs and n select lines. Figure 3.13a
shows a gate-level description of a four-input mux. It requires two select lines.
Figure 3.13b shows the standard representation for a four-input mux.
Question: Can you construct the gate-level representation for an eight-input
mux? yes. How many select lines must you have? 3

A One-Bit Adder (a.k.a. a Full Adder):

A simple algorithm for binary addition is to proceed as you have always done in the case of 
decimal addition, from right to left, one column at a time, adding the two digits from the two 
values plus the carry in, and generating a sum digit and a carry to the next column. The
only diﬀerence here (with binary addition) is you get a carry after 1, rather than
after 9.

Figure 3.14 is a truth table that describes the result of binary addition on one
column of bits within two n-bit operands. At each column, there are three values
that must be added: one bit from each of the two operands A and B and the carry
from the previous column. We designate these three bits as Ai , Bi , and Ci . There
are two results, the sum bit (Si) and the carry over to the next column, Ci+1 . Note
that if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e.,
Ci+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If
all three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and
a carry of 1.

Figure 3.15 shows a logic gate implementation of a one-bit adder. Note that each AND gate in Figure 3.15 produces 
an output 1 for exactly one of the eight input combinations of Ai, Bi, and Ci. The output of the OR gate for Ci+1 
must be 1 in exactly those cases where the corresponding input combinations in Figure 3.14 produce an output 1. 
Therefore, the inputs to the OR gate that generates Ci+1 are the outputs of the AND gates corresponding to those 
input combinations. Similarly, the inputs to the OR gate that generates Si are the outputs of the AND gates 
corresponding to the input combinations that require an output 1 for Si in the truth table of Figure 3.14.

Note that since the input combination 000 does not result in an output 1 for either Ci+1 or S, its corresponding 
AND gate is not an input to either of the two OR gates.

Figure 3.16 shows a circuit for adding two 4-bit binary numbers, using four of the one-bit adder circuits of Figure 
3.15. Note that the carry out of column i is an input to the addition performed in column i + 1.

If we wish to implement a logic circuit that adds two 16-bit numbers, we can do so with a circuit of 16 one-bit 
adders.

We should point out that historically the logic circuit of Figure 3.15 that provides three inputs (Ai, Bi, and Ci) 
and two outputs (the sum bit Si and the carry over to the next column Ci+1) has generally been referred to as a 
full adder to diﬀerentiate it from another structure, which is called a half adder. The distinction between the two 
is the carry bit. Note that the carry into the rightmost column in Figure 3.16 is 0. That is, in the rightmost 
circuit, S0 and C1 depend only on two inputs, A0 and B0. Since that circuit depends on only two inputs, it has been
referred to as a half adder. Since the other circuits depend on all three inputs, they are referred to as full 
adders. We prefer the term one-bit adder as a simpler term for describing what is happening in each column.

The Programmable Logic Array (PLA)

Figure 3.17 illustrates a very common building block for implementing any collection of logic functions one wishes 
to implement. The building block is called a programmable logic array (PLA). It consists of an array of AND gates 
(called an AND array) followed by an array of OR gates (called an OR array). The number of AND gates corresponds 
to the number of input combinations (rows) in the truth table. For n-input logic functions, we need a PLA with 2^n
n-input AND gates. In Figure 3.17, we have 2^3 three-input AND gates, corresponding to three logical input variables.
The number of OR gates corresponds to the number of logic functions we wish to implement, that is, the number of 
output columns in the truth table. The implementation algorithm is simply to connect the output of an AND gate to 
the input of an OR gate if the corresponding row of the truth table produces an output 1 for that output column. 
Hence the notion of programmable. That is, we say we program the connections from AND gate outputs to OR gate 
inputs to implement our desired logic functions

Figure 3.15 shows seven AND gates connected to two OR gates since our requirement was to implement two functions 
(sum and carry) of three input variables. Figure 3.17 shows a PLA that can implement any four functions of three 
variables by appropriately connecting AND gate outputs to OR gate inputs. That is, any function of three variables 
can be implemented by connecting the outputs of all AND gates corresponding to input combinations for which the 
output is 1 to inputs of one of the OR gates. Thus, we could implement the one-bit adder by programming the two OR 
gates in Figure 3.17 whose outputs are W and X by connecting or not connecting the outputs of the AND gates to the 
inputs of those two OR gates as speciﬁed by the two output columns of Figure 3.14.

Logical Completeness

We saw that any logic function we wished to implement could be accomplished with a PLA. We saw that the PLA 
consists of only AND gates, OR gates, and inverters. That means that any logic function can be implemented, 
provided that enough AND, OR, and NOT gates are available. We say that the set of gates {AND, OR, NOT} is logically 
complete because we can build a circuit to carry out the speciﬁcation of any truth table we wish without using any 
other kind of gate. That is, the set of gates {AND, OR, and NOT} is logically complete because a barrel of AND 
gates, a barrel of OR gates, and a barrel of NOT gates are suﬃcient to build a logic circuit that carries out the 
speciﬁcation of any desired truth table. The barrels may have to be big, but the point is, we do not need any other 
kind of gate to do the job.

Question: Is there any single two-input logic gate that is logically complete? no. For example, is the NAND gate 
logically complete? no. Hint: Can I implement a NOT gate with a NAND gate? no. If yes, can I then implement an AND gate 
using a NAND gate followed by a NOT gate? no. If yes, can I implement an OR gate using just AND gates and NOT gates? no

If all of the above is true, then the NAND gate is logically complete, and I can implement any desired logic 
function as described by its truth table with a barrel of NAND gates.

Now we are ready to discuss logic structures that do include the storage of information.


The R-S Latch

A simple example of a storage element is the R-S latch. It can store one bit of information, a 0 or a 1. The R-S 
latch can be implemented in many ways, the simplest being the one shown in Figure 3.18. Two 2-input NAND gates are 
connected such that the output of each is connected to one of the inputs of the other. The remaining inputs S and R 
are normally held at a logic level 1. 

The R-S latch gets its name from the old designations for setting the latch to store a 1 and setting the latch to 
store a 0. Setting the latch to store a 1 was referred to as setting the latch, and setting the latch to store a 0 
was referred to as resetting the latch. Ergo, R-S.

The Quiescent State: We describe the quiescent (or quiet) state of a latch as the state when the latch is storing a 
value, either 0 or 1, and nothing is trying to change that value. This is the case when inputs S and R both have the
logic value 1. In Figure 3.18 the letter a designates the value that is currently stored in the latch, which we also
refer to as the output of the latch.

Consider ﬁrst the case where the value stored and therefore the output a is 1. Since that means the value A is 1 
(and since we know the input R is 1 because we are in the quiescent state), the NAND gate’s output b must be 0. 
That, in turn, means B must be 0, which results in the output a equal to 1. As long as the inputs S and R remain 1, 
the state of the circuit will not change. That is, the R-S latch will continue to store the value 1 (the value of 
the output a).

If, on the other hand, we assume the output a is 0, then A must be 0, and the output b must be 1. That, in turn, 
results in B equal to 1, and combined with the input S equal to 1 (again due to quiescence), results in the output 
a equal to 0. Again, as long as the inputs S and R remain 1, the state of the circuit will not change. In this 
case, we say the R-S latch stores the value 0.

Setting the Latch to a 1 or a 0: The latch can be set to 1 by momentarily setting S to 0, provided we keep the value 
of R at 1. Similarly, the latch can be set to 0 by momentarily setting R to 0, provided we keep the value of S at 1. 
In order for the R-S latch to work properly, both S and R must never be allowed to be set to 0 at the same time.

We use the term set to denote setting a variable to 0 or 1, as in “set to 0” or “set to 1.” In addition, we often 
use the term clear to denote the act of setting a variable to 0.

If we set S to 0 for a very brief period of time, this causes a to equal 1, which in turn causes A to equal 1. Since
R is also 1, the output at b must be 0. This causes B to be 0, which in turn makes a equal to 1. If, after that very
brief period of time, we now return S to 1, it does not aﬀect a. Why? Answer: Since B is also 0, and since only one 
input 0 to a NAND gate is enough to guarantee that the output of the NAND gate is 1, the latch will continue to 
store a 1 long after S returns to 1.

In the same way, we can clear the latch (set the latch to 0) by setting R to 0 for a very short period of time. 

We should point out that if both S and R were allowed to be set to 0 at the same time, the outputs a and b would 
both be 1, and the ﬁnal state of the latch would depend on the electrical properties of the transistors making up 
the gates and not on the logic being performed. How the electrical properties of the transistors would determine the
ﬁnal state in this case is a subject we will have to leave for a later semester. :-(

Finally, we should note that when a digital circuit is powered on, the latch can be in either of its two states, 0 
or 1. It does not matter which state since we never use that information until after we have set it to 1 or 0.

The Gated D Latch

To be useful, it is necessary to control when a latch is set and when it is cleared. A simple way to accomplish this
is with the gated latch.

Figure 3.19 shows a logic circuit that implements a gated D latch. It consists of the R-S latch of Figure 3.18, plus
two additional NAND gates that allow the latch to be set to the value of D, but only when WE is asserted (i.e., when
WE equals 1). WE stands for write enable. When WE is not asserted (i.e., when WE equals 0), the outputs S and R are 
both equal to 1. Since S and R are inputs to the R-S latch, if they are kept at 1, the value stored in the latch 
remains unchanged, as explained earlier When WE is momentarily set to 1, exactly one of the outputs S or R is set to
0, depending on the value of D. If D equals 1, then S is set to 0. If D equals 0, then both inputs to the lower NAND 
gate are 1, resulting in R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R is 
set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according to whether D is 1 or 0. When WE 
returns to 0, S and R return to 1, and the value stored in the R-S latch persists.

The Concept of Memory

We now have all the tools we need to describe one of the most important structures in the electronic digital computer,
its memory.

Memory is made up of a (usually large) number of locations, each uniquely identiﬁable and each having the ability to
store a value. We refer to the unique identiﬁer associated with each memory location as its address. We refer to the
number of bits of information stored in each location as its addressability.

Address Space: We refer to the total number of uniquely identiﬁable locations as the memory’s address space. A 2 GB 
memory, for example, refers to a memory that consists of two billion uniquely identiﬁable memory locations.

Actually, the number two billion is only an approximation, due to the way we specify memory locations. Since 
everything else in the computer is represented by sequences of 0s and 1s, it should not be surprising that memory 
locations are identiﬁed by binary addresses as well. With n bits of address, we can uniquely identify 2^n locations.
Ten bits provide 1024 locations, which is approximately 1000. If we have 20 bits to represent each address, we have 
2^20 uniquely identiﬁable locations, which is approximately one million. With 30 bits, we have 2^30 locations, which
is approximately one billion. In the same way we use the preﬁxes “kilo” to represent 2^10 (approximately 1000) and 
“mega” to represent 2^20 (approximately one million), we use the preﬁx “giga” to represent 2^30  (approximately one 
billion). Thus, 2 giga really corresponds to the number of uniquely identiﬁable locations that can be speciﬁed with 
31 address bits. We say the address space is 2^31, which is exactly 2,147,483,648 locations, rather than 
2,000,000,000, although we colloquially refer to it as two billion.

Addressability

The number of bits stored in each memory location is the memory’s addressability. A 2-gigabyte memory (written 2GB)
is a memory consisting of 2,147,483,648 memory locations, each containing one byte (i.e., eight bits) of storage. 
Most memories are byte-addressable.

The reason is historical; most computers got their start processing data, and one character stroke on the keyboard 
corresponds to one 8-bit ASCII code. If the memory is byte-addressable, then each ASCII character occupies one 
location in memory. Uniquely identifying each byte of memory allows individual bytes of stored information to be 
changed easily.

Many computers that have been designed speciﬁcally to perform large scientiﬁc calculations are 64-bit addressable. 
This is due to the fact that numbers used in scientiﬁc calculations are often represented as 64-bit ﬂoating-point 
quantities.

Since scientiﬁc calculations are likely to use numbers that require 64 bits to represent them, it is reasonable to 
design a memory for such a computer that stores one such number in each uniquely identiﬁable memory location.

A 2^2-by-3-Bit Memory

Figure 3.20 illustrates a memory of size 2^2 by 3 bits. That is, the memory has an address space of four locations 
and an addressability of three bits. A memory of size 2^2 requires two bits to specify the address. We describe the 
two-bit address as A[1:0]. A memory of addressability three stores three bits of information in each memory location. 
We describe the three bits of data as D[2:0]. In both cases, our notation A[high:low] and D[high:low] reﬂects the 
fact that we have numbered the bits of address and data from right to left, in order, starting with the rightmost 
bit, which is numbered 0. The notation [high:low] means a sequence of high − low + 1 bits such that “high” is the 
bit number of the leftmost (or high) bit number in the sequence and “low” is the bit number of the rightmost (or low) 
bit number in the sequence.

Accesses of memory require decoding the address bits. Note that the address decoder takes as input the address bits 
A[1:0] and asserts exactly one of its four outputs, corresponding to the word line being addressed. In Figure 3.20, 
each row of the memory corresponds to a unique three-bit word, thus the term word line. Memory can be read by 
applying the address A[1:0], which asserts the word line to be read. Note that each bit of the memory is ANDed with 
its word line and then ORed with the corresponding bits of the other words. Since only one word line can be asserted
at a time, this is eﬀectively a mux with the output of the decoder providing the select function to each bit line. 
Thus, the appropriate word is read at D[2:0].

Figure 3.21 shows the process of reading location 3. The code for 3 is 11. The address A[1:0]=11 is decoded, and the
bottom word line is asserted. Note that the three other decoder outputs are not asserted. That is, they have the 
value 0. The value stored in location 3 is 101. These three bits are each ANDed with their word line producing the 
bits 101, which are supplied to the three output OR gates. Note that all other inputs to the OR gates are 0, since 
they have been produced by ANDing with their unasserted word lines. The result is that D[2:0] = 101. That is, the 
value stored in location 3 is output by the OR gates. Memory can be written in a similar fashion. The address 
speciﬁed by A[1:0] is presented to the address decoder, resulting in the correct word line being asserted. With 
write enable (WE) also asserted, the three bits D[2:0] can be written into the three gated latches corresponding to 
that word line.

Sequential Logic Circuits:

In this section, we discuss digital logic structures that can both process information (i.e., make decisions) and 
store information. That is, these structures base their decisions not only on the input values now present, but also
(and this is very important) on what has happened before. These structures are usually called sequential logic 
circuits. They are distinguishable from combinational logic circuits because, unlike combinational logic circuits,
they contain storage elements that allow them to keep track of prior history information.

Figure 3.22 shows a block diagram of a sequential logic circuit. Note the storage elements. Note also that the 
output can be dependent on both the inputs now and the values stored in the storage elements. The values stored in 
the storage elements reﬂect the history of what has happened before.

Sequential logic circuits are used to implement a very important class of mechanisms called ﬁnite state machines. We
use ﬁnite state machines in essentially all branches of engineering. For example, they are used as controllers of 
electrical systems, mechanical systems, and aeronautical systems. A traﬃc light controller that sets the traﬃc light
to red, yellow, or green depends on the light that is currently on (history information) and input information from 
sensors such as trip wires on the road, a timer keeping track of how long the current light has been on, and perhaps
optical devices that are monitoring traﬃc.

A Simple Example: The Combination Lock

A simple example shows the diﬀerence between combinational logic structures and sequential logic structures. Suppose
one wishes to secure a bicycle with a lock, but does not want to carry a key. A common solution is the combination 
lock. The person memorizes a “combination” and uses it to open the lock. Two common types of locks are shown in 
Figure 3.23.

In Figure 3.23a, the lock consists of a dial, with the numbers from 0 to 30 equally spaced around its circumference.
To open the lock, one needs to know the “combination.” One such combination could be: R13-L22-R3. If this were the 
case, one would open the lock by turning the dial two complete turns to the right (clockwise), and then continuing 
until the dial points to 13, followed by one complete turn to the left (counter clockwise), and then continuing until
the dial points to 22, followed by turning the dial again to the right (clockwise) until it points to 3. At that 
point, the lock opens. What is important here is the sequence of the turns. The lock will not open, for example if 
one performed two turns to the right, and then stopped on 22 (instead of 13), followed by one complete turn to the 
left, ending on 13, followed by one turn to the right, ending on 3. That is, even though the ﬁnal position of the 
dial is 3, and even though R22-L13-R3 uses the same three numbers as the combination R13-L22-R3, the lock would not 
open. Why? Because the lock stores the previous rotations and makes its decision (open or don’t open) on the basis of
the the history of the past operations, that is, on the correct sequence being performed.

Another type of lock is shown in Figure 3.23b. The mechanism consists of (usually) four wheels, each containing the 
digits 0 through 9. When the digits are lined up properly, the lock will open. In this case, the combination is the 
set of four digits. Whether or not this lock opens is totally independent of the past rotations of the four wheels. 
The lock does not care at all about past rotations. The only thing important is the current value of each of the 
four wheels. This is a simple example of a combinational structure.

It is curious that in our everyday speech, both mechanisms are referred to as “combination locks.” In fact, only the
lock of Figure 3.23b is a combinational lock. The lock of Figure 3.23a would be better called a sequential lock!

3.6.2 The Concept of State

For the mechanism of Figure 3.23a to work properly, it has to keep track of the sequence of rotations leading up to 
the opening of the lock. In particular, it has to diﬀerentiate the correct sequence R13-L22-R3 from all other 
sequences. For example, R22-L13-R3 must not be allowed to open the lock. Likewise, R10-L22- R3 must also not be 
allowed to open the lock.

For the lock of Figure 3.23a to work, it must identify several relevant situations, as follows:

A.  The  lock  is  not  open,  and  NO  relevant  operations  have  been performed.
B.  The  lock  is  not  open,  but  the  user  has  just  completed  the R13  operation.
C.   The  lock  is  not  open,  but  the  user  has  just  completed  R13, followed  by  L22.
D.  The  lock  is  open,  since  the  user  has  just  completed  R13, followed  by  L22,  followed  by  R3.

We have labeled these four situations A, B, C, and D. We refer to each of these situations as the state of the lock.
The notion of state is a very important concept in computer engineering, and actually, in just about all branches of
engineering. The state of a mechanism — more generally, the state of a system —is a snapshot of that system in which 
all relevant items are explicitly expressed.

That is: The state of a system is a snapshot of all the relevant elements of the system at the moment the snapshot 
is taken.

In the case of the lock of Figure 3.23a, there are four states A, B, C, and D. Either the lock is open (State D), or
if it is not open, we have already performed either zero (State A), one (State B), or two (State C) correct 
operations. This is the sum total of all possible states that can exist.

Question: Why are there exactly four states needed to describe the combination lock of Figure 3.23a? because there 
are no other variables that are valid in the system e.g. their is no other turn that is valid.

Can you think of a snapshot of the combination lock after an operation (Rn or Ln) that requires a ﬁfth state 
because it is not covered by one of the four states A, B, C, or D? no

There are many examples of systems that you are familiar with that can be easily described by means of states.

3.6.3 The Finite State Machine and Its State Diagram

The behavior of each of the combinational lock can be described by a finite state machine and represented as a state
diagram.

A ﬁnite state machine consists of ﬁve elements:

1. a ﬁnite number of states
2. a ﬁnite number of external inputs
3. a ﬁnite number of external outputs
4. an explicit speciﬁcation of all state transitions
5. an explicit speciﬁcation of what determines each external output value.

The set of states represents all possible situations (or snapshots) that the system can be in. Each state transition
describes what it takes to get from one state to another.

Let’s examine the ﬁnite state machines for the combination lock:

A state diagram is a convenient representation of a ﬁnite state machine. Figure 3.26 is a state diagram for the 
combination lock. Recall, we identiﬁed four states A, B, C, and D. Which state we are in depends on the progress we 
have made in getting from a random initial state to the lock being open. In the state diagram of Figure 3.26, each 
circle corresponds to one of the four states, A, B, C, or D.

The external inputs are R13, L22, R3, and R-other-than-13, L-other-than-22, and R-other-than-3.

The external output is either the lock is open or the lock is not open. (One logical variable will suﬃce to 
describe that!) As shown in the state diagram, in states A, B, and C, the combination lock is locked. In state D, 
the combination lock is open.

The explicit speciﬁcations of all state transitions are shown by the arrows in the state diagram. The more 
sophisticated term for “arrow” is arc. The arrowhead on each arc speciﬁes which state the system is coming from and 
which state it is going to. We refer to the state the system is coming from as the current state, and the state it 
is going to as the next state. The combination lock has eight state transitions. Associated with each transition is 
the input that causes the transition from the current state to the next state. For example, R13 causes the 
transition from state A to state B.

A couple of things are worth noting. First, it is usually the case that from a current state there are multiple 
transitions to next states. The state transition that occurs depends on both the current state and the value of the 
external input. For example, if the combination lock is in state B, and the input is L22, the next state is state C.
If the current state is state B and the input is anything other than L22, the next state is state A. In short, the 
next state is determined by the combination of the current state and the current external input.

The output values of a system can also be determined by the combination of the current state and the value of the 
current external input. However, as is the case for the combination lock, where states A, B, and C specify the lock 
is “locked,” and state D speciﬁes the lock is “unlocked,” the output can also be determined solely by the current 
state of the system. In all the systems we will study in this book, the output values will be speciﬁed solely by the
current state of the system.

3.6.4 The Synchronous Finite State Machine

Up to now a transition from a current state to a next state in our ﬁnite state machine happened when it happened. For
example, a person could insert a nickel into the soft drink machine and then wait 10 seconds or 10 minutes before 
inserting the next coin into the machine. And the soft drink machine would not complain. It would not dispense the 
soft drink until 15 cents was inserted, but it would wait patiently as long as necessary for the 15 cents to be 
inserted. That is, there is no ﬁxed amount of time between successive inputs to the ﬁnite state machine. This is 
true in the case of all four systems we have discussed. We say these systems are asynchronous because there is 
nothing synchronizing when each state transition must occur.

However, almost no computers work that way. On the contrary, we say that computers are synchronous because the state
transitions take place, one after the other, at identical ﬁxed units of time. They are controlled by a synchronous 
ﬁnite state machine.

It is worth pointing out that both the four asynchronous ﬁnite state machines discussed above and the synchronous 
ﬁnite state machine that controls a digital computer share an important characteristic: They carry out work, one 
state transition at a time, moving closer to a goal. In the case of the combination lock, as long as you make the 
correct moves, each state transition takes us closer to the lock opening. In the case of the soft drink machine, 
each state transition takes us closer to enjoying the taste of the soft drink. In the case of a computer, each 
state transition takes us closer to solving a problem by processing a computer program that someone has written.

3.6.5 The Clock

A synchronous ﬁnite state machine transitions from its current state to its next state after an identical ﬁxed 
interval of time. Control of that synchronous behavior is in part the responsibility of the clock circuit.

A clock circuit produces a signal, commonly referred to as THE clock, whose value alternates between 0 volts and 
some speciﬁed ﬁxed voltage. In digital logic terms, the clock is a signal whose value alternates between 0 and 1. 
Figure 3.28 shows the value of the clock signal as a function of time. Each of the repeated sequence of identical 
intervals is referred to as a clock cycle. A clock cycle starts when the clock signal transitions from 0 to 1 and 
ends the next time the clock signal transitions from 0 to 1.

In each clock cycle, a computer can perform a piece of useful work. When people say their laptop computers run at a 
frequency of 2 gigahertz, they are saying their laptop computers perform two billion pieces of work each second 
since 2 gigahertz means two billion clock cycles each second, each clock cycle lasting for just one-half of a 
nanosecond. The synchronous ﬁnite state machine makes one state transition each clock cycle.

We will show by means of a traﬃc signal controller how the clock signal controls the transition, ﬁxed clock cycle 
after ﬁxed clock cycle, from one state to the next.
In electronic circuit implementations of a synchronous ﬁnite state machine, the transition from one state to the 
next occurs at the start of each clock cycle.

3.6.6 Example: A Danger Sign

Many electrical, mechanical, and aeronautical systems are controlled by a synchronous ﬁnite state machine. Figure 
3.29 shows the danger sign as it will be placed on the highway. Note the sign says, “Danger, Move Right.” The sign 
contains ﬁve lights (labeled 1 through 5 in the ﬁgure).

The purpose of our synchronous ﬁnite state machine (a.k.a. a controller) is to direct the behavior of our system. In
our case, the system is the set of lights on the traﬃc danger sign. The controller’s job is to have the ﬁve lights 
ﬂash on and oﬀ to warn automobile drivers to move to the right. The controller is equipped with a switch. When the 
switch is in the ON position, the controller directs the lights as follows: During one unit of time, all lights will
be oﬀ. In the next unit of time, lights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on.
Then all ﬁve lights will be on. Then the sequence repeats: no lights on, followed by 1 and 2 on, followed by 1, 2, 3,
and 4 on, and so forth. Each unit of time lasts one second. To an automobile driver approaching the sign, the ﬁve 
lights clearly direct the driver to move to the right. The lights continue to sequence through these four states as 
long as the switch is on. If the switch is turned oﬀ, all the lights are turned oﬀ and remain oﬀ.

The State Diagram for the Danger Sign Controller 

Figure 3.30 is a state diagram for the synchronous ﬁnite state machine that controls the lights. There are four 
states, one for each of the four conditions corresponding to which lights are on. Note that the outputs (whether each
light is on or oﬀ) are determined by the current state of the system.

If the switch is on (input = 1), the transition from each state to the next state happens at one-second intervals, 
causing the lights to ﬂash in the sequence described. If the switch is turned oﬀ (input = 0), the state always 
transitions to state A, the “all oﬀ” state.

The Sequential Logic Circuit for the Danger Sign Controller

Recall that Figure 3.22 shows a generic block diagram for a sequential logic circuit.Figure 3.31 is a block diagram 
of the speciﬁc sequential logic circuit we need to control the lights. Several things are important to note in this 
ﬁgure.

First, the two external inputs: the switch and the clock. The switch determines whether the ﬁnite state machine will
transition through the four states or whether it will transition to state A, where all lights are oﬀ. The other 
input (the clock) controls the transition from state A to B, B to C, C to D, and D to A by controlling the state of 
the storage elements. 

Second, there are two storage elements for storing state information. Since there are four states, and since each 
storage element can store one bit of information, the four states are identiﬁed by the contents of the two storage 
elements: A (00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage element 1 contains 
the low bit. For example, the danger sign controller is in state B when storage element 2 is 0 and storage element 
1 is 1.

Third, combinational logic circuit 1 shows that the on/oﬀ behavior of the lights is controlled by the storage 
elements. That is, the input to the combinational logic circuit is from the two storage elements, that is, the
current state of the ﬁnite state machine.

Finally, combinational logic circuit 2 shows that the transition from the current state to the next state depends on
the two storage elements and the switch. If the switch is on, the output of combinational logic circuit 2 depends on
the state of the two storage elements.

The Combinational Logic: Figure 3.32 shows the logic that implements combinational logic circuits 1 and 2.

Two sets of outputs are required for the controller to work properly: a set of external outputs for the lights and a
set of internal outputs for the inputs to the two storage elements that keep track of the state.

First, let us look at the outputs that control the lights. As we have said, there are only three outputs necessary 
to control the lights. Light 5 is controlled by the output of the AND gate labeled V, since the only time light 5 
is on is when the controller is in state 11. Lights 3 and 4 are controlled by the output of the OR gate labeled X, 
since there are two states in which those lights are on, those labeled 10 and 11. Why are lights 1 and 2 controlled 
by the output of the OR gate labeled W? because the only time they are on is when the controller is in state 01 and
11

Next, let us look at the internal outputs that control the storage elements, which specify the next state of the 
controller. Storage element 2 should be set to 1 for the next clock cycle if the next state is 10 or 11. This is 
true only if the switch is on and the current state is either 01 or 10. Therefore, the output signal that will make 
storage element 2 be 1 in the next clock cycle is the output of the OR gate labeled Y. Why is the next state of 
storage element 1 controlled by the output of the OR gate labeled Z? storage element 1 should be set to 1 for the
next clock cyle if the state of the controller is currently 00 or 10 and the switch is on. This only occurs if
the output is produced by gate Z.

The Two Storage Elements: In order for the danger sign controller to work, the state transitions must occur once 
per second when the switch is on.

A Problem with Gated Latches as Storage Elements: What would happen if the storage elements were gated D latches? If 
the two storage elements were gated D latches, when the write enable signal (the clock) is 1, the output of OR gates
Y and Z would immediately change the bits stored in the two gated D latches. This would produce new input values to 
the three AND gates that are input to OR gates Y and Z, producing new outputs that would be applied to the inputs of
the gated latches, which would in turn change the bits stored in the gated latches, which would in turn mean new 
inputs to the three AND gates and new outputs of OR gates Y and Z. This would happen again and again, continually 
changing the bits stored in the two storage elements as long as the Write Enable signal to the gated D latches was 
asserted. The result: We have no idea what the state of the ﬁnite state machine would be for the next clock cycle. 
And, even in the current clock cycle, the state of the storage elements would change so fast that the ﬁve lights 
would behave erratically.

The problem is the gated D latch. We want the output of OR gates Y and Z to transition to the next state at the end 
of the current clock cycle and allow the current state to remain unchanged until then. That is, we do not want the 
input to the storage elements to take eﬀect until the end of the current clock cycle. Since the output of a gated D 
latch changes immediately in response to its input if the Write Enable signal is asserted, it cannot be the storage 
element for our synchronous ﬁnite state machine. We need storage elements that allow us to read the current state 
throughout the current clock cycle, and not write the next state values into the storage elements until the 
beginning of the next clock cycle.

The Flip-Flop to the Rescue: It is worth repeating; To prevent the above from happening, we need storage elements 
that allow us to read the current state throughout the current clock cycle, and not write the next state values into
the storage elements until the beginning of the next clock cycle. That is, the function to be performed during a 
single clock cycle involves reading and writing a particular variable. Reading must be allowed throughout the 
clock cycle, and writing must occur at the end of the clock cycle.

A ﬂip-ﬂop can accomplish that. One example of a ﬂip-ﬂop is the master/slave ﬂip-ﬂop shown in Figure 3.33. The 
master/slave ﬂip-ﬂop can be constructed out of two gated D latches, one referred to as the master, the other referred
to as the slave. Note that the write enable signal of the master is 1 when the clock is 0, and the write enable 
signal of the slave is 1 when the clock is 1.

Figure 3.34 is a timing diagram for the master/slave ﬂip-ﬂop, which shows how and why the master/slave ﬂip-ﬂop solves
the problem. A timing diagram shows time passing from left to right. Note that clock cycle n starts at the time 
labeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time labeled 4.

Consider clock cycle n, which we will discuss in terms of its ﬁrst half A, its second half B, and the four time 
points labeled 1, 2, 3, and 4.

At the start of each clock cycle, the outputs of the storage elements are the outputs of the two slave latches. 
These outputs (starting at time 1) are input to the AND gates, resulting in OR gates Y and Z producing the next 
state values for the storage elements (at time 2). The timing diagram shows the propagation delay of the 
combinational logic, that is, the time it takes for the combinational logic to produce outputs of OR gates Y and Z. 
Although OR gates Y and Z produce the Next State value sometime during half-cycle A, the write enable signal to the 
master latches is 0, so the next state cannot be written into the master latches.

At the start of half-cycle B (at time 3), the clock signal is 0, which means the write enable signal to the master 
latches is 1, and the master latches can be written. However, during the half-cycle B, the write enable to the slave
latches is 0, so the slave latches cannot write the new information now stored in the master latches.

At the start of clock cycle n+1 (at time 4), the write enable signal to the slave latches is 1, so the slave latches
can store the next state value that was created by the combinational logic during clock cycle n. This becomes the 
current state for clock cycle n+1.

Since the write enable signal to the master latches is now 0, the state of the master latches cannot change. Thus, 
although the write enable signal to the slave latches is 1, those latches do not change because the master latches 
cannot change.

In short, the output of the slave latches contains the current state of the system for the duration of the clock 
cycle and produces the inputs to the six AND gates in the combinational logic circuits. Their state changes at the 
start of the clock cycle by storing the next state information created by the combinational logic during the 
previous cycle but does not change again during the clock cycle. The reason they do not change again during the 
clock cycle is as follows: During half-cycle A, the master latches cannot change, so the slave latches continue to 
see the state information that is the current state for the new clock cycle. During half-cycle B, the slave latches 
cannot change because the clock signal is 0.

Meanwhile, during half-cycle B, the master latches can store the next state information produced by the combinational
logic, but they cannot write it into the slave latches until the start of the next clock cycle, when it becomes the 
state information for the next clock cycle.

3.7 Preview of Coming Attractions: The Data Path of the LC-3

We close out Chapter 3 with a discussion of Figure 3.35, the data path of the LC-3 computer. The data path consists 
of all the logic structures that combine to process information in the core of the computer. Right now, Figure 3.35 
is undoubtedly more than a little intimidating, but you should not be concerned by that. You are not ready to 
analyze it yet. That will come in Chapter 5. We have included it here, however, to show you that you are already 
familiar with many of the basic structures that make up a computer.

For example, you see ﬁve MUXes in the data path, and you already know how they work. Also, an adder (shown as the 
ALU symbol with a + sign inside) and an ALU. You know how those elements are constructed from gates.

One element that we have not identified explicitly yet is a register. A register is simply a set of n flip-flops that
collectively are used to store one n-bit value. In Figure 3.35, PC, IR, MAR, and MDR are all 16-bit registers that 
store 16 bits of information each. The block labeled REG FILE consists of eight registers that each store 16 bits of
information. As you know, one bit of information can be stored in one flip-flop. Therefore, each of these registers 
consists of 16 flip-flops. The data path also shows three 1-bit registers, N, Z, and P. Those registers require only
one flip-flop each. In fact, a register can be any size that we need. The size depends only on the number of bits we
need to represent the value we wish to store.

One way to implement registers is with master/slave ﬂip-ﬂops. Figure 3.36 shows a four-bit register made up of four 
master/slave ﬂip-ﬂops. We usually need ﬂip-ﬂops, rather than latches, because it is usually important to be able to 
both read the contents of a register throughout a clock cycle and also store a new value in the register at the end 
of that same clock cycle. As shown in Figure 3.36, the four-bit value stored in the register during a clock cycle is
Q₃, Q₂, Q₁, Q₀. At the end of that clock cycle, the value D₃, D₂, D₁, D₀ is written into the register.

The arrows in Figure 3.35 represent wires that transmit values from one struc- ture to another. Most of the arrows 
include a cross-hatch with a number next to it. The number represents the number of wires, corresponding to the 
number of bits being transmitted. Thus, for example, the arrow from the register labeled PC to one of the inputs of 
the MUX labeled ADDR1MUX indicates that 16 bits are transmitted from PC to an input of ADDR1MUX.

A two-input AND and a two-input OR are both examples of two-input logic functions. How many diﬀerent two-input 
logic functions are possible? There can be 16 different two input logic functions.

chapter 4

To get a task done by a computer, we need two things: (a) a computer program that speciﬁes what the computer must 
do to perform the task, and (b) the computer that is to carry out the task.

A computer program consists of a set of instructions, each specifying a well-deﬁned piece of work for the computer 
to carry out. The instruction is the smallest piece of work speciﬁed in a computer program. That is, the computer 
either carries out the work speciﬁed by an instruction or it does not. The computer does not have the luxury of 
carrying out only a piece of an instruction.

John von Neumann proposed a fundamental model of a computer for processing computer programs in 1946. Figure 4.1 
shows its basic components. We have taken a little poetic license and added a few of our own minor embellishments 
to von Neumann’s original diagram. The von Neumann model consists of ﬁve parts: memory, a processing unit, input, 
output, and a control unit. The computer program is contained in the computer’s memory. The data the program needs 
to carry out the work of the program is either contained in the program’s memory or is obtained from the input 
devices. The results of the program’s execution are provided by the output devices. The order in which the 
instructions are carried out is performed by the control unit.

We will describe each of the ﬁve parts of the von Neumann model in greater detail.

Memory

Previously you saw a simple 2^2-by-3-bit memory that was constructed out of gates and latches. A more realistic 
memory for one of today’s computer systems is 2^34 by 8 bits. That is, a typical memory in today’s world of 
computers consists of 2^34 distinct memory locations, each of which is capable of storing eight bits of information.
We say that such a memory has an address space of 2^34 uniquely identiﬁable locations, and an addressability of 
eight bits.

We refer to such a memory as a 16-gigabyte memory (abbreviated, 16 GB). The “16 giga” refers to the 2^34 locations, 
and the “byte” refers to the eight bits stored in each location. The term is 16 giga because 16 is 2^4 and giga is 
the term we use to represent 2^30, which is approximately one billion; 2^4 times 2^30 = 2^34.A byte is the word we 
use to describe eight bits, much the way we use the word gallon to describe four quarts.

We note (as we will note again and again) that with k bits, we can represent uniquely 2^k items. Thus, to uniquely 
identify 2^34 memory locations, each location must have its own 34-bit address. 

To read the contents of a memory location, we ﬁrst place the address of that location in the memory’s address 
register (MAR) and then interrogate the computer’s memory. The information stored in the location having that 
address will be placed in the memory’s data register (MDR). To write (or store) a value in a memory location, we 
ﬁrst write the address of the memory location in the MAR, and the value to be stored in the MDR. We then 
interrogate the computer’s memory with the write enable signal asserted. The information contained in the MDR will 
be written into the memory location whose address is in the MAR. 

Before we leave the notion of memory for the moment, let us again emphasize the two characteristics of a memory 
location: its address and what is stored there. Figure 4.2 shows a representation of a memory consisting of eight 
locations. Its addresses are shown at the left, numbered in binary from 0 to 7. Each location contains eight bits 
of information. Note that the value 6 is stored in the memory location whose address is 4, and the value 4 is 
stored in the memory location whose address is 6. These represent two very diﬀerent situations.

Finally, an analogy: the post oﬃce boxes in your local post oﬃce. The box number is like the memory location’s 
address. Each box number is unique. The information stored in the memory location is like the letters contained in 
the post oﬃce box. As time goes by, what is contained in the post oﬃce box at any particular moment can change. 
But the box number remains the same. So, too, with each memory location. The value stored in that location can be 
changed, but the location’s memory address remains unchanged.

Processing Unit

The actual processing of information in the computer is carried out by the processing unit. The processing unit in 
a modern computer can consist of many sophisticated complex functional units, each performing one particular 
operation (divide, square root, etc.). The simplest processing unit, and the one normally thought of when 
discussing the basic von Neumann model, is the ALU. ALU is the abbreviation for Arithmetic and Logic Unit, so called
because it is usually capable of performing basic arithmetic functions (like ADD and SUBTRACT) and basic logic 
operations (like bit-wise AND, OR, and NOT) . 

The ALU normally processes data elements of a ﬁxed size referred to as the word length of the computer. The data 
elements are called words. For example, to perform ADD, the ALU receives two words as inputs and produces a single 
word (the sum) as output. Each ISA has its own word length, depending on the intended use of the computer.

Most microprocessors today that are used in PCs or workstations have a word length of 64 bits or 32 bits. Even most
microprocessors now used in cell phones have 64-bit word lengths, However, the microprocessors used in very 
inexpensive applications often have word lengths of as little as 16 or even 8 bits.

It is almost always the case that a computer provides some small amount of storage very close to the ALU to allow 
results to be temporarily stored if they will be needed to produce additional results in the near future. For 
example, if a computer is to calculate (A+B)⋅C, it could store the result of A+B in memory, and then subsequently 
read it in order to multiply that result by C. However, the time it takes to access memory is long compared to the 
time it takes to perform the ADD or MULTIPLY. Almost all computers, therefore, have temporary storage for storing 
the result of A + B in order to avoid the much longer access time that would be necessary when it came time to 
multiply. The most common form of temporary storage is a set of registers. Typically, the size of each register is 
identical to the size of values processed by the ALU; that is, they each contain one word.

Current microprocessors typically contain 32 registers, each consisting of 32 or 64 bits, depending on the 
architecture. These serve the same purpose as the eight 16-bit registers in the LC-3. However, the importance of 
temporary storage for values that most modern computers will need shortly means many computers today have an 
additional set of special-purpose registers consisting of 128 bits of information to handle special needs. 

Input and Output

In order for a computer to process information, the information must get into the computer. In order to use the 
results of that processing, those results must be displayed in some fashion outside the computer. Many devices 
exist for the purposes of input and output. They are generically referred to in computer jar- gon as peripherals 
because they are in some sense accessories to the processing function. Nonetheless, they are no less important.

Control Unit

The control unit is like the conductor of an orchestra; it is in charge of making all the other parts of the 
computer play together. As we will see when we describe the step-by-step process of executing a computer program, 
it is the control unit that keeps track of both where we are within the process of executing the program and where 
we are in the process of executing each instruction.

To keep track of which instruction is being executed, the control unit has an instruction register to contain that 
instruction. To keep track of which instruction is to be processed next, the control unit has a register that 
contains the next instruction’s address. For historical reasons, that register is called the program counter 
(abbreviated PC), although a better name for it would be the instruction pointer, since the contents of this 
register is, in some sense, “pointing” to the next instruction to be processed. Curiously, Intel does in fact call 
that register the instruction pointer, but the simple elegance of that name has not caught on.

The LC-3: An Example von Neumann Machine

We have already shown you its data path in Chapter 3 (Figure 3.35) and identiﬁed several of its structures in above.
In this section, we will pull together all the parts of the LC-3 we need to describe it as a von Neumann computer 
(see Figure 4.3).

We constructed Figure 4.3 by starting with the LC-3’s full data path (Figure 3.35) and removing all elements that 
are not essential to pointing out the ﬁve basic components of the von Neumann model.

Note that there are two kinds of arrowheads in Figure 4.3: ﬁlled-in and not-ﬁlled-in. Filled-in arrowheads denote 
data elements that ﬂow along the corresponding paths. Not-ﬁlled-in arrowheads denote control signals that control 
the processing of the data elements. For example, the box labeled ALU in the processing unit processes two 16-bit 
values and produces a 16-bit result. The two sources and the result are all data, and are designated by ﬁlled-in 
arrowheads. The operation performed on those two 16-bit data elements (it is labeled ALUK) is part of the control—
therefore, a not-ﬁlled-in arrowhead.

MEMORY consists of the storage elements, along with the Memory Address Register (MAR) for addressing individual 
locations and the Memory Data Register (MDR) for holding the contents of a memory location on its way to/from the 
storage. Note that the MAR contains 16 bits, reﬂecting the fact that the memory address space of the LC-3 is 2^16 
memory locations. The MDR contains 16 bits, reﬂecting the fact that each memory location contains 16 bits—that is, 
the LC-3 is 16-bit addressable.

INPUT/OUTPUT

consists of a keyboard and a monitor. The simplest keyboard requires two registers: a keyboard data register (KBDR)
for holding the ASCII codes of keys struck and a keyboard status register (KBSR) for maintaining status information
about the keys struck. The simplest monitor also requires two registers: a display data register (DDR) for holding 
the ASCII code of something to be displayed on the screen and a display status register (DSR) for maintaining 
associated status information.

THE PROCESSING UNIT

consists of a functional unit (ALU) that performs arithmetic and logic operations and eight registers (R0, … R7) 
for storing temporary values that will be needed in the near future as operands for subsequent instructions. The 
LC-3 ALU can perform one arithmetic operation (addition) and two logical operations (bitwise AND and bitwise NOT).

THE CONTROL UNIT 

consists of all the structures needed to manage the processing that is carried out by the computer. Its most 
important structure is the ﬁnite state machine, which directs all the activity. Processing is carried out step by 
step, or rather, clock cycle by clock cycle. Note the CLK input to the ﬁnite state machine in Figure 4.3. It 
speciﬁes how long each clock cycle lasts. The instruction register (IR) is also an input to the ﬁnite state machine
since the LC-3 instruction being processed determines what activities must be carried out. The program counter (PC)
is also a part of the control unit; it keeps track of the next instruction to be executed after the current 
instruction ﬁnishes.

Note that all the external outputs of the ﬁnite state machine in Figure 4.3 have arrowheads that are not ﬁlled in. 
These outputs control the processing through- out the computer. For example, one of these outputs (two bits) is 
ALUK, which controls the operation performed in the ALU (ADD, AND, or NOT) during the current clock cycle. Another 
output is GateALU, which determines whether or not the output of the ALU is provided to the processor bus during 
the current clock cycle.

The complete description of the data path, control, and ﬁnite state machine for one implementation of the LC-3 is 
the subject of Appendix C.

Instruction Processing

The central idea in the von Neumann model of computer processing is that the program and data are both stored as 
sequences of bits in the computer’s memory, and the program is executed one instruction at a time under the 
direction of the control unit.

The Instruction

The most basic unit of computer processing is the instruction. It is made up of two parts, the opcode (what the 
instruction does) and the operands (who it does it to!).

There are fundamentally three kinds of instructions: operates, data movement, and control, although many ISAs have 
some special instructions that are necessary for those ISAs. Operate instructions operate on data. The LC-3 has 
three operate instructions: one arithmetic (ADD) and two logicals (AND and NOT). Data movement instructions move 
information from the processing unit to and from memory and to and from input/output devices. The LC-3 has six 
data movement instructions. Control instructions are necessary for altering the sequential processing of 
instructions.

An LC-3 instruction consists of 16 bits (one word), numbered from left to right, bit [15] to bit [0]. Bits [15:12] 
contain the opcode. This means there are at most 24 distinct opcodes. Actually, we use only 15 of the possible 
four-bit codes. One is reserved for some future use. Bits [11:0] are used to ﬁgure out where the operands are.

In this chapter, we will introduce ﬁve of the LC-3’s 15 instructions: two operates (ADD and AND), one data movement
(LD), and two control (BR and TRAP).

The ADD Instruction The ADD instruction is an operate instruction that requires three operands: two source operands
(the numbers to be added) and one destination operand (where the sum is to be stored after the addition is 
performed). We said that the processing unit of the LC-3 contained eight registers for purposes of storing data 
that may be needed later. In fact, the ADD instruction requires that at least one of the two source operands is 
contained in one of these registers, and that the result of the ADD is put into one of these eight registers. 
Since there are eight registers, three bits are necessary to identify each register. The 16-bit LC-3 ADD 
instruction has one of the following two forms (we say formats):

15 14 13 12  11 10 9   8  7  6   5   4  3   2  1  0
0  0  0  1 | 1  1  0 | 0  1  0 | 0 | 0  0 | 1  1  0
  ADD           R6        R2                  R6


15  14  13  12  11  10  9   8  7  6   5   4  3  2  1  0
 0   0  0   1 | 1   1   0 | 0  1  0 | 1 | 0  0  1  1  0
  ADD             R6          R2             imm

Both formats show the four-bit opcode for ADD, contained in bits [15:12]: 0001. Bits [11:9] identify the location
to be used for storing the result, in this case register 6 (R6). Bits [8:6] identify the register that contains one
of the two source operands, in this case R2. The only diﬀerence in the two formats is the 1 or 0 stored in bit 5, 
and what that means. In the ﬁrst case, bit 5 is 0, signifying that the second source operand is in the register 
speciﬁed by bits [2:0], in this case R6. In the second case, bit 5 is 1, signifying that the second source operand 
is formed by sign-extending the integer in bits [4:0] to 16 bits. In this case, the second source operand is the 
positive integer 6.

Thus, the instruction we have just encoded is interpreted, depending on whether bit 5 is a 0 or a 1 as either “Add 
the contents of register 2 (R2) to the contents of register 6 (R6) and store the result back into register 6 (R6),” 
or “Add the contents of register 2 (R2) to the positive integer 6 and store the result into register 6.”