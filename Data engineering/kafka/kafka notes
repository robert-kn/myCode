what is kafka? is a distributed, real time event streaming platform that adheres to client server architecture. It runs as a cluster of broker servers, acting as the event broker to receieve events from producers, store the stream of records, and distribute events. It also has servers that run kafka connect to import and export data as event streams. see kafkaarchitecture.png

see kafkafeatures.png

what is an event? in the context of event streaming, an event is a type of data which describes an entity's observable state at a time e.g. the gps coordinates of a car, temperature of a room, blood pressure of a patient etc. The format of the event can be primitive e.g. string, a key value pair, a key value with a timestamp. Event streaming occurs from one event source to a destination. This is called event streaming. In a real world scenario, you have many different event sources streaming to different destinations.

Data transfer pipelines may also be based on different communication protocols adding to the complexity. To overcome the challenge of handling different event sources and destinations, you need to use an event streaming platform such as apache kafka. An event streaming platform acts as a middle layer between event sources and destinations. All sources send events to the esp as opposed to the destinations see see sourcesToDestimations.png

ESP's (event streaming platforms) act as a middle layer between between event sources and destinations. They usually have different architectures and components. Below is a diagram of common ESP components: see ESP.png

see popularESPs.png

see apachekafka.png

see kaas.png

The event broker is designed to receive and consume events. It is the core component of an ESP. The event storage component is used to store events received from event sources. The analytic and query engine is used to analyse the stored events.

A kafka cluster has one or many brokers. A broker is dedicated server to receive, store, process and distribute events. Brokers are synchronised and use kraft controller nodes that use the consensus protocol to manage the kafka metadata log that contains information about each change to the cluster metadata. Each broker contains one or more topics. You can think of each topic as a database to store specific types of events such as logs, transactions etc. see see brokerandtopics.png and brokercomponents.png
 
Brokers distributes saved events to subscribed consumers. Kafka uses topic partitions and replications to increase fault tolerance and throughput so that event publication and consumption can be done in parallel with multiple brokers. In addition, if some brokers are down, kafka clients are still able to work with target topics replicated in other working brokers. see partitioningAndReplication.png

The kafka cli provides a collection of script files for users to build an event streaming pipeline. The most common one you will use will be the kafka-topics script see kafkaTopicsScript.png

kafka producers are client applications that publish events to a topic partition according to the same order that they are published. An event to be published can be optionally associated with a key. Events associated to the same key will be published to the same topic partition. Events not associated to any key will be published to topic partitions in rotation. see kafkaproducer.png

kafka producer cli is for used to manage producers producerCLI.png

kafka consumers are clients subscribed to topics. They read data in the same order they are published. They store an offset record for each partition and can read all events by resetting the offset to zero. see consumer.png and consumerCLI.png

Kafka streams api is a client library to facilitate data processing in event streaming pipelines. It processes and analyses data stored in kafka topics hence the input and output of the streams api are kafka topics. see streamProcessingTopology.png

see streamProcessing.png as opposed to curatedProcessor.png

kafka has a distributed client-server architecture. On the server side, kafka is a cluster with many associated servers called brokers, which act as "databases" to receive, store and distribute events. It also has some servers that run "kafka connect" to import and export data as event stream.

I do not understand the functionality of kafka connect i.e. to import and export data as event stream. What does this mean?

All of the brokers until versions 2.8 relied on a distributed system called zookeper for management and to ensure that all brokers work in an efficient and collaborative way.

However, kafka raft (kraft) is now used to eliminate kafka's reliance on zookeper for metadata management. It is a consensus protocol (what does this mean?) that streamlines kafka's architecture by consolidating metadata responsibilities within kafka itself using kafka controllers (I don't understand this statement).

producers publish data to a topic and consumers subscribe to the topic to receive data. kafka uses a tcp based network communication protocol to exchange data between clients and servers.