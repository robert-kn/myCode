what is an event? in the context of event streaming, an event is a type of data which describes an entity's 
observable state at a time e.g. the gps coordinates of a car, temperature of a room, blood pressure of a 
patient etc. The format of the event can be primitive e.g. string, a key value pair, a key value with a timestamp. 
Event streaming occurs from one event source to a destination. This is called event streaming. In a real world 
scenario, you have many different event sources streaming to different destinations. see event1.png

common event formats? see event_formats.png

what is event streaming? see event_streaming1.png event_streaming2 event_streaming3

To overcome such a challenge of handling different event sources and destinations, we will need to employ the event 
streaming platform. 

An ESP's (event streaming platforms) act as a middle layer between between event sources and destinations. They 
usually have different architectures and components. a diagram of common ESP components: see ESP1.png

Different ESP's may have different architectures and components see ESP2.png Here we show you some common components
included in most ESP systems. The first and foremost component is the event broker, which is designed to receive and
consume events. The second common component of an ESP is event storage, which is used for storing events being 
received from event sources. Accordingly, event destinations do not need to synchronize with event sources, and 
stored events can be retrieved at will. The third common component is the analytic and query engine, which is used 
for querying and analyzing the stored events.

Broker components see brokercomponents.png: It normally contains three subcomponents, ingester, processor, and 
consumption. The ingester is designed to efficiently receive events from various event sources. The processor 
performs operations on data such as serializing and deserializing, compressing and decompressing, encryption and 
decryption, and so on. The consumption component retrieves events from event storage and efficiently distributes 
them to subscribed event destinations.

see popularESPs.png: Each has its unique features and application scenarios. Among these ESPs, Apache Kafka is 
probably the most popular one. 

Implementing an ESP and its components from scratch can be extremely difficult, but there are several open source 
and commercial ESP solutions with built-in capabilities available in the market. Apache Kafka is an open source 
project which has become the most popular ESP. Kafka is a comprehensive platform and can be used in many application
scenarios. Kafka was originally used to track user activities such as keyboard strokes, mouse clicks, page views, 
searches, gestures, screen time, and so on. But now Kafka is also suitable for all kinds of metric streaming such as
sensor readings, GPS, and hardware and software monitoring. For enterprise applications and infrastructure with a 
huge number of logs, Kafka can be used to collect and integrate them into a centralized repository. For banks, 
insurance, or fintech companies, Kafka is widely used for payments and transactions. These scenarios are just the 
tip of the iceberg. Essentially, you can use Kafka when you want high throughput and reliable data transportation 
services among various event sources and destinations. All events will be ingested in Kafka and become available for 
subscriptions and consumption, including further data storage and movement to other online or offline databases and 
backups. Real time processing and analytics including dashboard, machine learning, AI algorithms, and so on, 
generating notifications such as email, text messages, and instant messages, or data governance and auditing to 
make sure sensitive data such as bank transactions are complying with regulations. see apachekafka.png
common_usecases.png


what is kafka? is a distributed, real time event streaming platform that adheres to client server architecture. It 
runs as a cluster of broker servers, acting as the event broker to receive events from producers, store the stream 
of records, and distribute events. see apachekafka.png. It also has servers that run kafka connect to import and 
export data as event streams. all the brokers before version 2.8 relied on another distributed system called 
Zookeeper for metadata management and to ensure all brokers work in an efficient and collaborative way. However, 
Kafka Raft, pronounced as KRaft, is now used to eliminate Kafka's reliance on Zookeeper for metadata management. 
see kafkaarchitecture.png

Using Kafka controllers, producers send or publish data to the topic, and the consumers subscribe to the topic to 
receive data. Kafka uses a transmission control protocol, TCP based network communication protocol, to exchange data
between clients and servers. For the client side, Kafka provides different types of clients such as Kafka command 
line interface, CLI. A collection of shell scripts to communicate with the Kafka server, several high level 
programming APIs such as Java, Scala, Python, Go, C, and C++, rest APIs, and some specific third party clients made 
by the Kafka community. You can select different clients based on your requirements.

see kafkafeatures.png: Kafka is a distribution system, which makes it highly scalable to handle high data throughput 
and concurrency. A Kafka cluster normally has multiple event brokers which can handle event streaming in parallel. 
Kafka is very fast and highly scalable. Kafka also divides event storage into multiple partitions and replications, 
which makes it fault-tolerant and highly reliable. Kafka stores the events permanently. As such, event consumption 
can be done whenever suitable for consumers without a deadline, and Kafka is open source, meaning that you can use 
it for free and even customize it based on your specific requirements.

Even though Kafka is open source and well documented, it is still challenging to configure and deploy Kafka without 
professional assistance. Deploying a Kafka cluster requires extensive efforts for tuning infrastructure and 
consistently adjusting the configurations, especially for enterprise-level deployments. Fortunately, several 
commercial service providers offer an on-demand ESP as a service to meet your streaming requirements. Many of them 
are built on top of Kafka and provide added value for customers. Some well known ESP providers include see kaas.png


A kafka cluster has one or many brokers. A broker is dedicated server to receive, store, process and distribute 
events. Brokers are synchronised and use kraft controller nodes that use the consensus protocol to manage the kafka 
metadata log that contains information about each change to the cluster metadata. Each broker contains one or more 
topics. You can think of each topic as a database to store specific types of events such as logs, transactions etc. 
see brokerandtopics.png

brokers save published events into topics and distribute the events to subscribed consumers

Kafka uses topic partitions and replications to increase fault tolerance and throughput so that event publication and
consumption can be done in parallel with multiple brokers. In addition, if some brokers are down, kafka clients are 
still able to work with target topics replicated in other working brokers. see partitioningAndReplication.png

The kafka cli provides a collection of script files for users to build an event streaming pipeline. The most common 
one you will use will be the kafka-topics script to manage topics in a kafka cluster see kafkaTopicsScript.png

kafka producers are client applications that publish events to a topic partition according to the same order that 
they are published. An event to be published can be optionally associated with a key. Events associated to the same 
key will be published to the same topic partition. Events not associated to any key will be published to topic 
partitions in rotation. see producer_features.png. Suppose you have an Event Source 1 which generates various log 
entries and an Event Source 2 which generates user activity tracking records. Then you can create a Kafka producer 
to publish log records to log topic partitions and a user producer to publish user activity events to user topic 
partitions, respectively. When you publish events in producers, you can choose to associate events with a key, for 
example, an application name or a user ID. see kafkaproducer.png

Similar to the Kafka topic CLI, Kafka provides the Kafka producer CLI for users to manage producers. The most 
important aspect is starting a producer to write or publish events to a topic. Here you start a producer and point 
it to the log_topic. Then you can type some messages in the console to start publishing events. For example, log1, 
log2, and log3. You can provide keys to events to make sure the events with the same key will go to the same 
partition. Here you are starting a producer to user_topic with the parse.key option to be true and you also specify 
the key.separator to be comma. Then you can write messages as follows, key, user1, value login website, key, user1, 
value, click the top item and key, user1, value, logout website. Accordingly, all events about user one will be 
saved in the same partition to facilitate the reading for consumers. see producerCLI.png

Once events are published and properly stored in topic partitions, you can create consumers to read them. Consumers 
are client applications that can subscribe to topics and read the stored events. Then event destinations can further
read events from Kafka consumers. Consumers read data from topic partitions in the same order as they are published.
Consumers also store an offset for each topic partition as the last read position. With the offset, consumers are
guaranteed to read events as they occur. A playback is also possible by resetting the offset to zero. This way, the 
consumer can read all events in the topic partition from the beginning. In Kafka, producers and consumers are fully 
decoupled. As such, producers don't need to synchronize with consumers, and after events are stored in topics, 
consumers can have independent schedules to consume them. 

To read published log and user events from topic partitions, you will need to create log and user consumers and make
them subscribe to corresponding topics. Then Kafka will push the events to those subscribed consumers. Then the 
consumers will further send to event destinations. see consumer.png 

To start a consumer is also easy using the Kafka consumer script. Let's read events from the log_topic. You just 
need to run the Kafka console consumer script and specify a Kafka cluster and the topic to subscribe to. Here you 
can subscribe to and read events from the topic log_topic. Then the started consumer will read only the new events 
starting from the last partition offset. After those events are consumed, the partition offset for the consumer will
also be updated and committed back to Kafka. Very often a user wants to read all events from the beginning as a 
playback of all historical events. To do so, you just need to add the from-beginning option. Now you can read all 
events starting from offset 0. see consumerCLI.png


Let's have a look at a more concrete example to help you understand how to build an event streaming pipeline end to 
end. Suppose you want to collect and analyze weather and Twitter event streams so that you can correlate how people 
talk about extreme weather on Twitter. Here you can use two event sources: IBM weather API to obtain real time and 
forecasted weather data in JSON format. Twitter API to obtain real-time tweets and mentions also in JSON format. To 
receive weather and Twitter JSON data in Kafka, you then create a weather topic and a Twitter topic in a Kafka 
cluster with some partitions and replications. To publish weather and Twitter JSON data to the two topics, you need 
to create a weather producer and a Twitter producer. The event's JSON data will be serialized into bytes and saved 
in Kafka topics. To read events from the two topics, you need to create a weather consumer and a Twitter consumer. 
The bytes stored in Kafka topics will be deserialized into event JSON data. If you now want to transport the weather
and Twitter event JSON data from the consumers to a relational database, you will use a DB writer to parse those 
JSON files and create database records, and then you can write those records into a database using SQL insert 
statements. Finally, you can query the database records from the relational database and visualize and analyze them 
in a dashboard to complete the end-to-end pipeline. see weather_pipeline_example.png

see recap.png

In event streaming, in addition to transporting data, data engineers also need to process data through. For example,
data filtering, aggregation, and enhancement. Any applications developed to process streams are called stream 
processing applications. For stream processing applications based on Kafka, a straightforward way is to implement 
an ad hoc data processor to read events from one topic, process them, and publish them to another topic.

Let's look at an example. You first request raw weather JSON data from a weather API, and you start a weather 
producer to publish the raw data into a raw_weather_topic. Then you start a consumer to read the raw weather data 
from the weather topic. Next, you create an ad hoc data processor to filter the raw weather data to only include 
extreme weather events, such as very high temperatures. Such a processor could be a simple script file or an 
application which works with Kafka clients to read and write data from Kafka. Afterwards, the processor sends the 
processed data to another producer and it gets published to a processed_weather_topic. Finally, the processed 
weather data will be consumed by a dedicated consumer and sent to a dashboard for visualization. Such ad hoc 
processors may become complicated if you have many different topics to be processed. see adhoc_processing.png

A solution that may solve these challenges is Kafka. It provides the Streams API to facilitate stream processing. 
Kafka Streams API is a simple client library aiming to facilitate data processing in event streaming pipelines. It 
processes and analyzes data stored in Kafka topics. Thus, both the input and output of the Streams API are Kafka
topics. Additionally, Kafka Streams API ensures that each record will only be processed once. Finally, it processes 
only one record at a time. see streams_api_features.png

Kafka Streams API is based on a computational graph called a stream processing topology. In this topology, each node
is a stream processor, which receives streams from its upstream processor; performs data transformations, such as 
mapping, filtering, formatting, and aggregation; and produces output streams to its downstream stream processors. 
Thus, the edges of the graph are the I/O streams. There are two special types of processors. On the left, you can 
see the source processor which has no upstream processors. A source processor acts like a consumer, which consumes 
streams from Kafka topics and forwards the process streams to its downstream processors. On the right, you can see 
the sink processor, which has no downstream processors. A sink processor acts like a producer which publishes the 
received stream to a Kafka topic. see streamProcessingTopology.png

Let's redesign the previous weather stream processing application with Kafka Streams API. Suppose you have a 
raw_weather_topic and a processed_weather_topic in Kafka. Now, instead of spending a huge amount of effort 
developing an ad hoc processor, you could just plug in the Kafka Streams API here. In the Kafka Streams topology, we
have three stream processors, the source processor that consumes raw weather streams from the raw_weather_topic and 
forwards the weather stream to the stream processor to filter the stream based on high temperature. Then the filtered
stream will be forwarded to the sink processor, which then publishes the output to the processed_weather_topic. 
Concluding, this is a much simpler design than an ad hoc data processor, especially if you have many different topics
to be processed. see streamProcessing.png  



