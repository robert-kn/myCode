What will you learn about? see learning1.png 

Are you an aspiring Big Data Engineer or Developer interested in creating Data Pipelines for serving Data Warehouses and Data Analytics platforms? Would you like to learn all about ETL and ELT data pipelines and how to build them using Bash scripting and open-source tools such as Apache Airflow and Apache Kafka? This course may be just right for you. 

ETL stands for Extract, Transform, and Load. It refers to the process of curating data from multiple sources and preparing the data for integration and loading into a destination platform such as a data warehouse or analytics environment. ELT is similar but loads the data in its raw format, reserving the transformations for people to apply themselves in a ‘self-serve analytics’ destination environment. Both methods are typical examples of data pipeline deployments. 

In this course, you will explore the fundamental principles and techniques behind ETL and ELT processes. You will learn how to construct a basic ETL data pipeline from scratch using Bash shell-scripting. You will also learn about the tools, technologies, and use cases for the two main paradigms within data pipeline engineering: batch and streaming data pipelines.  You will further cement this knowledge by exploring and applying two popular open-source data pipeline tools: Apache Airflow and Apache Kafka.

You will learn all about Apache Airflow and use it to build, put into production, and monitor a basic batch ETL workflow. You will implement this data pipeline using Airflow’s central construct of a directed acyclic graph (DAG), consisting of simple Bash tasks, Python function and their dependencies.

You will also learn about Apache Kafka and use it to get hands-on experience with streaming data pipelines, implementing Kafka’s message producers and consumers, and creating a Kafka weather topic.

What is ETL? see ETL1.png ETL2.png ETL3.png ETL4.png

What is ELT? see ELT1.png ELT2.png ELT3.png ELT4.png

Comparing ETL and ELT? see comparingETLandELT1.png comparingETLandELT2.png comparingETLandELT3.png comparingETLandELT4.png comparingETLandELT5.png

What are the data extraction techniques? see rawDataSourcesExamples1 rawDataSourcesExamples2 techniquesForExtractingData1 techniquesForExtractingData2 usecases.png summaryDataExtractionTechniques.png

What are some data transformation techniques (formating data to suit the application)? see dataTransformationTechniques1.png dataTransformationTechniques2 dataTransformationTechniques3
dataTransformationTechniques4 dataTransformationTechniques5 dataTransformationTechniques6 
dataTransformationTechniques7

Examples of data loading techniques? see dataLoadingTechniques1.png dataLoadingTechniques2 dataLoadingTechniques3
dataLoadingTechniques4 dataLoadingTechniques5 dataLoadingTechniques6 dataLoadingTechniques7 dataLoadingTechniques8
dataLoadingTechniques9

ETL Techniques

ETL stands for Extract, Transform, and Load, and refers to the process of curating data from multiple sources, 
conforming it to a unified data format or structure, and loading the transformed data into its new environment. 

see ETLTechniques.png

Extract 

Data extraction is the first stage of the ETL process, where data is acquired from various source systems. The data 
may be completely raw, such as sensor data from IoT devices, or perhaps it is unstructured data from scanned medical
documents or company emails. It may be streaming data coming from a social media network or near real-time stock 
market buy/sell transactions, or it may come from existing enterprise databases and data warehouses.

Transform 
The transformation stage is where rules and processes are applied to the data to prepare it for loading into the 
target system. This is normally done in an intermediate working environment called a “staging area.” Here, the data 
are cleaned to ensure reliability and conformed to ensure compatibility with the target system.  

Many other transformations may be applied, including:  

Cleaning: fixing any errors or missing values  

Filtering: selecting only what is needed  

Joining: merging disparate data sources  

Normalizing: converting data to common units  

Data Structuring: converting one data format to another, such as JSON, XML, or CSV to database tables 

Feature Engineering: creating KPIs for dashboards or machine learning   

Anonymizing and Encrypting: ensuring privacy and security 

Sorting: ordering the data to improve search performance 

Aggregating: summarizing granular data 

Load 

The load phase is all about writing the transformed data to a target system. The system can be as simple as a 
comma-separated file, which is essentially just a table of data like an Excel spreadsheet. The target can also be a 
database, which may be part of a much more elaborate system, such as a data warehouse, a data mart, data lake, or 
some other unified, centralized data store forming the basis for analysis, modeling, and data-driven decision making
by business analysts, managers, executives, data scientists, and users at all levels of the enterprise.

In most cases, as data is being loaded into a database, the constraints defined by its schema must be satisfied for 
the workflow to run successfully. The schema, a set of rules called integrity constraints, includes rules such as 
uniqueness, referential integrity, and mandatory fields. Thus such requirements imposed on the loading phase help 
ensure overall data quality. 

ETL Workflows as Data Pipelines 

Generally, an ETL workflow is a well thought out process that is carefully engineered to meet technical and end-user requirements.  

Traditionally, the overall accuracy of the ETL workflow has been a more important requirement than speed, although 
efficiency is usually an important factor in minimizing resource costs. To boost efficiency, data is fed through a 
data pipeline in smaller packets (see Figure 2). While one packet is being extracted, an earlier packet is being 
transformed, and another is being loaded. In this way, data can keep moving through the workflow without 
interruption. Any remaining bottlenecks within the pipeline can often be handled by parallelizing slower tasks. 

With conventional ETL pipelines, data is processed in batches, usually on a repeating schedule that ranges from 
hours to days apart. For example, records accumulating in an Online Transaction Processing System (OLTP) can be 
moved as a daily batch process to one or more Online Analytics Processing (OLAP) systems where subsequent analysis 
of large volumes of historical data is carried out. 

Batch processing intervals need not be periodic and can be triggered by events, such as  

when the source data reaches a certain size, or  

when an event of interest occurs and is detected by a system, such as an intruder alert, or  

on-demand, with web apps such as music or video streaming services 

Staging Areas 

ETL pipelines are frequently used to integrate data from disparate and usually siloed systems within the enterprise.
These systems can be from different vendors, locations, and divisions of the company, which can add significant 
operational complexity. As an example, (see Figure 3) a cost accounting OLAP system might retrieve data from 
distinct OLTP systems utilized by the separate payroll, sales, and purchasing departments.

ETL Workflows as DAGs 

ETL workflows can involve considerable complexity. By breaking down the details of the workflow into individual 
tasks and dependencies between those tasks, one can gain better control over that complexity. Workflow orchestration
tools such as Apache Airflow do just that.

Airflow represents your workflow as a directed acyclic graph (DAG). A simple example of an Airflow DAG is illustrated
in Figure 4. Airflow tasks can be expressed using predefined templates, called operators. Popular operators include 
Bash operators, for running Bash code, and Python operators for running Python code, which makes them extremely 
versatile for deploying ETL pipelines and many other kinds of workflows into production. 

Popular ETL tools 

There are many ETL tools available today. Modern enterprise grade ETL tools will typically include the following 
features: 

Automation: Fully automated pipelines 

Ease of use: ETL rule recommendations 

Drag-and-drop interface: “0-code” rules and data flows 

Transformation support: Assistance with complex calculations 

Security and Compliance: Data encryption and HIPAA, GDPR compliance 

Some well-known ETL tools are listed below, along with some of their key features. Both commercial and open-source 
tools are included in the list. 

Talend Open Studio

Supports big data, data warehousing, and profiling

Includes collaboration, monitoring, and scheduling

Drag-and-drop GUI for ETL pipeline creation

Automatically generates Java code

Integrates with many data warehouses

Open-source


AWS Glue

ETL service that simplifies data prep for analytics 

Suggests schemas for storing your data

Create ETL jobs from the AWS Console


IBM InfoSphere DataStage 

A data integration tool for designing, developing, and running ETL and ELT jobs

The data integration component of IBM InfoSphere Information Server

Drag-and-drop graphical interface

Uses parallel processing and enterprise connectivity in a highly scalable platform

Alteryx 

Self-service data analytics platform 

Drag-and-drop accessibility to ETL tools

No SQL or coding required to create pipelines


Apache Airflow and Python

Versatile “configuration” as code data pipeline platform

Open-sourced by Airbnb

Programmatically author, schedule, and monitor workflows

Scales to Big Data

Integrates with cloud platforms

The Pandas Python library 

Versatile and popular open-source programming tool 

Based on data frames – table-like structures

Great for ETL, data exploration, and prototyping 

Doesn’t readily scale to Big Data


What are data pipelines? see introductiontodatapipelines1.png introductiontodatapipelines2 introductiontodatapipelines3
introductiontodatapipelines4 introductiontodatapipelines5 introductiontodatapipelines6 introductiontodatapipelines7
introductiontodatapipelines8

What are the key data pipeline processes? see keydatapipelineprocesses1.png keydatapipelineprocesses2 
keydatapipelineprocesses3 keydatapipelineprocesses4 keydatapipelineprocesses5 keydatapipelineprocesses6 
keydatapipelineprocesses7 keydatapipelineprocesses8 

what are differences between batch versus streaming data pipelines? see batchVersusStreaming1.png batchVersusStreaming2
batchVersusStreaming3 batchVersusStreaming4 batchVersusStreaming5 batchVersusStreaming6 batchVersusStreaming7
batchVersusStreaming8 batchVersusStreaming9 batchVersusStreaming10

What are the features of modern data pipeline tools? see featuresOfModernDataPipelines1 featuresOfModernDataPipelines2
featuresOfModernDataPipelines3 featuresOfModernDataPipelines4 featuresOfModernDataPipelines5 featuresOfModernDataPipelines6
featuresOfModernDataPipelines7 featuresOfModernDataPipelines8 featuresOfModernDataPipelines9 featuresOfModernDataPipelines10
featuresOfModernDataPipelines11


what is apache airflow? see airflow1.png airflow2.png

Airflow comes with a built-in scheduler, which handles the triggering of all scheduled workflows. The scheduler is responsible for submitting individual tasks from each scheduled workflow to the executor. The executor handles the running of these tasks by assigning them to workers, which then run the tasks. The web server component of the Airflow provides a user-friendly, graphical user interface. From this UI, you can inspect, trigger, and debug any of your DAGs and their individual tasks. The DAG directory contains all of your DAG files, ready to be accessed by the scheduler, the executor, and each of its employed workers. Finally, Airflow hosts a metadata database, which is used by the scheduler, executor, and the web server to store the state of each DAG and its tasks.

lifecycle of an apache airflow task state? see airflowjobstatus.png

No status -- The task has not yet been cued for execution. Scheduled -- The scheduler has determined that the task's dependencies are met and has scheduled it to run. Removed -- For some reason, the task has vanished from the DAG since the run started. Upstream failed -- An upstream task has failed. Queued -- The task has been assigned to the executor, and is waiting for a worker to become available. Running -- The task is being run by a worker. Success -- The task completed successfully, and no errors were encountered. Failed -- The task could not be completed successfully due to an error, and Up for retry -- The task will be rescheduled as per the retrial configuration. Ideally, a task should flow throughout the scheduler from no status, to scheduled, to queued, to running, and finally to success.

apache airflow features? see airflowFeatures1.png airflowFeatures2.png

what is a dag? see whatisadag1.png and whatisadag2.png

tasks and operators? see tasksandoperators.png

dag definition components? see dagdefcomponents1.png and dagdefcomponents2.png

Anatomy of a DAG
A DAG consists of these logical blocks.

Imports
DAG Arguments
DAG Definition
Task Definitions
Task Pipeline

imports block example

# import the libraries
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow.models import DAG
# Operators; you need this to write tasks!
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator


DAG Arguments block example

#defining DAG arguments
# You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'Your name',
    'start_date': days_ago(0),
    'email': ['youemail@somemail.com'],
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

DAG arguments are like the initial settings for the DAG.

The above settings mention:

The owner name
When this DAG should run from: days_ago(0) means today
The email address where the alerts are sent to
The number of retries in case of failure
The time delay between retries

The other options that you can include are:

'queue': The name of the queue the task should be a part of
'pool': The pool that this task should use
'email_on_failure': Whether an email should be sent to the owner on failure
'email_on_retry': Whether an email should be sent to the owner on retry
'priority_weight': Priority weight of this task against other tasks.
'end_date': End date for the task
'wait_for_downstream': Boolean value indicating whether it should wait for downtime
'sla': Time by which the task should have succeeded. This can be a timedelta object
'execution_timeout': Time limit for running the task. This can be a timedelta object
'on_failure_callback': Some function, or list of functions to call on failure
'on_success_callback': Some function, or list of functions to call on success
'on_retry_callback': Another function, or list of functions to call on retry
'sla_miss_callback': Yet another function, or list of functions when 'sla' is missed
'on_skipped_callback': Some function to call when the task is skipped
'trigger_rule': Defines the rule by which the generated task gets triggered

DAG definition block example

# define the DAG
dag = DAG(
    dag_id='unique_id_for_DAG',
    default_args=default_args,
    description='A simple description of what the DAG does',
    schedule_interval=timedelta(days=1),
)

Here you are creating a variable named dag by instantiating the DAG class with the following parameters:

unique_id_for_DAG is the ID of the DAG. This is what you see on the web console. This is what you can use to trigger the DAG using a TriggerDagRunOperator.

You are passing the dictionary default_args, in which all the defaults are defined.

description helps us in understanding what this DAG does.

schedule_interval tells us how frequently this DAG runs. In this case every day. (days=1).

task definitions block example

The tasks can be defined using any of the operators that have been imported.

# define the tasks
# define a task with BashOperator
task1 = BashOperator(
    task_id='unique_task_id',
    bash_command='<some bashcommand>',
    dag=dag,
)
# define a task with PythonOperator
task2 = PythonOperator(
    task_id='bash_task',
    python_callable=<the python function to be called>,
    dag=dag,
)
# define a task with EmailOperator
task3 = EmailOperator(
    task_id='mail_task',
    to='recipient@example.com',
    subject='Airflow Email Operator example',
    html_content='<p>This is a test email sent from Airflow.</p>',
    dag=dag,
)

A task is defined using:

A task_id which is a string that helps in identifying the task
The dag this task belongs to
The actual task to be performed
The bash command it represents in case of BashOperator
The Python callable function in case of a PythonOperator
Details of the sender, subject of the mail and the mail text as HTML in case of EmailOperator

task pipeline block example

# task pipeline
task1 >> task2 >> task3

You can also use upstream and downstream to define the pipeline. For example:

task1.set_downstream(task2)
task3.set_upstream(task2)

Task pipeline helps us to organize the order of tasks. In the example, the task task1 must run first, followed by task2, followed by the task task3.

how the airflow scheduler works see scheduler.png

what are the advantages of workflows as code? see advantages.png

Submit a DAG

Submitting a DAG is as simple as copying the DAG Python file into the dags folder in the AIRFLOW_HOME directory.
Airflow searches for Python source files within the specified DAGS_FOLDER.

export AIRFLOW_HOME=/home/project/airflow
echo $AIRFLOW_HOME

cp [dag_file_name].py $AIRFLOW_HOME/dags

airflow dags list | grep "[dag_file_name].py"

The logging capability is required for developers to monitor the status of tasks in DAG runs and to diagnose and debug issues. By default, Airflow logs are saved to local file systems as log files.

see logStorage.png 

how to review airflow log files see reviewlogs.png

airflow monitoring metrics see loggingandmonitoring1.png and loggingandmonitoring2.png


