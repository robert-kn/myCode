what does the value we derive from data depend on? see value.png

what topics does a data engineer need to be familiar with? see familiar-concepts.png

what does a modern data ecosystem consist of? see constituent-parts.png type-of-data.png first-step1.png first-step2.png 
challenges1.png 

the next steps? see next-steps.png

challenges that need to be met when storing data in enterprise data repositories (EDR)? see challenges2.png

challenges faced when determining how to get data to end users from EDR? see challenges3.png

business stakeholders may need dashboards, data analysts may need access to the raw data, applications may need custom api's to pull the data

what are the emerging technologies that are shaping the modern data ecosystem? see emerging-tech.png

thanks to cloud technologies every enterprise has access to what? see cloud-tech.png

what do data engineers do? see data-engineer-role1.png data-engineer-role2.png data-engineer-role3.png

what does a data analyst do? data-analyst-role1.png data-analyst-role2.png data-analyst-role3.png

what do data scientists do? see data-scientist-role1.png data-scientist-role2.png data-scientist-role3.png

what do business analysts do? business-analyst-role1.png business-analyst-role2.png 

summary of the differences between business analysts, data analysts and data scientists see summary-difference.png

what does data engineering concern itself with? see data-engineering-concern1.png data-engineering-concern2.png data-engineering-concern3.png data-engineering-concern4.png data-engineering-concern5.png data-engineering-concern6.png data-engineering-concern7.png data-engineering-concern8.png data-engineering-concern9.png 

responsibilities of a data engineer? responsibilities-data-engineer1.png responsibilities-data-engineer2.png responsibilities-data-engineer3.png

what knowledge do data engineers need to have? knowledge1.png knowledge2.png knowledge3.png knowledge4.png knowledge5.png knowledge6.png knowledge7.png

functional skills? see functional-skills1.png functional-skills2.png

Data engineers ecosystem includes: see ecosystem1.png ecosystem2.png

categories of data: see catgories1.png

the type of data drives the kind of data repositories that can be chosen to store the data and also the queries that 
can be used to store the data 

what are the different file-formats? see file-formats.png

what are the different types of data repositories? see repository-type1.png repository-type2.png

what is the data integration process? see data-integration-process.png

see data-pipeline-process.png

for languages used in data analyst ecosystem see languages1.png languages2.png languages3.png languages4.png
langauges5.png languages6.png languages7.png languages8.png languages9.png languages10.png languages11.png
languages12.png languages13.png languages14.png languages15.png languages16.png
 
see BI-reporting-tools.png

what is data comprised of? see data-comprised-of.png

what do you need to understand when working with files as a data engineer? see formats1.png formats2.png formats3.png
formats4.png formats5.png formats6.png formats7.png

what is metadata? metadata is data that provides information about other data

Here we will consider the concept of metadata within the context of databases, data warehousing, business intelligence
systems, and all kinds of data repositories and platforms.

we'll consider the following three main types of metadata:

(a) technical metadata: is metadata which defines the data structures in data repositories or platforms, primarily 
from a technical perspective. For example, technical metadata in a data warehouse includes assets such as:

Tables that record information about the tables stored in a database, like: each table's name, the number of columns
and rows each table has

A data catalog, which is an inventory of tables that contain information, like: the name of each database in the 
enteprise data warehouse, the name of each column present in each database, the names of every table that each 
column is contained in, the type of data that each column contains

The technical metadata for relational databases is typically stored in specialized tables in the database called 
the System Catalog.

(b) process metadata: Process metadata describes the processes that operate behind business systems such as data 
warehouses, accounting systems, or customer relationship management tools.

Many important enterprise systems are responsible for collecting and processing data from various sources. Such 
critical systems need to be monitored for failures and any performance anomalies that arise. Process metadata for 
such sytems includes tracking things like: process start and end times, disk usage, where data was moved from and to,
and how many users access the system at any given time

This sort of data is invaluable for troubleshooting and optimizing workflows and ad hoc queries.

(c) Business metadata: 

Users who want to explore and analyze data within and outside the enterprise are typically interested in data 
discovery. They need to be able to find data which is meaningful and valuable to them and know where that data can 
be accessed from. These business-minded users are thus interested in business metadata, which is information about 
the data described in readily interpretable ways, such as: how the data is acquired, what the data is measuring or 
describing, the connection between the data and other data sources 

Business metadata also serves as documentation for the entire data warehouse system.

Managing metadata: includes developing and administering policies and processes to ensure information can be 
accessed and integrated from various sources and appropriately shared across the entire enterprise. Creation of a 
reliable, user-friendly data catalog is a primary objective of a metadata management model. The data catalog is a 
core component of a modern metadata management system, serving as the main asset around which metadata management is
administered. It serves as the basis by which companies can inventory and efficiently organize their data systems. 
A modern metadata managment model will include a web-based user interface that enables engineers and business users 
to easily search for and find information on key attributes such as CustomerName or ProductType. This kind of model 
is central to any Data Governance initiative.

Why is metadata management important? Having access to a well implemented data catalog greatly enhances data 
discovery, repeatability, governance, and can also facilitate access to data. Well managed metadata helps you to 
understand both the business context associated with the enterprise data and the data lineage, which helps to 
improve data governance. Data lineage provides information about the origin of the data and how it gets transformed 
and moved, and thus it facilitates tracing of data errors back to their root cause. Data governance is a data 
management concept concerning the capability that enables an organization to ensure that high data quality exists 
throughout the complete lifecycle of the data, and data controls are implemented that support business objectives.

The key focus areas of data governance include availability, usability, consistency, data integrity and data security and includes establishing processes to ensure effective data management throughout the enterprise such as 
accountability for the adverse effects of poor data quality and ensuring that the data which an enterprise has can 
be used by the entire organization.

what is a data repository? see repository1.png repository2.png repository3.png repository4.png repository5.png
repository6.png repository7.png repository8.png repository9.png repository10.png repository11.png repository12.png

what is a relational database? relational1.png relational2.png relational3.png relational4.png relational5.png
relational6.png relational7.png relationa8.png relational9.png relational10.png relational11.png

what is a nosql database? see nosql1.png nosql2.png nosql3.png nosql4.png nosql5.png nosql6.png nosql7.png nosql8.png nosql9.png nosql10.png nosql11.png nosql12.png nosql13.png nosql14.png nosql15.png nosql16.png nosql17.png
nosql18.png nosql19.png nosql20.png nosql21.png nosql22.png

characteristics of data warehouses, data marts and data lakes see characteristics1.png characteristics2.png characteristics3.png characteristics4.png characteristics5.png characteristics6.png characteristics7.png characteristics8.png characteristics9.png characteristics10.png characteristics11.png characteristics12.png characteristics13.png characteristics14.png characteristics15.png characteristics16.png 

The difference also lies in how data is extracted from the source systems, the transformations that need to be applied, and how the data is transported into the mart. Dependent data marts, for example, pull data from an enterprise data warehouse, where data has already been cleaned and transformed. Independent data marts need to carry out the transformation process on the source data since it is coming directly from operational systems and external sources. 

what is ETL? see etl1.png etl2.png etl3.png etl4.png etl5.png etl6.png etl7.png etl8.png

what is ELT? see elt1.png elt2.png elt3.png elt4.png elt5.png

what is data integration? see data-integration1.png data-integration2.png data-integration3.png 

While data integration combines disparate data into a unified view of the data, a data pipeline covers the entire 
data movement journey from source to destination systems. In that sense, you use a data pipeline to perform data 
integration, while ETL is a process within data integration. There is no one approach to data integration.

data-integration4.png data-integration5.png data-integration6.png data-integration7.png data-integration8.png 

elements that are common in big data definitions see big-data1.png big-data2.png big-data3.png big-data4.png
big-data5.png big-data6.png big-data7.png

what big data processing tools currently exist? see big-data-tools1.png big-data-tools2.png big-data-tools3.png 
big-data-tools4.png big-data-tools5.png big-data-tools6.png big-data-tools7.png big-data-tools8.png big-data-tools9.png
big-data-tools10.png big-data-tools11.png big-data-tools12.png big-data-tools13.png big-data-tools14.png 
big-data-tools15.png

what are the layers of a data platform architecture? see architecture1.png architecture2.png architecture3.png
architecture4.png architecture5.png architecture6.png architecture7.png architecture8.png architecture9.png 
architecture10.png architecture11.png architecture12.png architecture13.png architecture14.png architecture15.png
architecture16.png architecture17.png architecture18.png architecture19.png architecture20.png architecture21.png

what are the factors considered when selecting and designing data stores? see data-stores1.png data-stores2.png
data-stores3.png data-stores4.png data-stores5.png data-stores6.png data-stores7.png data-stores8.png data-stores9.png
data-stores10.png data-stores11.png data-stores12.png data-stores13.png data-stores14.png data-stores15.png 
data-stores16.png 

data platforms and security see security1.png security2.png security3.png security4.png security5.png security6.png
security7.png security8.png security9.png security10.png

how to gather and import data? see gather1.png gather2.png gather3.png gather4.png gather5.png gather6.png
gather7.png gather8.png gather9.png gather10.png gather11.png gather12.png gather13.png

data wrangling see wrangling1.png wrangling2.png wrangling3.png wrangling4.png wrangling5.png wrangling6.png 
wrangling7.png wrangling8.png wrangling9.png wrangling10.png wrangling11.png wrangling12.png wrangling13.png 
wrangling14.png wrangling15.png wrangling16.png wrangling17.png 

data wrangling tools see tools1.png tools2.png tools3.png tools4.png tools5.png tools6.png tools7.png tools8.png
tools9.png tools10.png tools11.png tools12.png 

performance, tuning and troubleshooting see tuning1.png tuning2.png tuning3.png tuning4.png tuning5.png tuning6.png
tuning7.png tuning8.png tuning9.png tuning10.png tuning11.png tuning12.png tuning13.png tuning14.png tuning15.png
tuning16.png tuning17.png

Governance and compliance see governance1.png governance2.png governance3.png governance4.png governance5.png
governance6.png governance7.png governance8.png governance9.png governance10.png governance11.png governance12.png
governance13.png governance14.png governance15.png governance16.png governance17.png governance18.png governance19.png

Overview of the DataOps Methodology

Gartner defines DataOps as a collaborative data management practice focused on improving the communication, 
integration, and automation of data flows between data managers and consumers across an organization. DataOps aims 
to create predictable delivery and change management of data, data models, and related artifacts. DataOps uses 
technology to automate data delivery with the appropriate levels of security, quality, and metadata to improve the 
use and value of data in a dynamic environment.”

A small team working on a simpler or limited number of use cases can meet business requirements efficiently. As data
pipelines and data infrastructures get more complex, and data teams and consumers grow in size, you need 
development processes and efficient collaboration between teams to govern the data and analytics lifecycle. From 
data ingestion and data processing to analytics and reporting, you need to reduce data defects, ensure shorter 
cycle times, and ensure 360-degree access to quality data for all stakeholders.

DataOps helps you achieve this through metadata management, workflow and test automation, code repositories, 
collaboration tools, and orchestration to help manage complex tasks and workflows. Using the DataOps methodology 
ensures all activities occur in the right order the right security permissions. It helps set in a continual process 
that allows you to cut wastages, streamline steps, automate processes, increase throughput, and improve continually.

Several DataOps Platforms are available in the market, some of the popular ones being IBM DataOps, Nexla, 
Switchboard, Streamsets, and Infoworks.

DataOps Methodology:
The purpose of the DataOps Methodology is to enable an organization to utilize a repeatable process to build and 
deploy analytics and data pipelines. Successful implementation of this methodology allows an organization to know, 
trust, and use data to drive value.

It ensures that the data used in problem-solving and decision making is relevant, reliable, and traceable and 
improves the probability of achieving desired business outcomes. And it does so by tackling the challenges 
associated with inefficiencies in accessing, preparing, integrating, and making data available.

In a nutshell, the DataOps Methodology consists of three main phases:

The Establish DataOps Phase provides guidance on how to set up the organization for success in managing data.

The Iterate DataOps Phase delivers the data for one defined sprint.

The Improve DataOps Phase ensures learnings from each sprint is channeled back to continually improve the DataOps
process.

The figure below presents a high-level overview of these phases and the key activities within each of these phases.

see dataops.png

Benefits of using the DataOps methodology:

Adopting the DataOps methodology helps organizations to organize their data and make it more trusted and secure. 
Using the DataOps methodology, organizations can:

Automate metadata management and catalog data assets, making them easy to access.

Trace data lineage to establish its credibility and for compliance and audit purposes.

Automate workflows and jobs in the data lifecycle to ensure data integrity, relevancy, and security.

Streamline the workflow and processes to ensure data access and delivery needs can be met at optimal speed.

Ensure a business-ready data pipeline that is always available for all data consumers and business stakeholders.

Build a data-driven culture in the organization through automation, data quality, and governance.

As a data practitioner, using the methodology can help you reduce development time, cut wastages and duplication of 
effort, increase your productivity and throughput, and ensure that your actions produce the best possible quality 
of data. 

With DataOps, data professionals, consumers, and stakeholders can collaborate more effectively towards the shared 
goal of creating valuable insights for business. While implementing the methodology will require systemic change, 
time, and resources, but in the end, it makes data and analytics more efficient and reliable.

Interestingly, it also opens up additional career opportunities for you as a data engineer. DataOps Engineers are 
technical professionals that focus on the development and deployment lifecycle rather than the product itself. And 
as you grow in experience, you can move into more specialist roles within DataOps, contributing to defining the 
data strategy, developing and deploying business processes, establishing performance metrics, and measuring 
performance.

career opportunities see opportunities1.png opportunities2.png opportunities3.png opportunities4.png opportunities5.png
opportunities6.png

Data engineering learning path see path1.png path2.png path3.png path4.png path5.png 

Description of the Data Warehousing Specialist role: Data Warehousing Specialists design, model, and implement 
corporate data warehousing activities, program and configure warehouses of databases, and provide support to data 
warehouse users. The tasks for this specialist role are focussed on data warehousing and form a significant subset 
of the wider Data Engineering role.

Opportunity estimates for the Data Warehousing Specialist role: According to careeronestop.org, an organization 
sponsored by the U.S. Department of Labour, the future is bright for Data Warehouse Specialists and very similar or 
related roles, such as Data Architects and Database Administrators. The expected growth rate of opportunities in 
these fields is higher than the average, and is expected to average about 8 to 10 percent per year over the next 
decade. Approximately 13,900 openings are expected to emerge each year in the U.S. alone. According to salary.com, 
the median salary for a Data Warehouse Specialist in the US is $110,168.

Data Warehousing Specialist alternative job titles: Just like the role of Data Engineer, the Data Warehousing 
Specialist role is quite fluid and can vary considerably. In fact, the Data Warehousing Specialist is a particluar 
kind of Data Engineer, more tightly focused on the data warehousing aspects of the broader discipline. Accordingly, 
searching online job postings for this particular role returns many other closely related positions, including:

1. Data Warehouse Specialist
2. Data Warehouse Engineer
3. Data Warehouse Solution Architect
4. Data Warehousing/ETL Solution Specialist
5. Data Warehouse Architect
6. Data Architect, Data Warehousing & MPP
7. Data Warehouse Analyst
8. Data Warehouse Administrator
9. Data Warehousing Development Specialist

Tasks performed by Data Warehousing Specialists: As a specialization within the broader field of Data Engineering, 
Data Warehousing Specialists may be responsible for many kinds of tasks. These tasks may include any of the 
following:

Developing processes, procedures, and software applications for enterprise data management

Analyzing and improving data warehousing processes for efficiency, accuracy, usablilty, or security

Designing, modelling, or implementing corporate data warehousing activities

Developing or maintaining standards, such as organization, structure, or nomenclature, for the design of data 
warehouse elements, such as data architectures, models, tools, and databases

Providing or coordinating troubleshooting support for data warehouses

Writing or modifying programs to meet customer requirements

Creating documentation such as metadata and diagrams of entity relationships, business processes, and process flow

Designing, implementing, or operating comprehensive data warehouse systems to balance optimization of data access 
with batch loading and resource utilization factors, according to customer requirements

Performing system analysis, data analysis or programming, using a variety of computer languages and procedures

Reviewing designs, code, test plans, or documentation to ensure quality

Creating plans, test files, and scripts for data warehouse testing, ranging from unit to integration testing

Implementing business rules via stored procedures, middleware, or other technologies

Supporting users of the data warehouse

Technical Skills required for the Data Warehousing Specialist role:

Programming - writing computer programs for various purposes.
Systems analysis - determining how a business system works, and understanding how changes in conditions may affect 
outcomes.
Data architecture - understanding the models, the policies, rules or standards that govern which data is collected, 
how data is stored, arranged, and integrated, and how to put data to use in data systems.
Data management and analysis - securely collecting, storing, and analyzing data.
Data pipelines - building and maintaining data pipelines.
Business intelligence - data warehousing; extract, transform, and load (ETL); and data mapping.
Database normalization - data integrity and normalization.
Data storage structures - especially relational databases.
Data migration - ETL of data from one system to another.
Metadata management and metadata standards.
Data integration platforms.
Overview of the Data Engineering Ecosystem.
Cloud data - building scalable cloud data infrastructure.
Database administration, including Big Data administration.
Data acquisition and transformation - Digitizing data for display, analysis, and storage.
Data lakes, data marts, data reservoirs.
IoT - Integrating data from various connected devices and systems in IoT using data pipelines.
Building event streaming pipelines.

Law & Government:

Acts & Regulations - knowledge of laws, regulations, requirements and ethical issues related to the access and use 
of information, for example intellectual capital, personally identifiable information, and customer data.

Software and IT skills:

Cloud computing and cloud platforms: Amazon Web Services (AWS), Microsoft Azure, SpringCloud, GCS (Google Cloud 
Storage).
Data warehouse tools: Snowflake, Data Bricks, BigQuery, Redshift, Db2.
Data pipeline tools: Apache Kafka, Apache Airflow, Luigi.
Big data tools: Apache Hadoop, Apache Spark, Apache Hive.
Operating systems: UNIX, Linux.
Programming languages: SQL, Bash, Python, R, Java, C++.
Databases - Cassandra, Microsoft SQL Server, MySQL, PostgreSQL, Amazon DynamoDB, Apache Solr, IBM Db2, MongoDB, 
neo4j, Oracle PL/SQL, PostgreSQL.
Metadata management software - CA Erwin Data Modeler; Oracle Warehouse Builder; SAS Data Integration Server; Talend 
Data Fabric; Alation Data Catalog, SAP Information Steward, Azure Data Catalog, IBM Watson Knowledge Catalog, 
Oracle Enterprise Metadata Management (OEMM), Adaptive Metadata Manager, Unifi Data Catalog, data.world, and 
Informatica Enterprise Data Catalog.
Agile software development methodologies.
Version control - Git.
Modelling and API development.
Business intelligence and data analysis software - IBM Cognos Impromptu, MicroStrategy, Microsoft Power BI, Google 
Analytics, InsightSquared, Oracle Business Intelligence Enterprise Edition, Qlik Tech QlikView, Sisense, Tableau, 
Dundas BI, SAS Analytics, Domo, SAP Lumira

Pathways to becoming a Data Warehouse Specialist:

There are many possible paths to becoming a Data Warehousing Specialist. Most practitioners have a minimum of a 
Bachelor’s Degree in a mathematical or computational field such as Computer Science, Computer Engineering or the 
Mathematical Sciences. However, many practitioners instead have a Technology or Technical Diploma in a 
Computational or Information Technology discipline. Combining this educational background with some hands-on 
experience with application development and use of software for managing databases and metadata is a good way to 
prepare for the role.

