what is NoSQL? see nosql-1.png nosql-2 nosql-3 nosql-4 

Let's take a look at the history of the NoSQL movement going back to the period between 1970 and 2000. Although there were some 
nonrelational databases like IBMs IMS, which was a hierarchical database used for the Apollo space missions, the market was dominated 
by relational databases. So when application architects and developers needed a data store for their applications, they were pretty 
much picking from a variety of common relational databases.

When Internet applications and companies started exploding during the dot-com boom in the late nineties and early 2000s, applications went from needing to serve thousands of internal employees at companies to needing to serve millions of users on the public Internet. For these applications, availability and performance were paramount. And these new scale problems led to a drive to create new scalable technologies to support them. During that time, several large tech companies, including IBM, Google and Meta, developed a lot of innovative technology, released white papers and open sourced their technology.

In the last ten years or so, several NoSQL databases have leveraged a fully managed service model, otherwise called database as a service or DBaaS. To offload the administration and maintenance from the end user and allow developers to focus on building applications with these modern databases.

why NoSQL? see nosql-5 nosql-6

what are the characteristics of NoSQL databases? see nosql-7 nosql-8 nosql-9 nosql-10 nosql-11

benefits of NoSQL databases? see nosql-12 nosql-13 nosql-14 nosql-15 Historically, large databases have run on expensive machines or mainframes. Modern enterprises are employing cloud architectures to support their applications, and the distributed data nature of NoSQL databases means that they can be deployed and operated on clusters of servers in cloud architectures, thereby massively reducing cost nosql-16. Cost is important for any technology venture, and it is common to hear of NoSQL adopters cutting significant costs versus their existing databases... and still be able to get the same or better performance and functionality nosql-17. nosql-18
nosql-19

Document store databases

Document-store databases, also known as document-oriented databases, store data in a document format, typically JSON or BSON (binary JSON), where each document contains key-value pairs or key-document pairs. These databases are schema-less, allowing flexibility in data structures within a collection.

Characteristics
1. Provides schema flexibility: Documents within collections can have varying structures, allowing for easy updates and accommodation of evolving data requirements.
2. Performs efficient create, read, update, and delete (CRUD) operations: well-suited for read and write-intensive applications due to their ability to retrieve whole documents.
3. Provides scalability: horizontal scalability by sharding data across clusters.

Use cases
1. Content management systems (CMS): CMS platforms like WordPress use document store databases for fast storage and access to content types such as articles, images, and user data. (MongoDB)
2. E-commerce: E-commerce platforms need effective management of product catalogs with diverse attributes and hierarchies, accommodating the dynamic nature of e-commerce product listings. (Couchbase or Amazon DocumentDB, using MongoDB compatibility)

Frequently mentioned vendors
MongoDB
Couchbase
Amazon DocumentDB

Key-value stores

Key-value stores are the simplest NoSQL databases, storing data as a collection of key-value pairs, where the key is unique and directly points to its associated value.

Characteristics

Delivers high performance: efficient for read and write operations, optimized for speedy retrieval based on keys
Provides scalability: easily scalable due to their simple structure and ability to distribute data across nodes
Uses caching for fast access
Provides session management
Works with distributed systems

Use cases:
1. Enhanced web performance by caching frequently accessed data (Using Redis or Memcached)
2. E-commerce platforms, software applications, including gaming: Amazon DynamoDB provides a highly scalable key-value store, facilitating distributed systems' seamless operation by handling high traffic and scaling dynamically.

Frequently mentioned vendors
Redis
Memcached
Amazon DynamoDB

Column-family stores

Definition: Column-family stores NoSQL databases, also referred to as columnar databases, organize data in columns rather than rows. These databases store columns of data together, making them efficient for handling large data sets with dynamic schemas.

Characteristics
Uses column-oriented storage: Data is grouped by columns rather than rows, allowing for efficient retrieval of specific columns.
Delivers scalability: Distributed architecture for high availability and scalability.

Use cases
IoT applications manage massive amounts of sensor data efficiently due to their ability to handle time-stamped data at scale, referred to as time-series data analysis. (Apache Cassandra)
Applications that store and analyze user preferences and behaviors usually deliver personalization. (HBase, part of the Hadoop ecosystem)
Large-scale data analysis.

Frequently mentioned vendors
Apache Cassandra
HBase

Graph databases:

Definition: Graph NoSQL databases are designed to manage highly interconnected data, representing relationships as first-class citizens alongside nodes and properties.

Characteristics:
Analyzes the data using a graph data model: relationships are as important as the data itself, enabling efficient traversal and querying of complex relationships.
Fast performance for relationship queries: optimized for queries involving relationships, making them ideal for social networks, recommendation systems, and network analysis.

Use cases:
Social networks require efficient data management of relationships between users, posts, comments, and likes. (Neo4j)
Recommendation systems: Organizations need a database structure that can create sophisticated recommendation engines, analyzing complex relationships between users, products, and behaviors for precise recommendations. (Amazon Neptune)

Frequently mentioned vendors
Neo4j
Amazon Neptune
ArangoDB Memcached

Wide-column stores:

Wide-column store NoSQL databases organize data in tables, rows, and columns, like relational databases, but with a flexible schema.

Characteristics:
Use columnar storage: Data is stored in columns, allowing for efficient retrieval of specific columns rather than entire rows.
Provide horizontal scalability and fault tolerance.

Use cases:
Analyzing big data: Efficiently handling large-scale data processing for real-time big data analytics. (Apache HBase used in conjunction with Hadoop)
Managing enterprise content: Large organizations databases need to manage vast amounts of structured data like employee records or inventory due. (Cassandra)

Frequently mentioned vendors
Apache HBase
Apache Cassandra

Expanded use case example: Using MongoDB for a content management system (CMS)

Content management systems (CMS) intelligently collect, govern, manage, and enrich enterprise content, including HTML pages, images, articles, and more. Content management systems help companies deploy their content efficiently and securely across any cloud and within any application.

Good content management means that team members can quickly add, update, and remove content from the database and the associated pages that feature that content. Examples include pushing out breaking news, updating current news, including weather forecasts, pushing advertising content, updating college admission policies, launching new city services, and more.

For example, using MongoDB as a backend database for a content management system (CMS) is a practical choice when you need to manage and serve a variety of content types, especially in scenarios where you expect frequent schema changes or scaling requirements.

Next, let's check out some of the aspects of managing content using a content management system, specifically using MongoDB.

Content structure using MongoDB

In MongoDB, you represent content as documents. Each document corresponds to a piece of content, such as an article, image, video, or page. You can use the subdocuments in the document to organize the content hierarchy and structure.

Example of structuring: Storing a blog post
When storing a blog post, you will store core attributes like title, content, created at, and the image URL. Then, using an array field, you can store tags. The comments on that post are stored as an array of objects.

// Collection: posts
{
"\_id":1,
"title":"Sample Blog Post",
"content":"This is the content of the blog post...",
"author":{
"name":"John Doe",
"email":"john@example.com",
"bio":"A passionate blogger.",
"created\_at":"2023-09-20T00:00:00Z"
},
"created\_at":"2023-09-20T08:00:00Z",
"tags":["mongodb","blogging","example"],
"comments":[
{
"text":"Great post!",
"author":"Emily Johnson",
"created\_at":"2023-09-20T10:00:00Z"
},
{
"text":"Thanks for sharing!",
"author":"James Martin",
"created\_at":"2023-09-20T11:00:00Z"
}
]
}

Metadata and indexing using MongoDB

You can use the indexing capabilities of MongoDB to optimize content retrieval. You can create indexes on fields commonly used for filtering or searching, such as keywords, publication date, or content type, or use MongoDB's text index support for text search queries on fields containing string content. Text indexes improve performance when searching for specific words or phrases within string content.

For example, you want to provide searching capability on the content of your blogs. You will first create a text index:

db.articles.createIndex( { subject: "text" } )

And then you can provide a query such as:

db.posts.find( { $text: { $search: "digital life" } } )

where MongoDB will look for stemmed versions of these words: digital or life

Scaling your CMS using MongoDB

As your CMS grows, MongoDB can help you scale. You can use sharding for horizontal scaling or use zone-based sharding for global distribution.

Using sharding for horizontal scaling (increased capacity)
Let's consider a company that currently has 100 million customers. This company expects to expand its customer base to 200 million customers. This increase in the number of customers means that the company will need to double its IT data storage hardware. The company can scale vertically, which can cost exponentially more as the hardware cost isn't linear with the performance. The following diagram shows that the company can scale horizontally and use sharding to manage the databases. see sharding.png

key value NoSQL databases: see key-value-1.png key-value-2 key-value-3 key-value-4 key-value-5

document based NoSQL databases: see document-1 document-2 document-3 document-4 document-5 

column based nosql databases: see column-db-1.png column-db-2. suitable use cases; are great for when you're dealing with large amounts of sparse data. When compared to row-oriented databases, column-based databases can better compress data and save storage space. In addition, these databases continue the trend of horizontal scalability. As with key value and document databases, column-based databases can handle being deployed across clusters of nodes. Like document databases, a column-based NoSQL database can be used for event logging and blogs, but the data would be stored in a different fashion. For enterprise logging, every application can write to its own set of columns and have each row key formatted in such a way to promote easy lookup based on application and timestamp. Counters are a unique use case for column-based databases. You may come across applications that need an easy way to count or increment as events occur. Some column-based databases, like Cassandra have special column types that allow for simple counters. In addition, columns can have a time-to-live parameter, making them useful for data with an expiration date or time like trial periods or ad timing. see column-db-3 column-db-4

graph nosql databases: see graph-db-1 graph-db-2 graph-db-3 graph-db-4 graph-db-5

NoSQL Database Deployment Options

Database deployment options refer to the various methods and strategies for implementing and managing databases within an organization. Choosing the right deployment option is crucial for optimizing performance, scalability, and efficiency. Here, you'll explore some common database deployment options and their key characteristics

What you will learn

After completing this reading, you'll be able to

Identify methods for deploying NoSQL databases
List examples of NoSQL database deployments
Describe some of the advantages and challenges associated with deploying NoSQL databases

On-premises deployment
On-premises deployment involves hosting the entire database infrastructure within the organization's physical location or a dedicated data center.

Example
An organization manages customer relationship data using an on-premises Oracle Database installed on dedicated servers within its corporate data center.

Advantages
Full control: Organizations have complete control over hardware specifications, software configurations, and security measures.

Compliance: On-premises solutions might be necessary for industries with stringent regulatory or compliance requirements.

Challenges
Upfront costs: High initial investments in hardware, infrastructure, and skilled personnel.

Scalability: Limited scalability compared to cloud alternatives, which might result in challenges during periods of rapid growth.

Cloud deployment
Cloud deployment involves utilizing cloud service providers to host and manage databases over the internet.

Example
A startup leverages Amazon Web Services (AWS) to deploy and host its e-commerce database, utilizing Amazon relational database service (RDS) for scalability and managed database services.

Advantages
Scalability: Resources can be scaled up or down based on demand, providing flexibility and cost savings.

Cost-effectiveness: Pay-as-you-go pricing allows organizations to pay only for the resources they consume.

Automation: Cloud providers handle routine maintenance tasks, updates, and backups.

Challenges
Connectivity dependencies: Relies on internet connectivity, which might pose issues in areas with limited or unreliable access.

Security concerns: Requires robust security measures to protect sensitive data from unauthorized access.

Hybrid deployment
Hybrid deployment combines on-premises and cloud solutions, allowing organizations to distribute data and workloads based on specific needs.

Example
A financial institution stores sensitive customer information, such as account balances, in an on-premises database while utilizing a cloud-based service, like AWS Lambda, for processing non-sensitive data analytics workloads.

Advantages
Flexibility: Companies can keep sensitive data on-premises while using the cloud for scalable and dynamic workloads.

Disaster recovery: Companies can use cloud storage for backups, enhancing disaster recovery capabilities.

Challenges
Integration complexity: Integration complexities require effective integration between on-premises and cloud environments for seamless operation.

Management overhead: Monitoring and managing two different environments might increase administrative complexity.

Database as a service (DBaaS)
DBaaS provides a fully managed database solution where the service provider handles administrative tasks, allowing organizations to focus on application development.

Example
A software development company uses Microsoft Azure SQL Database as a DBaaS solution to host and manage its relational databases, allowing developers to focus on application development rather than database administration tasks.

Advantages
Reduced overhead: Organizations benefit from reduced administrative tasks, including maintenance, backups, and updates.

Rapid deployment: Quick deployment without the need for in-depth database expertise.

Challenges
Limited control: Organizations have less control over underlying infrastructure, which might be a concern for some organizations.

Data security: Trusting a third-party provider with sensitive data raises security and privacy considerations.

Containerized deployment
Containerization involves packaging the database and its dependencies into containers for consistent deployment across various environments.

Example
A tech company adopts Docker containers to deploy its microservices-based application, with each microservice encapsulating a specific function and a separate container running a MongoDB database.

Advantages
Portability: Containers ensure consistent operation across development, testing, and production environments.

Resource efficiency: Lightweight containers consume fewer resources compared to traditional virtual machines.

Challenges
Orchestration complexity: Production-scale deployment requires knowledge of container orchestration tools like Kubernetes.

Learning curve: Teams might need to familiarize themselves with containerization concepts and tools.

Serverless deployment
Serverless architecture provisions and scales database resources automatically based on demand, with organizations paying for actual usage.

Example
A mobile app developer utilizes Google Cloud Firestore as a serverless NoSQL database for storing user data, where the database scales automatically based on demand, and the developer only pays for the data storage and operations used by the app.

Advantages
No manual provisioning: No need for manual provisioning or maintenance of server resources.

Cost savings: Efficient resource utilization leads to cost savings as organizations only pay for the resources consumed.

Challenges
Limited control: Organizations have less control over underlying infrastructure, which might concern some organizations.

Applicability: Serverless might not be suitable for all types of databases or workloads due to architectural constraints.

Recap
In this reading, you learned that:

The choice of a database deployment option depends on organizational priorities, budget constraints, scalability needs, and how the company manages the nature of the data. It's essential to carefully evaluate these options in alignment with specific use cases to ensure optimal performance and efficiency.

On-premises deployment involves hosting the entire database infrastructure within the organization's physical location or a dedicated data center, providing full control and compliance with regulatory requirements but with high upfront costs and limited scalability.

Cloud deployment uses cloud service providers to host and manage databases. Cloud deployment offers scalability, cost-effectiveness, and automation. However, cloud deployments have connectivity dependencies and can be prone to security concerns.

Hybrid deployment combines on-premises and cloud solutions, allowing organizations to distribute data and workloads based on specific needs, providing flexibility and enhanced disaster recovery capabilities; however, effective integration between on-premises and cloud environments requires increased administrative complexity and costs.

Database as a Service (DBaaS) provides a fully managed database solution where the company's choice service provider handles administrative tasks. When using DBaaS, organizations focus on application development but usually have limited control over underlying infrastructure and data security.

Containerized deployment involves packaging the database and its dependencies into containers for consistent deployment across various environments, which can ensure consistent operations and resource efficiency—however, if containerization practices are not already in place, the company will need time for employees to gain knowledge of container orchestration tools and implement them.

Serverless deployment provisions and scales database resources automatically based on demand, with organizations paying for actual usage, eliminating manual provisioning and maintenance of server resources, and leading to cost savings. However, companies usually have limited control over the underlying infrastructure, and serverless deployments might not be technically suitable for all databases or workloads.

ACID versus BASE: see acid-base-1.png. The ACID acronym stands for atomic. All operations in a transaction succeed, or every operation is rolled back. Consistent. On the completion of a transaction, the structural integrity of the data in the database is not compromised. Isolated. Transactions cannot compromise the integrity of other transactions by interacting with them while they are still in progress. Durable. The data related to the completed transaction will persist, even in the case of network or power outages. If a transaction fails, it will not impact the already changed data. acid-base-2 acid-base-3. The BASE acronym stands for basically available. Rather than enforcing immediate consistency BASE modeled NoSQL databases will ensure the availability of data by spreading and replicating it across the nodes of the database cluster. Soft state. Due to the lack of immediate consistency, data values may change over time. In the BASE model, data stores don't have to be right consistent nor do different replicas have to be mutually consistent all the time. Eventually consistent. The fact that the BASE model does not enforce immediate consistency does not mean that it never achieves it. However, until it does, data reads might be inconsistent.

Distributed databases: 

A distributed database is a collection of multiple interconnected databases, which are spread physically across various locations that communicate via a computer network. A distributed database is physically distributed across the data sites by fragmenting and replicating the data. And a distributed database follows the base consistency model. see distributed-1.png distributed-2
distributed-3 distributed-4 distributed-5

MongoDB Design Patterns

MongoDB design patterns help optimize data models based on application queries and usage. However, MongoDB design patterns improve application performance by reducing schema complexity. It helps identify data storage patterns and the type of data returned to the application.

However, MongoDB design patterns are best practices for structuring data and queries to optimize performance, scalability, and maintainability. These patterns arise from common use cases and challenges encountered. Some prominent MongoDB design patterns include approximation, attribute, polymorphic, outliers, and bucket. Let's read about each of these MongoDB design patterns.

Approximation

Approximation MongoDB design pattern helps calculate frequent expenses, where the precision for those calculations is not a priority. For example, Instagram followers are for celebrities.

The approximation design pattern is useful for fewer writes to the database and maintains statistically valid numbers. However, it cannot represent exact numbers

Attribute

The Attribute MongoDB design pattern helps document various characteristics and similar fields that you cannot identify when raising a query. This attribute is useful when the fields need to be sorted and found in a small subset of documents or when both conditions meet within the documents, such as when selling products on e-commerce.

The attribute MongoDB design pattern is useful in less indexing and generating simple and faster queries.

Polymorphic

The polymorphic MongoDB design pattern helps store different documents in the same collection. However, it often uses a discriminator field to differentiate between types of documents, such as customer communication stored using various channels.

This type of design pattern is easy to implement and runs queries across a single collection.

Outlier

The Outlier MongoDB design pattern is useful for queries or documents that don't fit into the typical data pattern, such as a list of a celebrity's social media followers (which could be in millions). This design pattern prevents documents or queries from determining an application's solution; however, it is tailored for typical use cases but doesn't address ad hoc queries.

Bucket Pattern

The bucket pattern in MongoDB design helps to manage streaming data such as time series, real-time analytics, or Internet of Things (IoT) applications. The bucket MongoDB design pattern reduces the number of documentation while collecting them, improves index performance, and simplifies data access by leveraging pre-aggregation.

For example: Collecting weather data using multiple sensors. This data collection helps to reduce efforts to review temperature and wind speed every minute. However, you would receive a summary per hour, such as:

Average temperature
Average wind speed
However, you can review the weather forecast in detail. For example, find the temperature per minute and calculate the median and variance. The benefit of the bucket design pattern is that it doesn't repeat device identifiers and entry data with every record. However, if this didn't happen, there would be 1440 records per device per hour, creating a large amount of data.

Summary

In this reading, you've learned that MongoDB design patterns optimize the data model to access the application's patterns. Such as:

Approximation
Attribute
Polymorphic
Outliers
Bucket

CAP theorem: see cap-1 cap-2 cap-3 cap-4 In a cluster with eight distributed nodes, a network partition could occur, and communication will be broken between all the nodes. In our case, instead of one 8-node cluster we will have two smaller 4-node clusters available. Consistency between the two clusters will be achieved when network communication is re-established. Partition tolerance has become more of a necessity than an option in distributed systems. It is made possible by sufficiently replicating records across combinations of nodes and networks. For such systems as NoSQL, since partition tolerance is mandatory, a system can be either Consistent and Partition Tolerant (CP) or Available and Partition Tolerant (AP). Existing NoSQL systems, like MongoDB or Cassandra, can be classified using CAP Theorem. For example, MongoDB chooses consistency as the primary design driver of the solution, and Apache Cassandra chooses availability. This doesn't mean that MongoDB cannot be available, or that Cassandra cannot become fully consistent. It means that these solutions first ensure that they are consistent (in the case of MongoDB) or available (in the case of Cassandra) and the rest is tunable. see cap-5

challenges in migrating from RDBMS to NoSQL databases: see rdbms-to-nosql-1 rdbms-to-nosql-2 rdbms-to-nosql-3 In NoSQL models should be based on how the app interacts with the data rather than how the model can be stored as rows in one or more tables. Another factor to pay attention to is the fact that in RDBMS, data is normalized while in NoSQL it is denormalized. With NoSQL, starting from your query means that you will structure your data on disk accordingly. Thus, you may need to store the same data in different models just to answer the question, and this will lead to data denormalization. While data in RDBMS is normalized, you need to be open to the situation in which you start with your queries instead of your data. rdbms-to-nosql-4. When migrating from relational to NoSQL databases, you need to understand that sometimes services require availability more than consistency when both availability and performance are needed. Thus, distributed systems consistency cannot be ensured. Remember cap theorem, many of today's online services value availability more than consistency. And because of this they look for systems that can provide it, taking into consideration the amount of data they are dealing with and their geographical presence. One last thing to know when dealing with NoSQL databases, these are not designed to support transactions or joins or complex processing except in limited cases. You need to consider this when moving from RDBMS to NoSQL.

Data Modeling in Relational and Document-based Databases

After completing this reading, you will be able to:

Compare modeling the same data in relational and document databases.
Describe the complexities of querying data.
Describe how data duplication affects data modeling
Describe the considerations associated with evolving the schema.

Introduction

First, compare the information organization in a relational database to a NoSQL document database.

A relational database organizes data into tables with predefined schemas and relationships among tables. In a relational model, you avoid duplicating any data so that the data is stored once and updated in one location. Wherever and whenever the data is needed, the data is referenced. That 's the relational aspect of the data.

On the other hand, an excellent document-based design starts with using that data.

Let's use the example of library books and data organization.

Example: A library book catalog

Library book catalogs store information about books and their authors.

Using a relational database for a library books catalog
If you work with library book data using a relational database, you will create the Books, Authors, Genres and BookGenres tables:

see books.png authors.png genres.png book-genres.png

Using a document database for a library books catalog

In contrast, a document database is simplified and displayed as a single document with all the required information.

{“_id”: 1,”title”: “The Great Gatsby”,”author”: “F. Scott Fitzgerald,””isbn”: “978-0743273565”,”published_year”: 1925,”genres”: [“Fiction”, “Classic”]}

Next, compare the complexities of querying the data in relational and document databases.

Complexities of querying data

Let's examine the complexities of querying the same book data using a relational database compared to a NoSQL database.

Querying book data using a relational database

Now, query the data. You will need all the book details at once. You will structure your SQL query by performing a JOIN among those tables. Here's your SQL query example:

SELECT Books.Title AS BookTitle, Authors.Name AS Author, GROUP_CONCAT(Genres.GenreName) AS Genres FROM Books JOIN Authors ON Books.AuthorID = Authors.AuthorID JOIN BookGenres ON Books.BookID = BookGenres.BookID JOIN Genres ON BookGenres.GenreID = Genres.GenreID GROUP BY Books.Title, Authors.Name;

The number of joins, use of the GROUP_CONCAT function, and the GROUP BY function make this query complex. Here's how the query breaks down.

The SELECT command obtains the book title, author name, and a concatenated list of genres for each book.
The JOIN command joins the Books table with the Authors table on the AuthorID field to get the author’s name.
The JOIN command joins the Books table with the BookGenres table on the BookID field to associate books with genres.
The JOIN command joins the BookGenres table with the Genres table on the GenreID field to get genre names.
The query uses the GROUP_CONCAT function (the exact function may vary depending on your SQL database system; GROUP_CONCAT is used in MySQL and STRING_AGG in PostgreSQL) to concatenate multiple genre names into a single string.
The query uses GROUP BY to display the results by book title and author name.

Here is an example output: example-output.png

Querying book data using a NoSQL document database

You already know the output you need and that the output is a first-order criterion in a document database. Remember that your book document looks like this:

{“_id”: 1,”title”: “The Great Gatsby”,”author”: “F. Scott Fitzgerald,””isbn”: “978-0743273565”,”published_year”: 1925,”genres”: [“Fiction”, “Classic”]}

When creating a document database query, using MongoDB, for example, you specify empty brackets as the first argument denoting you want to see all documents. The second argument, called projection, allows you to choose which fields to present to the client information. You won 't always need to show all of the fields all of the time. And you'll notice the simplicity of this query, as it didn 't need any joins with other collections.

Here is your query:

Db.books.find({}, { title:1, author: 1, genres: 1 })

The data you need to fulfill the request is already in the document and does not require any joins with other collections.

Important! What about data duplication?

Data duplication is a common practice and is known as "denormalization." Data duplication can improve read performance by avoiding complex joins and queries. However, you will face challenges with data consistency and increased storage requirements.
For example, imagine that the author, J.R.R. Tolkien, wants to be known by his full name, John Ronald Reuel Tolkien, and you need to make this change in a relational database and a document database. How would you implement this change in these two different types of databases?

Relational database	
This request requires only one change in the Authors table in a relational database.	

Document database
Since the author has written 12 books in a document database, you must update this information in 12 documents, which is a small price to pay, as this is a rare event.

Next, explore the considerations associated with evolving a schema.

Schema Evolution Considerations

Changing the schema in a database can be necessary to accommodate evolving application requirements. However, a significant difference exists in how a relational and document database enables schema changes.

In relational databases, changing the schema typically involves modifying existing tables, adding new tables, or altering relationships between tables. Here's an example:

ALTER TABLE table_name ADD column_name data_type;

After making schema changes, you might need to migrate existing data to match the new schema, which can involve data transformation and migration scripts.

In document databases like MongoDB, the schema is typically more flexible, and you can often add or remove fields to documents (individual documents, as there is generally no collection-wide enforced schema) without a predefined structure. Here's an example:

Db.books.update ({ _id: 1 },{$set: {Newfield: “Some value”}});

Summary

In this reading, you learned that:

A relational database organizes data into tables with predefined schemas and relationships among tables.
A good document-based design starts with the usage of data.
The number of SELECT, JOIN, GROUP, and additional commands contribute to the complexity of building a relational database query.
For document database queries, you won 't always need to show all of the fields all of the time, and you'll notice the simplicity of these queries, as the queries might not need joins with other collections.
In relational databases, changing the schema typically involves modifying existing tables, adding new tables, or altering relationships between tables.
In document databases, such as MongoDB, the schema is typically more flexible, and you can often add or remove fields to documents without a predefined structure.

Vector Databases

Vector databases, a newer NoSQL database, are rapidly becoming popular with the exponential increase in the use of Large Language Models (LLMs), such as OpenAI's GPT. But what is a vector database? Here's how IBM defines a vector database:

A vector database is designed to store, manage, and index massive quantities of high-dimensional vector data efficiently.
Source: https://www.ibm.com/topics/vector-database

Vectors

You can transform text, images, audio, and video into vectors, also known as vector data, using embedding functions based on various methods, including machine learning models, word embeddings, and feature extraction algorithms. see vectors-1.png

For example, the vector representation of dog is [2.1,-0.3, 7.2, 9.6, 6.1]

Similar words for dogs include the word canine or K9, so a vector database will identify both terms and include the same vector values. see vectors-2.png 

Important! When words have relationships or similar contexts, but the meaning is not identical, these words have vectors that are closer together within the database that help identify the relationship.

For example, you'll see the word animal represented as [1.9, -0.4, 7.2, 8.0, 6.3]

Each numeric value you see displayed inside of a vector is one dimension. see vectors-3.png

Vectors can have a few or thousands of dimensions, depending on the granularity and complexity of the classification required.

Returning to the example of a dog classifications, we know a dog is an animal, but not all animals are dogs. The relevance and relationships between the words "dog" and "animal" mean that these words will be much closer together as vector values and within the database itself. The following illustration shows the one dimension, dimension 7.2, that the word dog and animal share.
see vectors-4.png 

Next, explore why companies are moving to vector databases.

Vector Database benefits
In contrast to conventional techniques that involve querying databases for exact matches or predefined criteria, a vector database empowers you to discover the most similar or related data by considering their semantic or contextual significance.

In other words, unlike other database types that require an exact term search, you can use a vector database to conduct similarity searches and retrieve data according to their vector distance or likeness.

For example, you can use a vector database to perform the following tasks:

Recommend TV shows to watch based on your current viewing habits.
Locate related products based on the first product's features and ratings when shopping online.
Because related vector data exists mathematically closer to each other within the database, search and data delivery times are faster. So rather than having to perform additional analysis techniques to retrieve related data, the trained model and vector database delivers relevant search results faster.

Popular Vector Databases

Database offerings and their features are changing concurrently with the exponentially fast speed of AI development. Before selecting a vector database, you'll want to review its applicability to your data and LLM. Next, check out these currently popular vector databases:

Chroma
Chroma is an open source embedding database with which you can perform the following tasks:

Store embeddings and their metadata
Embed documents and queries
Search embeddings


Pinecone
Pinecone provides long-term memory for high-performance AI applications. Pinecone emphasizes the following capabilities and features:

Runs as a fully managed service
Provides high scalability
Provides real-time data ingestion
Delivers low-latency search


Weaviate
Weaviate, an open-source vector database that stores data objects and vector embeddings from machine-learning models, is said to provide the following capabilities and features:

Provides efficient similarity searches
Scales to store and process billions of data objects
Runs the GraphQL API
Provides real-time updates

Recap

After completing this reading, you know that:

Vector databases store, manage, and index massive quantities of high-dimensional vector data efficiently.
You can transform text, images, audio, and video into vector data.
Vector data consists of a series of numbers known as dimensions.
Vector databases store Items with similar or "like" vector numbers closely together within the database
Chroma, Pinecone, and Weaviate are three popular vector databases

mongodb: see mongpdb-1 mongodb-2 mongodb-3 mongodb-4 mongodb-5 mongodb-6 mongodb-7 mongodb-8 mongodb-9 

advantages of using mongodb: mongodb-10 mongodb-11 mongodb-12 mongodb-13 mongodb-14 mongodb-15 mongodb-16

use cases for mongo-db: mongodb-17 mongodb-18 mongodb-19 mongodb-20 mongodb-21 mongodb-22

see Getting Started with MongoDB

Glossary: Basics of MongoDB

Aggregation pipeline: The aggregation pipeline in MongoDB allows for data transformation and processing using a series of stages, including filtering, grouping, sorting, and projecting. The aggregation pipeline is a powerful tool for expressive data manipulation.

Code-first: Code-first refers to a development approach where developers create the application code first and let the code define the database schema. In MongoDB, this means that the schema is flexible and adapts to evolving application needs.

Collection: In MongoDB, a collection is a group of MongoDB documents. Collections are analogous to tables in a relational database and store related data documents in a schema-free, JSON-like format.

Expressive querying: Expressive querying refers to the ability to write complex and flexible queries that address data retrieval and manipulation needs, often facilitated by MongoDB's query language and aggregation framework.

High availability (HA): High availability (HA) in MongoDB refers to the ability of the database system to maintain near-continuous operation and data accessibility, even in the face of hardware failures or other issues. High availability is often achieved through features like replication and failover.

JSON: JSON is an acronym for JavaScript Object Notation, a lightweight data-interchange format used in NoSQL databases and other data systems. JSON is human-readable and easy for machines to parse.

MQL: MongoDB Query Language is a query language specific to MongoDB used to retrieve and manipulate data in the database.

Operational data: Operational data in MongoDB refers to the data that the application actively uses and manipulates, as opposed to historical or archived data.

Unstructured data: Unstructured data in MongoDB is data that does not adhere to a fixed schema. MongoDB allows for flexible and unstructured data storage, making MongoDB suitable for semi-structured or rapidly changing data.

CRUD operations: the mongo shell is an interactive JavaScript interface and you can use it to perform data and administrative operations on MongoDB. mongodb-23 mongodb-24 mongodb-25. Mongo shell is a JavaScript interpreter, meaning you can define variables and perform other functions in it too. mongodb-26 mongodb-27 (This will return the first document in natural order, which is the order in which the database refers to documents on disk.) mongodb-28 (In this second read operation, we want to find the first student with a specific email address.) mongodb-29 (retrieve all students with the last name of Doe.) mongodb-30 (count how many students have the last name Doe) mongodb-31 mongodb-32 (a changes document which in our case only has two assignments which come under $set, then we would call the updateOne function. To update all of the students to online only due to lockdown, we can run updateMany function without any filter criteria and a change document.) mongodb-33

see MongoDB CRUD

indexes: mongodb-34 mongodb-35 mongodb-36 In the Campus Management Database, for the course enrollment collection, we find students using ‘courseId’. Instead of scanning the whole collection, we create an index on the field ‘courseId’ in the course enrollment collection. ‘courseId : 1’ means store the index in ascending order. Our index on ‘courseId’ will help find those documents more efficiently. But to organize the students by ‘studentId’ in ascending order, MongoDB will need to perform in-memory sort, which will not be efficient. In this situation, we can change our index to be a compound index, which means indexing more than one field. Because the items in the index are organized in ascending order, once we find all documents with a matching ‘courseId’ of ‘1547’, they will all be already sorted by ‘studentId’ because of this index. mongodb-37. MongoDB stores indexes in a tree form: a balanced tree to be precise. Imagine our last example: ‘courseId,’ being the first field in our compound index, is in ascending order. Under each tree node, you will have students in ascending order too. This makes finding documents much more efficient, whether it’s an equality or a range search. And if you are sorting on a field which is already indexed, MongoDB doesn’t need to sort it again. mongodb-38

see MongoDb indexing

aggregation framework: also sometimes referred to as an aggregation pipeline, is a series of operations that you apply on your data to obtain a sought after outcome. For example, to understand whether students are really developing their knowledge, you want to see the average student scores in 2020 organized by courseid. To obtain this information, you will need to filter the 2020 documents, then group those documents by course, then calculate the average score i.e. see agg-framework-1 agg-framework-2 agg-framework-3 
agg-framework-4 agg-framework-5 agg-framework-6 agg-framework-7 agg-framework-8 agg-framework-9 agg-framework-10

see MongoDB Aggregation

replication:  A typical MongoDB cluster is made of three data-bearing nodes. All three nodes have the same data, hence the name Replica Set. Data is written to the primary node which then gets replicated to the secondary nodes see mongodb-replication-1. Having multiple copies of the data means that replication creates redundancy. So if one piece of server hardware fails, you still have multiple copies of the data. This capability provides you with a highly available database during such failures or plan maintenance periods. Such planned maintenance should be done in a rolling fashion, taking one node out for operating system, security, hardware, or software updates, or for upgrading MongoDB itself. A common misconception about replication is that replication can save you from disasters, such as accidentally deleting your database. Because the database is a replica set, what happens on the primary node is replicated on the secondary nodes. For disaster recovery scenarios, you'll rely on backups and restoration processes mongodb-replication-2.

Here's an overview of the MongoDB replication process. The application writes changes to the primary node. The primary node records those changes including timestamps in its operations log known as the Oplog. All of the other data-bearing nodes, the secondary nodes, observe the primary node Oplog for new changes. The secondary nodes copy and apply the new changes by first recording those changes in their respective Oplogs with the time stamp that indicates when they receive the change and reference the primary Oplog timestamps to record only the most recent changes mongodb-replication-3.

see mongodb-replication-4 mongodb-replication-5 

sharding: After using MongoDB for some time, you will probably outgrow your hardware. For example, if you need to store even more data or you want to improve your read and write performance, naturally, you will invest in bigger and faster hardware to increase capacity, but sometimes that is not feasible. In those situations, you can scale horizontally by implementing sharding, which is the partitioning of your biggest collections mongodb-sharding-1. Sharding your biggest or most demanding collections has many benefits. When partitioning your data across shards, you increase your throughput by directing your queries only to relevant shards. You can also store more data which previously you couldn't fit on a single node. You can also split data across shards based on regions. Data for US customers will only live on shards running in the US, while European customers have their data on European base shards. As a global application, you get one view of your database to manage mongodb-sharding-2. The primary node is the only node that can accept write operations including inserts, updates, and deletes from client applications mongodb-sharding-3. An election is the process of selecting a new primary node. As MongoDB is a highly available system, MongoDB can automatically elect one of the secondary nodes as the new primary node. The election of a new primary node happens when; the current primary node becomes unavailable, a new replica set is initialized and needs to choose its initial primary node, or when a database administrator initiates a manual failover for maintenance or upgrades mongodb-sharding-4. MongoDB's voting system ensures only one member becomes a primary, so to speak, there is no split brain. Database replica members who are eligible and have the most recent data and minimal replication lag are eligible to become the primary node. The choice of which node becomes the primary node is based on which node member receives the most votes.

accessing mongodb from python: see mongodb-python-1 The MongoClient is a class that helps you interact with MongoDB. First, we import MongoClient from pymongo, which is an official MongoDB driver for Python. The ‘uri’ tells us the address where MongoDB is. Then we create MongoClient, which we should only do once in our code. The next part gets the object pointing to the campus management database, And finally, from the campus management database object, we point to the ‘students’ collection. mongodb-python-2 Let’s create a student document. For this, we call the ‘insert_one’ function, which takes the document we are trying to insert as its input. mongodb-python-3 mongodb-python-4 mongodb-python-5 When we do a ‘find’ on a collection in MongoDB, we get a cursor back. This cursor points to our documents in MongoDB. If you have run this using mongo shell, the shell retrieves those documents for you behind the scenes. To retrieve documents from our cursor, we call python ‘dumps’, which will exhaust our cursor to retrieve all the documents. mongodb-python-6 mongodb-python-7 In the Replace example, we were sending the whole document back with the new changes. For larger documents, this will take a lot of transfer time between the client and the database. We can use MongoDB’s in-place updates for small changes. We do this by constructing a change document, which in our case only has two assignments, which come under ‘$set’, And with this in-place update, we don’t even need to retrieve the student document. To update all of the students to online only due to lockdown, we can run the ‘update_many’ function without any filter criteria, and change all documents. mongodb-python-8

see Accessing MongoDB using Python

Best Practices of MongoDB

In this reading, you'll learn about the definition of MongoDB and describe the best practices for MongoDB.

What is MongoDB?
MongoDB is an open source document-oriented database designed to store large data sets for industries. It enables efficient work with data and falls under not only structured query language (NoSQL) database because data storage and retrieval do not form tables.

MongoDB. Inc., under a server-side public license (SSPL), manages and develops the MongoDB database. It provides driver support for various programming languages, such as C, C++, C #, .Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid.

Best practices: MongoDB
MongoDB offers flexibility and scalability to harness its full potential. Let's understand the best practices for optimizing performance and designing efficient schemas in MongoDB.

Data Modelling
MongoDB offers a flexible document-based model for representing complex, hierarchical data structures. Therefore, it is important to thoroughly understand the application's data requirements to design a MongoDB schema. However, various factors such as data relationships, read patterns, and scalability are also important.

Indexing
Indexing the fields that are frequently queried or used for sorting helps to improve query execution times. However, ensuring that the queries support appropriate indexes and avoid creating extra indexes is vital. This helps to impact the right performance and save storage space. Also, keep in mind that indexing is a continuous process; however, queries need to be monitored and updated based on the business requirements.

Aggregation framework
The MongoDB aggregation framework provides powerful tools for performing data transformations, analytics, and computations within the database. Instead of fetching large datasets and processing them in application code, the aggregation pipeline can be used to perform complex operations directly within MongoDB. However, you should familiarize yourself with aggregation operators and stages, such as $match, $group, $project, and $lookup, to manipulate and aggregate data efficiently.

Scaling horizontally
Scaling horizontally is one of the key points of MongoDB. Sharding helps distribute data across servers or shards to handle large volumes of data and high throughput workloads. However, consider the scalability requirements and plans for early sharding when designing MongoDB deployment. Further, select an appropriate shard key to distribute data equally across the shards by regularly monitoring their distribution and performance based on application growth.
Preventing unauthorized access

Databases often contain valuable and sensitive information, such as personal data, financial records, intellectual property, and proprietary business information. To achieve this, follow security best practices such as enabling authentication, using role-based access control (RBAC) to define user permissions, and configuring network encryption with transport layer security/secure sockets layer (TLS/SSL). Regularly update MongoDB to the latest version to patch security vulnerabilities and maintain a robust security posture.

Summary
In this reading, you've learned about what MongoDB is. MongoDB is an open source document-oriented database designed to store large data sets for industries. It supports various programming languages such as C, C++, C#, .Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid.

To use the full potential of MongoDB, one should follow several best practices, such as:

Data Modelling
Indexing
Aggregation framework
Scaling horizontally
Preventing unauthorized access