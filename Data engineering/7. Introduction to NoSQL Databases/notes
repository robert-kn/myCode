what is NoSQL? see nosql-1.png nosql-2 nosql-3 nosql-4 

Let's take a look at the history of the NoSQL movement going back to the period between 1970 and 2000. Although there were some 
nonrelational databases like IBMs IMS, which was a hierarchical database used for the Apollo space missions, the market was dominated 
by relational databases. So when application architects and developers needed a data store for their applications, they were pretty 
much picking from a variety of common relational databases.

When Internet applications and companies started exploding during the dot-com boom in the late nineties and early 2000s, applications went from needing to serve thousands of internal employees at companies to needing to serve millions of users on the public Internet. For these applications, availability and performance were paramount. And these new scale problems led to a drive to create new scalable technologies to support them. During that time, several large tech companies, including IBM, Google and Meta, developed a lot of innovative technology, released white papers and open sourced their technology.

In the last ten years or so, several NoSQL databases have leveraged a fully managed service model, otherwise called database as a service or DBaaS. To offload the administration and maintenance from the end user and allow developers to focus on building applications with these modern databases.

why NoSQL? see nosql-5 nosql-6

what are the characteristics of NoSQL databases? see nosql-7 nosql-8 nosql-9 nosql-10 nosql-11

benefits of NoSQL databases? see nosql-12 nosql-13 nosql-14 nosql-15 Historically, large databases have run on expensive machines or mainframes. Modern enterprises are employing cloud architectures to support their applications, and the distributed data nature of NoSQL databases means that they can be deployed and operated on clusters of servers in cloud architectures, thereby massively reducing cost nosql-16. Cost is important for any technology venture, and it is common to hear of NoSQL adopters cutting significant costs versus their existing databases... and still be able to get the same or better performance and functionality nosql-17. nosql-18
nosql-19

Document store databases

Document-store databases, also known as document-oriented databases, store data in a document format, typically JSON or BSON (binary JSON), where each document contains key-value pairs or key-document pairs. These databases are schema-less, allowing flexibility in data structures within a collection.

Characteristics
1. Provides schema flexibility: Documents within collections can have varying structures, allowing for easy updates and accommodation of evolving data requirements.
2. Performs efficient create, read, update, and delete (CRUD) operations: well-suited for read and write-intensive applications due to their ability to retrieve whole documents.
3. Provides scalability: horizontal scalability by sharding data across clusters.

Use cases
1. Content management systems (CMS): CMS platforms like WordPress use document store databases for fast storage and access to content types such as articles, images, and user data. (MongoDB)
2. E-commerce: E-commerce platforms need effective management of product catalogs with diverse attributes and hierarchies, accommodating the dynamic nature of e-commerce product listings. (Couchbase or Amazon DocumentDB, using MongoDB compatibility)

Frequently mentioned vendors
MongoDB
Couchbase
Amazon DocumentDB

Key-value stores

Key-value stores are the simplest NoSQL databases, storing data as a collection of key-value pairs, where the key is unique and directly points to its associated value.

Characteristics

Delivers high performance: efficient for read and write operations, optimized for speedy retrieval based on keys
Provides scalability: easily scalable due to their simple structure and ability to distribute data across nodes
Uses caching for fast access
Provides session management
Works with distributed systems

Use cases:
1. Enhanced web performance by caching frequently accessed data (Using Redis or Memcached)
2. E-commerce platforms, software applications, including gaming: Amazon DynamoDB provides a highly scalable key-value store, facilitating distributed systems' seamless operation by handling high traffic and scaling dynamically.

Frequently mentioned vendors
Redis
Memcached
Amazon DynamoDB

Column-family stores

Definition: Column-family stores NoSQL databases, also referred to as columnar databases, organize data in columns rather than rows. These databases store columns of data together, making them efficient for handling large data sets with dynamic schemas.

Characteristics
Uses column-oriented storage: Data is grouped by columns rather than rows, allowing for efficient retrieval of specific columns.
Delivers scalability: Distributed architecture for high availability and scalability.

Use cases
IoT applications manage massive amounts of sensor data efficiently due to their ability to handle time-stamped data at scale, referred to as time-series data analysis. (Apache Cassandra)
Applications that store and analyze user preferences and behaviors usually deliver personalization. (HBase, part of the Hadoop ecosystem)
Large-scale data analysis.

Frequently mentioned vendors
Apache Cassandra
HBase

Graph databases:

Definition: Graph NoSQL databases are designed to manage highly interconnected data, representing relationships as first-class citizens alongside nodes and properties.

Characteristics:
Analyzes the data using a graph data model: relationships are as important as the data itself, enabling efficient traversal and querying of complex relationships.
Fast performance for relationship queries: optimized for queries involving relationships, making them ideal for social networks, recommendation systems, and network analysis.

Use cases:
Social networks require efficient data management of relationships between users, posts, comments, and likes. (Neo4j)
Recommendation systems: Organizations need a database structure that can create sophisticated recommendation engines, analyzing complex relationships between users, products, and behaviors for precise recommendations. (Amazon Neptune)

Frequently mentioned vendors
Neo4j
Amazon Neptune
ArangoDB Memcached

Wide-column stores:

Wide-column store NoSQL databases organize data in tables, rows, and columns, like relational databases, but with a flexible schema.

Characteristics:
Use columnar storage: Data is stored in columns, allowing for efficient retrieval of specific columns rather than entire rows.
Provide horizontal scalability and fault tolerance.

Use cases:
Analyzing big data: Efficiently handling large-scale data processing for real-time big data analytics. (Apache HBase used in conjunction with Hadoop)
Managing enterprise content: Large organizations databases need to manage vast amounts of structured data like employee records or inventory due. (Cassandra)

Frequently mentioned vendors
Apache HBase
Apache Cassandra

Expanded use case example: Using MongoDB for a content management system (CMS)

Content management systems (CMS) intelligently collect, govern, manage, and enrich enterprise content, including HTML pages, images, articles, and more. Content management systems help companies deploy their content efficiently and securely across any cloud and within any application.

Good content management means that team members can quickly add, update, and remove content from the database and the associated pages that feature that content. Examples include pushing out breaking news, updating current news, including weather forecasts, pushing advertising content, updating college admission policies, launching new city services, and more.

For example, using MongoDB as a backend database for a content management system (CMS) is a practical choice when you need to manage and serve a variety of content types, especially in scenarios where you expect frequent schema changes or scaling requirements.

Next, let's check out some of the aspects of managing content using a content management system, specifically using MongoDB.

Content structure using MongoDB

In MongoDB, you represent content as documents. Each document corresponds to a piece of content, such as an article, image, video, or page. You can use the subdocuments in the document to organize the content hierarchy and structure.

Example of structuring: Storing a blog post
When storing a blog post, you will store core attributes like title, content, created at, and the image URL. Then, using an array field, you can store tags. The comments on that post are stored as an array of objects.

// Collection: posts
{
"\_id":1,
"title":"Sample Blog Post",
"content":"This is the content of the blog post...",
"author":{
"name":"John Doe",
"email":"john@example.com",
"bio":"A passionate blogger.",
"created\_at":"2023-09-20T00:00:00Z"
},
"created\_at":"2023-09-20T08:00:00Z",
"tags":["mongodb","blogging","example"],
"comments":[
{
"text":"Great post!",
"author":"Emily Johnson",
"created\_at":"2023-09-20T10:00:00Z"
},
{
"text":"Thanks for sharing!",
"author":"James Martin",
"created\_at":"2023-09-20T11:00:00Z"
}
]
}

Metadata and indexing using MongoDB

You can use the indexing capabilities of MongoDB to optimize content retrieval. You can create indexes on fields commonly used for filtering or searching, such as keywords, publication date, or content type, or use MongoDB's text index support for text search queries on fields containing string content. Text indexes improve performance when searching for specific words or phrases within string content.

For example, you want to provide searching capability on the content of your blogs. You will first create a text index:

db.articles.createIndex( { subject: "text" } )

And then you can provide a query such as:

db.posts.find( { $text: { $search: "digital life" } } )

where MongoDB will look for stemmed versions of these words: digital or life

Scaling your CMS using MongoDB

As your CMS grows, MongoDB can help you scale. You can use sharding for horizontal scaling or use zone-based sharding for global distribution.

Using sharding for horizontal scaling (increased capacity)
Let's consider a company that currently has 100 million customers. This company expects to expand its customer base to 200 million customers. This increase in the number of customers means that the company will need to double its IT data storage hardware. The company can scale vertically, which can cost exponentially more as the hardware cost isn't linear with the performance. The following diagram shows that the company can scale horizontally and use sharding to manage the databases. see sharding.png

key value NoSQL databases: see key-value-1.png key-value-2 key-value-3 key-value-4 key-value-5

document based NoSQL databases: see document-1 document-2 document-3 document-4 document-5 

column based nosql databases: see column-db-1.png column-db-2. suitable use cases; are great for when you're dealing with large amounts of sparse data. When compared to row-oriented databases, column-based databases can better compress data and save storage space. In addition, these databases continue the trend of horizontal scalability. As with key value and document databases, column-based databases can handle being deployed across clusters of nodes. Like document databases, a column-based NoSQL database can be used for event logging and blogs, but the data would be stored in a different fashion. For enterprise logging, every application can write to its own set of columns and have each row key formatted in such a way to promote easy lookup based on application and timestamp. Counters are a unique use case for column-based databases. You may come across applications that need an easy way to count or increment as events occur. Some column-based databases, like Cassandra have special column types that allow for simple counters. In addition, columns can have a time-to-live parameter, making them useful for data with an expiration date or time like trial periods or ad timing. see column-db-3 column-db-4

graph nosql databases: see graph-db-1 graph-db-2 graph-db-3 graph-db-4 graph-db-5

NoSQL Database Deployment Options

Database deployment options refer to the various methods and strategies for implementing and managing databases within an organization. Choosing the right deployment option is crucial for optimizing performance, scalability, and efficiency. Here, you'll explore some common database deployment options and their key characteristics

What you will learn

After completing this reading, you'll be able to

Identify methods for deploying NoSQL databases
List examples of NoSQL database deployments
Describe some of the advantages and challenges associated with deploying NoSQL databases

On-premises deployment
On-premises deployment involves hosting the entire database infrastructure within the organization's physical location or a dedicated data center.

Example
An organization manages customer relationship data using an on-premises Oracle Database installed on dedicated servers within its corporate data center.

Advantages
Full control: Organizations have complete control over hardware specifications, software configurations, and security measures.

Compliance: On-premises solutions might be necessary for industries with stringent regulatory or compliance requirements.

Challenges
Upfront costs: High initial investments in hardware, infrastructure, and skilled personnel.

Scalability: Limited scalability compared to cloud alternatives, which might result in challenges during periods of rapid growth.

Cloud deployment
Cloud deployment involves utilizing cloud service providers to host and manage databases over the internet.

Example
A startup leverages Amazon Web Services (AWS) to deploy and host its e-commerce database, utilizing Amazon relational database service (RDS) for scalability and managed database services.

Advantages
Scalability: Resources can be scaled up or down based on demand, providing flexibility and cost savings.

Cost-effectiveness: Pay-as-you-go pricing allows organizations to pay only for the resources they consume.

Automation: Cloud providers handle routine maintenance tasks, updates, and backups.

Challenges
Connectivity dependencies: Relies on internet connectivity, which might pose issues in areas with limited or unreliable access.

Security concerns: Requires robust security measures to protect sensitive data from unauthorized access.

Hybrid deployment
Hybrid deployment combines on-premises and cloud solutions, allowing organizations to distribute data and workloads based on specific needs.

Example
A financial institution stores sensitive customer information, such as account balances, in an on-premises database while utilizing a cloud-based service, like AWS Lambda, for processing non-sensitive data analytics workloads.

Advantages
Flexibility: Companies can keep sensitive data on-premises while using the cloud for scalable and dynamic workloads.

Disaster recovery: Companies can use cloud storage for backups, enhancing disaster recovery capabilities.

Challenges
Integration complexity: Integration complexities require effective integration between on-premises and cloud environments for seamless operation.

Management overhead: Monitoring and managing two different environments might increase administrative complexity.

Database as a service (DBaaS)
DBaaS provides a fully managed database solution where the service provider handles administrative tasks, allowing organizations to focus on application development.

Example
A software development company uses Microsoft Azure SQL Database as a DBaaS solution to host and manage its relational databases, allowing developers to focus on application development rather than database administration tasks.

Advantages
Reduced overhead: Organizations benefit from reduced administrative tasks, including maintenance, backups, and updates.

Rapid deployment: Quick deployment without the need for in-depth database expertise.

Challenges
Limited control: Organizations have less control over underlying infrastructure, which might be a concern for some organizations.

Data security: Trusting a third-party provider with sensitive data raises security and privacy considerations.

Containerized deployment
Containerization involves packaging the database and its dependencies into containers for consistent deployment across various environments.

Example
A tech company adopts Docker containers to deploy its microservices-based application, with each microservice encapsulating a specific function and a separate container running a MongoDB database.

Advantages
Portability: Containers ensure consistent operation across development, testing, and production environments.

Resource efficiency: Lightweight containers consume fewer resources compared to traditional virtual machines.

Challenges
Orchestration complexity: Production-scale deployment requires knowledge of container orchestration tools like Kubernetes.

Learning curve: Teams might need to familiarize themselves with containerization concepts and tools.

Serverless deployment
Serverless architecture provisions and scales database resources automatically based on demand, with organizations paying for actual usage.

Example
A mobile app developer utilizes Google Cloud Firestore as a serverless NoSQL database for storing user data, where the database scales automatically based on demand, and the developer only pays for the data storage and operations used by the app.

Advantages
No manual provisioning: No need for manual provisioning or maintenance of server resources.

Cost savings: Efficient resource utilization leads to cost savings as organizations only pay for the resources consumed.

Challenges
Limited control: Organizations have less control over underlying infrastructure, which might concern some organizations.

Applicability: Serverless might not be suitable for all types of databases or workloads due to architectural constraints.

Recap
In this reading, you learned that:

The choice of a database deployment option depends on organizational priorities, budget constraints, scalability needs, and how the company manages the nature of the data. It's essential to carefully evaluate these options in alignment with specific use cases to ensure optimal performance and efficiency.

On-premises deployment involves hosting the entire database infrastructure within the organization's physical location or a dedicated data center, providing full control and compliance with regulatory requirements but with high upfront costs and limited scalability.

Cloud deployment uses cloud service providers to host and manage databases. Cloud deployment offers scalability, cost-effectiveness, and automation. However, cloud deployments have connectivity dependencies and can be prone to security concerns.

Hybrid deployment combines on-premises and cloud solutions, allowing organizations to distribute data and workloads based on specific needs, providing flexibility and enhanced disaster recovery capabilities; however, effective integration between on-premises and cloud environments requires increased administrative complexity and costs.

Database as a Service (DBaaS) provides a fully managed database solution where the company's choice service provider handles administrative tasks. When using DBaaS, organizations focus on application development but usually have limited control over underlying infrastructure and data security.

Containerized deployment involves packaging the database and its dependencies into containers for consistent deployment across various environments, which can ensure consistent operations and resource efficiency—however, if containerization practices are not already in place, the company will need time for employees to gain knowledge of container orchestration tools and implement them.

Serverless deployment provisions and scales database resources automatically based on demand, with organizations paying for actual usage, eliminating manual provisioning and maintenance of server resources, and leading to cost savings. However, companies usually have limited control over the underlying infrastructure, and serverless deployments might not be technically suitable for all databases or workloads.

ACID versus BASE: see acid-base-1.png. The ACID acronym stands for atomic. All operations in a transaction succeed, or every operation is rolled back. Consistent. On the completion of a transaction, the structural integrity of the data in the database is not compromised. Isolated. Transactions cannot compromise the integrity of other transactions by interacting with them while they are still in progress. Durable. The data related to the completed transaction will persist, even in the case of network or power outages. If a transaction fails, it will not impact the already changed data. acid-base-2 acid-base-3. The BASE acronym stands for basically available. Rather than enforcing immediate consistency BASE modeled NoSQL databases will ensure the availability of data by spreading and replicating it across the nodes of the database cluster. Soft state. Due to the lack of immediate consistency, data values may change over time. In the BASE model, data stores don't have to be right consistent nor do different replicas have to be mutually consistent all the time. Eventually consistent. The fact that the BASE model does not enforce immediate consistency does not mean that it never achieves it. However, until it does, data reads might be inconsistent.

Distributed databases: 

A distributed database is a collection of multiple interconnected databases, which are spread physically across various locations that communicate via a computer network. A distributed database is physically distributed across the data sites by fragmenting and replicating the data. And a distributed database follows the base consistency model. see distributed-1.png distributed-2
distributed-3 distributed-4 distributed-5

MongoDB Design Patterns

MongoDB design patterns help optimize data models based on application queries and usage. However, MongoDB design patterns improve application performance by reducing schema complexity. It helps identify data storage patterns and the type of data returned to the application.

However, MongoDB design patterns are best practices for structuring data and queries to optimize performance, scalability, and maintainability. These patterns arise from common use cases and challenges encountered. Some prominent MongoDB design patterns include approximation, attribute, polymorphic, outliers, and bucket. Let's read about each of these MongoDB design patterns.

Approximation

Approximation MongoDB design pattern helps calculate frequent expenses, where the precision for those calculations is not a priority. For example, Instagram followers are for celebrities.

The approximation design pattern is useful for fewer writes to the database and maintains statistically valid numbers. However, it cannot represent exact numbers

Attribute

The Attribute MongoDB design pattern helps document various characteristics and similar fields that you cannot identify when raising a query. This attribute is useful when the fields need to be sorted and found in a small subset of documents or when both conditions meet within the documents, such as when selling products on e-commerce.

The attribute MongoDB design pattern is useful in less indexing and generating simple and faster queries.

Polymorphic

The polymorphic MongoDB design pattern helps store different documents in the same collection. However, it often uses a discriminator field to differentiate between types of documents, such as customer communication stored using various channels.

This type of design pattern is easy to implement and runs queries across a single collection.

Outlier

The Outlier MongoDB design pattern is useful for queries or documents that don't fit into the typical data pattern, such as a list of a celebrity's social media followers (which could be in millions). This design pattern prevents documents or queries from determining an application's solution; however, it is tailored for typical use cases but doesn't address ad hoc queries.

Bucket Pattern

The bucket pattern in MongoDB design helps to manage streaming data such as time series, real-time analytics, or Internet of Things (IoT) applications. The bucket MongoDB design pattern reduces the number of documentation while collecting them, improves index performance, and simplifies data access by leveraging pre-aggregation.

For example: Collecting weather data using multiple sensors. This data collection helps to reduce efforts to review temperature and wind speed every minute. However, you would receive a summary per hour, such as:

Average temperature
Average wind speed
However, you can review the weather forecast in detail. For example, find the temperature per minute and calculate the median and variance. The benefit of the bucket design pattern is that it doesn't repeat device identifiers and entry data with every record. However, if this didn't happen, there would be 1440 records per device per hour, creating a large amount of data.

Summary

In this reading, you've learned that MongoDB design patterns optimize the data model to access the application's patterns. Such as:

Approximation
Attribute
Polymorphic
Outliers
Bucket

CAP theorem: see cap-1 cap-2 cap-3 cap-4 In a cluster with eight distributed nodes, a network partition could occur, and communication will be broken between all the nodes. In our case, instead of one 8-node cluster we will have two smaller 4-node clusters available. Consistency between the two clusters will be achieved when network communication is re-established. Partition tolerance has become more of a necessity than an option in distributed systems. It is made possible by sufficiently replicating records across combinations of nodes and networks. For such systems as NoSQL, since partition tolerance is mandatory, a system can be either Consistent and Partition Tolerant (CP) or Available and Partition Tolerant (AP). Existing NoSQL systems, like MongoDB or Cassandra, can be classified using CAP Theorem. For example, MongoDB chooses consistency as the primary design driver of the solution, and Apache Cassandra chooses availability. This doesn't mean that MongoDB cannot be available, or that Cassandra cannot become fully consistent. It means that these solutions first ensure that they are consistent (in the case of MongoDB) or available (in the case of Cassandra) and the rest is tunable. see cap-5

challenges in migrating from RDBMS to NoSQL databases: see rdbms-to-nosql-1 rdbms-to-nosql-2 rdbms-to-nosql-3 In NoSQL models should be based on how the app interacts with the data rather than how the model can be stored as rows in one or more tables. Another factor to pay attention to is the fact that in RDBMS, data is normalized while in NoSQL it is denormalized. With NoSQL, starting from your query means that you will structure your data on disk accordingly. Thus, you may need to store the same data in different models just to answer the question, and this will lead to data denormalization. While data in RDBMS is normalized, you need to be open to the situation in which you start with your queries instead of your data. rdbms-to-nosql-4. When migrating from relational to NoSQL databases, you need to understand that sometimes services require availability more than consistency when both availability and performance are needed. Thus, distributed systems consistency cannot be ensured. Remember cap theorem, many of today's online services value availability more than consistency. And because of this they look for systems that can provide it, taking into consideration the amount of data they are dealing with and their geographical presence. One last thing to know when dealing with NoSQL databases, these are not designed to support transactions or joins or complex processing except in limited cases. You need to consider this when moving from RDBMS to NoSQL.

Data Modeling in Relational and Document-based Databases

After completing this reading, you will be able to:

Compare modeling the same data in relational and document databases.
Describe the complexities of querying data.
Describe how data duplication affects data modeling
Describe the considerations associated with evolving the schema.

Introduction

First, compare the information organization in a relational database to a NoSQL document database.

A relational database organizes data into tables with predefined schemas and relationships among tables. In a relational model, you avoid duplicating any data so that the data is stored once and updated in one location. Wherever and whenever the data is needed, the data is referenced. That 's the relational aspect of the data.

On the other hand, an excellent document-based design starts with using that data.

Let's use the example of library books and data organization.

Example: A library book catalog

Library book catalogs store information about books and their authors.

Using a relational database for a library books catalog
If you work with library book data using a relational database, you will create the Books, Authors, Genres and BookGenres tables:

see books.png authors.png genres.png book-genres.png

Using a document database for a library books catalog

In contrast, a document database is simplified and displayed as a single document with all the required information.

{“_id”: 1,”title”: “The Great Gatsby”,”author”: “F. Scott Fitzgerald,””isbn”: “978-0743273565”,”published_year”: 1925,”genres”: [“Fiction”, “Classic”]}

Next, compare the complexities of querying the data in relational and document databases.

Complexities of querying data

Let's examine the complexities of querying the same book data using a relational database compared to a NoSQL database.

Querying book data using a relational database

Now, query the data. You will need all the book details at once. You will structure your SQL query by performing a JOIN among those tables. Here's your SQL query example:

SELECT Books.Title AS BookTitle, Authors.Name AS Author, GROUP_CONCAT(Genres.GenreName) AS Genres FROM Books JOIN Authors ON Books.AuthorID = Authors.AuthorID JOIN BookGenres ON Books.BookID = BookGenres.BookID JOIN Genres ON BookGenres.GenreID = Genres.GenreID GROUP BY Books.Title, Authors.Name;

The number of joins, use of the GROUP_CONCAT function, and the GROUP BY function make this query complex. Here's how the query breaks down.

The SELECT command obtains the book title, author name, and a concatenated list of genres for each book.
The JOIN command joins the Books table with the Authors table on the AuthorID field to get the author’s name.
The JOIN command joins the Books table with the BookGenres table on the BookID field to associate books with genres.
The JOIN command joins the BookGenres table with the Genres table on the GenreID field to get genre names.
The query uses the GROUP_CONCAT function (the exact function may vary depending on your SQL database system; GROUP_CONCAT is used in MySQL and STRING_AGG in PostgreSQL) to concatenate multiple genre names into a single string.
The query uses GROUP BY to display the results by book title and author name.

Here is an example output: example-output.png

Querying book data using a NoSQL document database

You already know the output you need and that the output is a first-order criterion in a document database. Remember that your book document looks like this:

{“_id”: 1,”title”: “The Great Gatsby”,”author”: “F. Scott Fitzgerald,””isbn”: “978-0743273565”,”published_year”: 1925,”genres”: [“Fiction”, “Classic”]}

When creating a document database query, using MongoDB, for example, you specify empty brackets as the first argument denoting you want to see all documents. The second argument, called projection, allows you to choose which fields to present to the client information. You won 't always need to show all of the fields all of the time. And you'll notice the simplicity of this query, as it didn 't need any joins with other collections.

Here is your query:

Db.books.find({}, { title:1, author: 1, genres: 1 })

The data you need to fulfill the request is already in the document and does not require any joins with other collections.

Important! What about data duplication?

Data duplication is a common practice and is known as "denormalization." Data duplication can improve read performance by avoiding complex joins and queries. However, you will face challenges with data consistency and increased storage requirements.
For example, imagine that the author, J.R.R. Tolkien, wants to be known by his full name, John Ronald Reuel Tolkien, and you need to make this change in a relational database and a document database. How would you implement this change in these two different types of databases?

Relational database	
This request requires only one change in the Authors table in a relational database.	

Document database
Since the author has written 12 books in a document database, you must update this information in 12 documents, which is a small price to pay, as this is a rare event.

Next, explore the considerations associated with evolving a schema.

Schema Evolution Considerations

Changing the schema in a database can be necessary to accommodate evolving application requirements. However, a significant difference exists in how a relational and document database enables schema changes.

In relational databases, changing the schema typically involves modifying existing tables, adding new tables, or altering relationships between tables. Here's an example:

ALTER TABLE table_name ADD column_name data_type;

After making schema changes, you might need to migrate existing data to match the new schema, which can involve data transformation and migration scripts.

In document databases like MongoDB, the schema is typically more flexible, and you can often add or remove fields to documents (individual documents, as there is generally no collection-wide enforced schema) without a predefined structure. Here's an example:

Db.books.update ({ _id: 1 },{$set: {Newfield: “Some value”}});

Summary

In this reading, you learned that:

A relational database organizes data into tables with predefined schemas and relationships among tables.
A good document-based design starts with the usage of data.
The number of SELECT, JOIN, GROUP, and additional commands contribute to the complexity of building a relational database query.
For document database queries, you won 't always need to show all of the fields all of the time, and you'll notice the simplicity of these queries, as the queries might not need joins with other collections.
In relational databases, changing the schema typically involves modifying existing tables, adding new tables, or altering relationships between tables.
In document databases, such as MongoDB, the schema is typically more flexible, and you can often add or remove fields to documents without a predefined structure.

Vector Databases

Vector databases, a newer NoSQL database, are rapidly becoming popular with the exponential increase in the use of Large Language Models (LLMs), such as OpenAI's GPT. But what is a vector database? Here's how IBM defines a vector database:

A vector database is designed to store, manage, and index massive quantities of high-dimensional vector data efficiently.
Source: https://www.ibm.com/topics/vector-database

Vectors

You can transform text, images, audio, and video into vectors, also known as vector data, using embedding functions based on various methods, including machine learning models, word embeddings, and feature extraction algorithms. see vectors-1.png

For example, the vector representation of dog is [2.1,-0.3, 7.2, 9.6, 6.1]

Similar words for dogs include the word canine or K9, so a vector database will identify both terms and include the same vector values. see vectors-2.png 

Important! When words have relationships or similar contexts, but the meaning is not identical, these words have vectors that are closer together within the database that help identify the relationship.

For example, you'll see the word animal represented as [1.9, -0.4, 7.2, 8.0, 6.3]

Each numeric value you see displayed inside of a vector is one dimension. see vectors-3.png

Vectors can have a few or thousands of dimensions, depending on the granularity and complexity of the classification required.

Returning to the example of a dog classifications, we know a dog is an animal, but not all animals are dogs. The relevance and relationships between the words "dog" and "animal" mean that these words will be much closer together as vector values and within the database itself. The following illustration shows the one dimension, dimension 7.2, that the word dog and animal share.
see vectors-4.png 

Next, explore why companies are moving to vector databases.

Vector Database benefits
In contrast to conventional techniques that involve querying databases for exact matches or predefined criteria, a vector database empowers you to discover the most similar or related data by considering their semantic or contextual significance.

In other words, unlike other database types that require an exact term search, you can use a vector database to conduct similarity searches and retrieve data according to their vector distance or likeness.

For example, you can use a vector database to perform the following tasks:

Recommend TV shows to watch based on your current viewing habits.
Locate related products based on the first product's features and ratings when shopping online.
Because related vector data exists mathematically closer to each other within the database, search and data delivery times are faster. So rather than having to perform additional analysis techniques to retrieve related data, the trained model and vector database delivers relevant search results faster.

Popular Vector Databases

Database offerings and their features are changing concurrently with the exponentially fast speed of AI development. Before selecting a vector database, you'll want to review its applicability to your data and LLM. Next, check out these currently popular vector databases:

Chroma
Chroma is an open source embedding database with which you can perform the following tasks:

Store embeddings and their metadata
Embed documents and queries
Search embeddings


Pinecone
Pinecone provides long-term memory for high-performance AI applications. Pinecone emphasizes the following capabilities and features:

Runs as a fully managed service
Provides high scalability
Provides real-time data ingestion
Delivers low-latency search


Weaviate
Weaviate, an open-source vector database that stores data objects and vector embeddings from machine-learning models, is said to provide the following capabilities and features:

Provides efficient similarity searches
Scales to store and process billions of data objects
Runs the GraphQL API
Provides real-time updates

Recap

After completing this reading, you know that:

Vector databases store, manage, and index massive quantities of high-dimensional vector data efficiently.
You can transform text, images, audio, and video into vector data.
Vector data consists of a series of numbers known as dimensions.
Vector databases store Items with similar or "like" vector numbers closely together within the database
Chroma, Pinecone, and Weaviate are three popular vector databases

mongodb: see mongpdb-1 mongodb-2 mongodb-3 mongodb-4 mongodb-5 mongodb-6 mongodb-7 mongodb-8 mongodb-9 

advantages of using mongodb: mongodb-10 mongodb-11 mongodb-12 mongodb-13 mongodb-14 mongodb-15 mongodb-16

use cases for mongo-db: mongodb-17 mongodb-18 mongodb-19 mongodb-20 mongodb-21 mongodb-22

see Getting Started with MongoDB

Glossary: Basics of MongoDB

Aggregation pipeline: The aggregation pipeline in MongoDB allows for data transformation and processing using a series of stages, including filtering, grouping, sorting, and projecting. The aggregation pipeline is a powerful tool for expressive data manipulation.

Code-first: Code-first refers to a development approach where developers create the application code first and let the code define the database schema. In MongoDB, this means that the schema is flexible and adapts to evolving application needs.

Collection: In MongoDB, a collection is a group of MongoDB documents. Collections are analogous to tables in a relational database and store related data documents in a schema-free, JSON-like format.

Expressive querying: Expressive querying refers to the ability to write complex and flexible queries that address data retrieval and manipulation needs, often facilitated by MongoDB's query language and aggregation framework.

High availability (HA): High availability (HA) in MongoDB refers to the ability of the database system to maintain near-continuous operation and data accessibility, even in the face of hardware failures or other issues. High availability is often achieved through features like replication and failover.

JSON: JSON is an acronym for JavaScript Object Notation, a lightweight data-interchange format used in NoSQL databases and other data systems. JSON is human-readable and easy for machines to parse.

MQL: MongoDB Query Language is a query language specific to MongoDB used to retrieve and manipulate data in the database.

Operational data: Operational data in MongoDB refers to the data that the application actively uses and manipulates, as opposed to historical or archived data.

Unstructured data: Unstructured data in MongoDB is data that does not adhere to a fixed schema. MongoDB allows for flexible and unstructured data storage, making MongoDB suitable for semi-structured or rapidly changing data.

CRUD operations: the mongo shell is an interactive JavaScript interface and you can use it to perform data and administrative operations on MongoDB. mongodb-23 mongodb-24 mongodb-25. Mongo shell is a JavaScript interpreter, meaning you can define variables and perform other functions in it too. mongodb-26 mongodb-27 (This will return the first document in natural order, which is the order in which the database refers to documents on disk.) mongodb-28 (In this second read operation, we want to find the first student with a specific email address.) mongodb-29 (retrieve all students with the last name of Doe.) mongodb-30 (count how many students have the last name Doe) mongodb-31 mongodb-32 (a changes document which in our case only has two assignments which come under $set, then we would call the updateOne function. To update all of the students to online only due to lockdown, we can run updateMany function without any filter criteria and a change document.) mongodb-33

see MongoDB CRUD

indexes: mongodb-34 mongodb-35 mongodb-36 In the Campus Management Database, for the course enrollment collection, we find students using ‘courseId’. Instead of scanning the whole collection, we create an index on the field ‘courseId’ in the course enrollment collection. ‘courseId : 1’ means store the index in ascending order. Our index on ‘courseId’ will help find those documents more efficiently. But to organize the students by ‘studentId’ in ascending order, MongoDB will need to perform in-memory sort, which will not be efficient. In this situation, we can change our index to be a compound index, which means indexing more than one field. Because the items in the index are organized in ascending order, once we find all documents with a matching ‘courseId’ of ‘1547’, they will all be already sorted by ‘studentId’ because of this index. mongodb-37. MongoDB stores indexes in a tree form: a balanced tree to be precise. Imagine our last example: ‘courseId,’ being the first field in our compound index, is in ascending order. Under each tree node, you will have students in ascending order too. This makes finding documents much more efficient, whether it’s an equality or a range search. And if you are sorting on a field which is already indexed, MongoDB doesn’t need to sort it again. mongodb-38

see MongoDb indexing

aggregation framework: also sometimes referred to as an aggregation pipeline, is a series of operations that you apply on your data to obtain a sought after outcome. For example, to understand whether students are really developing their knowledge, you want to see the average student scores in 2020 organized by courseid. To obtain this information, you will need to filter the 2020 documents, then group those documents by course, then calculate the average score i.e. see agg-framework-1 agg-framework-2 agg-framework-3 
agg-framework-4 agg-framework-5 agg-framework-6 agg-framework-7 agg-framework-8 agg-framework-9 agg-framework-10

see MongoDB Aggregation

replication:  A typical MongoDB cluster is made of three data-bearing nodes. All three nodes have the same data, hence the name Replica Set. Data is written to the primary node which then gets replicated to the secondary nodes see mongodb-replication-1. Having multiple copies of the data means that replication creates redundancy. So if one piece of server hardware fails, you still have multiple copies of the data. This capability provides you with a highly available database during such failures or plan maintenance periods. Such planned maintenance should be done in a rolling fashion, taking one node out for operating system, security, hardware, or software updates, or for upgrading MongoDB itself. A common misconception about replication is that replication can save you from disasters, such as accidentally deleting your database. Because the database is a replica set, what happens on the primary node is replicated on the secondary nodes. For disaster recovery scenarios, you'll rely on backups and restoration processes mongodb-replication-2.

Here's an overview of the MongoDB replication process. The application writes changes to the primary node. The primary node records those changes including timestamps in its operations log known as the Oplog. All of the other data-bearing nodes, the secondary nodes, observe the primary node Oplog for new changes. The secondary nodes copy and apply the new changes by first recording those changes in their respective Oplogs with the time stamp that indicates when they receive the change and reference the primary Oplog timestamps to record only the most recent changes mongodb-replication-3.

see mongodb-replication-4 mongodb-replication-5 

sharding: After using MongoDB for some time, you will probably outgrow your hardware. For example, if you need to store even more data or you want to improve your read and write performance, naturally, you will invest in bigger and faster hardware to increase capacity, but sometimes that is not feasible. In those situations, you can scale horizontally by implementing sharding, which is the partitioning of your biggest collections mongodb-sharding-1. Sharding your biggest or most demanding collections has many benefits. When partitioning your data across shards, you increase your throughput by directing your queries only to relevant shards. You can also store more data which previously you couldn't fit on a single node. You can also split data across shards based on regions. Data for US customers will only live on shards running in the US, while European customers have their data on European base shards. As a global application, you get one view of your database to manage mongodb-sharding-2. The primary node is the only node that can accept write operations including inserts, updates, and deletes from client applications mongodb-sharding-3. An election is the process of selecting a new primary node. As MongoDB is a highly available system, MongoDB can automatically elect one of the secondary nodes as the new primary node. The election of a new primary node happens when; the current primary node becomes unavailable, a new replica set is initialized and needs to choose its initial primary node, or when a database administrator initiates a manual failover for maintenance or upgrades mongodb-sharding-4. MongoDB's voting system ensures only one member becomes a primary, so to speak, there is no split brain. Database replica members who are eligible and have the most recent data and minimal replication lag are eligible to become the primary node. The choice of which node becomes the primary node is based on which node member receives the most votes.

accessing mongodb from python: see mongodb-python-1 The MongoClient is a class that helps you interact with MongoDB. First, we import MongoClient from pymongo, which is an official MongoDB driver for Python. The ‘uri’ tells us the address where MongoDB is. Then we create MongoClient, which we should only do once in our code. The next part gets the object pointing to the campus management database, And finally, from the campus management database object, we point to the ‘students’ collection. mongodb-python-2 Let’s create a student document. For this, we call the ‘insert_one’ function, which takes the document we are trying to insert as its input. mongodb-python-3 mongodb-python-4 mongodb-python-5 When we do a ‘find’ on a collection in MongoDB, we get a cursor back. This cursor points to our documents in MongoDB. If you have run this using mongo shell, the shell retrieves those documents for you behind the scenes. To retrieve documents from our cursor, we call python ‘dumps’, which will exhaust our cursor to retrieve all the documents. mongodb-python-6 mongodb-python-7 In the Replace example, we were sending the whole document back with the new changes. For larger documents, this will take a lot of transfer time between the client and the database. We can use MongoDB’s in-place updates for small changes. We do this by constructing a change document, which in our case only has two assignments, which come under ‘$set’, And with this in-place update, we don’t even need to retrieve the student document. To update all of the students to online only due to lockdown, we can run the ‘update_many’ function without any filter criteria, and change all documents. mongodb-python-8

see Accessing MongoDB using Python

Best Practices of MongoDB

In this reading, you'll learn about the definition of MongoDB and describe the best practices for MongoDB.

What is MongoDB?
MongoDB is an open source document-oriented database designed to store large data sets for industries. It enables efficient work with data and falls under not only structured query language (NoSQL) database because data storage and retrieval do not form tables.

MongoDB. Inc., under a server-side public license (SSPL), manages and develops the MongoDB database. It provides driver support for various programming languages, such as C, C++, C #, .Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid.

Best practices: MongoDB
MongoDB offers flexibility and scalability to harness its full potential. Let's understand the best practices for optimizing performance and designing efficient schemas in MongoDB.

Data Modelling
MongoDB offers a flexible document-based model for representing complex, hierarchical data structures. Therefore, it is important to thoroughly understand the application's data requirements to design a MongoDB schema. However, various factors such as data relationships, read patterns, and scalability are also important.

Indexing
Indexing the fields that are frequently queried or used for sorting helps to improve query execution times. However, ensuring that the queries support appropriate indexes and avoid creating extra indexes is vital. This helps to impact the right performance and save storage space. Also, keep in mind that indexing is a continuous process; however, queries need to be monitored and updated based on the business requirements.

Aggregation framework
The MongoDB aggregation framework provides powerful tools for performing data transformations, analytics, and computations within the database. Instead of fetching large datasets and processing them in application code, the aggregation pipeline can be used to perform complex operations directly within MongoDB. However, you should familiarize yourself with aggregation operators and stages, such as $match, $group, $project, and $lookup, to manipulate and aggregate data efficiently.

Scaling horizontally
Scaling horizontally is one of the key points of MongoDB. Sharding helps distribute data across servers or shards to handle large volumes of data and high throughput workloads. However, consider the scalability requirements and plans for early sharding when designing MongoDB deployment. Further, select an appropriate shard key to distribute data equally across the shards by regularly monitoring their distribution and performance based on application growth.
Preventing unauthorized access

Databases often contain valuable and sensitive information, such as personal data, financial records, intellectual property, and proprietary business information. To achieve this, follow security best practices such as enabling authentication, using role-based access control (RBAC) to define user permissions, and configuring network encryption with transport layer security/secure sockets layer (TLS/SSL). Regularly update MongoDB to the latest version to patch security vulnerabilities and maintain a robust security posture.

Summary
In this reading, you've learned about what MongoDB is. MongoDB is an open source document-oriented database designed to store large data sets for industries. It supports various programming languages such as C, C++, C#, .Net, Go, Java, Node.js, Perl, PHP, Python, Motor, Ruby, Scala, Swift, and Mongoid.

To use the full potential of MongoDB, one should follow several best practices, such as:

Data Modelling
Indexing
Aggregation framework
Scaling horizontally
Preventing unauthorized access

apache cassandra overview: see cassandra-1, cassandra-2, cassandra-3 cassandra-4 cassandra-5 cassandra-6

Architecture of Cassandra

Cassandra is a popular open-source distributed NoSQL database system known for its scalability, high performance, and fault tolerance. While it doesn't have a traditional architectural structure like a building, it does have a robust design that allows it to handle large amounts of data across multiple nodes. After reading this document, you will have a basic understanding of the components.

Apache Cassandra topology

Cassandra is a distributed system architecture. The basic component of Cassandra's architecture is the standalone unit, node. It is also known as a single Cassandra. Nodes can be added or removed without affecting the system's availability. Each node operates independently and communicates with other nodes through a peer-to-peer protocol. see cassandra-7

As well as being a distributed system, Cassandra is designed to be a peer-to-peer architecture, with each node connected to all other nodes. Every Cassandra node can run all database operations and handle client requests independently, eliminating the need for a primary node. see cassandra-8

How do the nodes in this peer-to-peer architecture know to which node to route a request without a primary node? What if a certain node is down or up? Through the gossip protocol.

The gossip protocol enables nodes to exchange details and information, updating each node about the status of all other nodes. A node performs gossip communications with up to three other nodes every second. The gossip messages follow a specific format and use version numbers to communicate efficiently. So, each node can build the entire metadata of the cluster (which nodes are up/down, what the tokens allocated to each node are, and so on).

Components of a Cassandra node

Several components in Cassandra nodes are involved in the write and read operations. Some of them are listed next:

Memtable
Memtables serve as in-memory structures within Cassandra, buffering write operations before being written onto disk. Typically, each table has an active Memtable. Eventually, these Memtables are flushed to disk, transforming into immutable SSTables (Sorted String Tables).

The triggering of Memtable flushes can occur through various methods:

Exceeding a predefined threshold for Memtable memory usage.
Approaching the maximum size of the CommitLog, which prompts Memtable flushes to free up CommitLog segments.
Setting specific time intervals to trigger flushes on a per-table basis.

These triggers initiate the process where the buffered data in Memtables is persisted onto disk as SSTables, ensuring data durability and efficient retrieval in the Cassandra database system.

Commit log

Commit logs in Cassandra function as append-only logs, capturing all local mutations on a specific Cassandra node. Before you write data to a Memtable, you must record it in a commit log. This process ensures durability in the event of an unexpected shutdown. Upon restarting, any mutations in the commit log are applied to the Memtables, guaranteeing data consistency and recovery in the Cassandra database system.

SSTables

SSTables (Sorted String Tables) are the immutable data files in Cassandra, storing data persistently on disk. These files are created by flushing memtables or streaming from other nodes. When you generate SSTables, Cassandra initiates the compaction processes to merge multiple SSTables into one. You should be able to see the new SSTable while the older SSTables become eligible for removal.

An SSTable comprises various distinct components stored in separate files, some of which include:

Data.db: This file contains the actual data stored by Cassandra.
Index.db: An index file that maps partition keys to specific positions within the Data.db file, aiding in efficient data retrieval.
Summary.db: This file provides a sample subset (typically every 128th entry) of the information contained in the Index.db file, offering an overview for quicker data access.
Filter.db: Cassandra employs a Bloom Filter in this file, which serves as a probabilistic data structure, assisting in determining if a partition key exists in the SSTable without requiring a disk seek.
CompressionInfo.db: This file holds metadata regarding the offsets and lengths of compressed chunks within the Data.db file, facilitating the decompression of stored data.

These distinct files within an SSTable collectively form an organized structure that enables efficient data storage, indexing, retrieval, and compression within the Cassandra database system.

Write process at node level

Writes are distributed across nodes in the cluster, utilizing a coordinator node that manages the operation and replicates data to appropriate replicas based on the configured consistency level.

Logging data in the commit log
Writing data to the Memtable
Flushing data from the Memtable
Storing data on disk in SSTables

see cassandra-9

Read at node level

While writes in Cassandra are very simple and fast operations done in memory, the read is a bit more complicated since it needs to consolidate data from memory (Memtable) and disk (SSTables). Since you can fragment data on disk into several SSTables, the reading process needs to identify which SSTables most likely contain info about the querying partitions. The Bloom Filter information makes this selection through the following steps:

1. Checks the Memtable
2. Checks Bloom filter
3. Checks partition key cache, if enabled
4. If the partition is not in the cache, the partition summary is checked
5. Then, the partition index is accessed
6. Locates the data on the disk
7. Fetches the data from the SSTable on disk
8. Data is consolidated from Memtable and SSTables before being sent to the coordinator

see cassandra-11

Data distribution

Partitioner: Cassandra uses a partitioner to distribute data across nodes. This consistent hashing algorithm ensures an even data distribution across the cluster, preventing hotspots and facilitating horizontal scaling.

see cassandra-12

Data model
Column family data model: Cassandra follows a column-family-based data model, organizing data into rows and columns. It stores data in tables composed of rows indexed by a unique key. Each row contains columns with different attributes.

see cassandra-13

Replication and consistency
Replication: Data replication is configurable in Cassandra. Each piece of data is replicated across multiple nodes (replication factor) to ensure fault tolerance and high availability, even in node failures.

Consistency levels: Cassandra offers tunable consistency levels for read and write operations. Users can choose between different consistency levels, balancing consistency and performance based on application requirements.

Read and Write Operations
Distributed writes: Writes are distributed across nodes in the cluster, utilizing a coordinator node that manages the operation and replicates data to appropriate replicas based on the configured consistency level.

Read optimization: Cassandra supports efficient reads by allowing read operations from multiple replicas. It uses the "hinted handoff" mechanism to ensure consistency in case some replicas are temporarily unavailable.

Architecture Components
Gossip protocol: Cassandra uses the gossip protocol for inter-node communication. Nodes exchange information about the cluster's state, such as membership changes or node failures, ensuring each node stays updated.

Compaction and compression: Cassandra periodically performs compaction to merge and organize data files, optimizing storage and improving performance. It also supports data compression to reduce disk usage.

Snitches and load balancing: Snitches determine network topology, optimizing data replication and routing. Load balancing ensures that queries are evenly distributed across nodes to prevent uneven distribution.

Security
Security features: Cassandra offers security features like authentication, encryption, and access control mechanisms to ensure data integrity, confidentiality, and protection against unauthorized access.

Summary
Cassandra's architecture is designed for high availability, fault tolerance, scalability, and efficient data management across distributed environments, making it a powerful choice for handling large-scale data-intensive applications.

key features of apache cassandra: cassandra-14 While all NoSQL databases are distributed, you will not find many NoSQL databases that are both distributed and decentralized. Distributed means that Cassandra clusters can be run on multiple machines, while to the users and applications, everything appears as a unified whole. The architecture is built in such a way that the combination of Cassandra application client and server will provide sufficient information to route the user request optimally in the cluster. As an end user, you can write data to any of the Cassandra nodes in the cluster, and Cassandra will understand and serve your request. Decentralized means that every node in the Cassandra cluster is identical. That is, there are no primary or secondary nodes. Cassandra uses peer-to-peer communication protocol and keeps all the nodes in sync through a protocol called gossip.

cassandra-15 How does data end up in this distributed architecture? It all starts with the queries that are planned to be performed. Just to take a very simple example, you have some initial data containing user info and the state. If your queries are similar to, I would like to know about all users in a state, in this case, you need to group your data on the state column, and this is done by declaring a table that has the state column as the partition key. Just remember that Cassandra groups data based on your declared partition key and then distributes the data in the cluster by hashing each partition key called tokens. Each Cassandra node has a predefined list of supported token intervals, and data is routed to the appropriate node based on the key value hash and this predefined token allocation in the cluster. 

cassandra-16 After data is initially distributed in the cluster, Cassandra proceeds with replicating the data. The number of replicas refer to how many nodes contain a certain piece of data at a particular time. Data replication is done clockwise in the cluster, taking into consideration the rack and the data center's placement of the nodes. Data replication is done according to the set replication factor, which specifies the number of nodes that will hold the replicas for each partition. Let's take the data from the previous screen and try to distribute the California state data, partition CA, in an eight-node cluster, distributed in two data centers, DC1 with replication factor 3 and DC2 with replication factor 2. You can see in the diagram that some nodes are placed in the same rack. Cassandra will try to distribute data as much as possible between racks. 

cassandra-17 One of the most important Cassandra features is its availability. Also, Cassandra is frequently referred to as eventual or tunable consistency in the sense that, by default, Cassandra trades consistency in order to achieve availability. But the good news is that developers can control exactly how much consistency they would like to have strong or eventual. As you may recall from the NoSQL introduction video earlier in the course, distributed systems cannot, according to CAP theorem, be consistent and available at the same time. Cassandra has been designed to always be available, meaning that if you lose a part of your cluster, there will still be nodes available to answer the service request, though the return data might be inconsistent. Consistency of the data can be controlled at the operation level, and it is tuned between strong consistency and eventual consistency. If data inconsistencies exist, these conflicts will be resolved during read operations. This is another unique Cassandra feature.

cassandra-18 Fault tolerance is an inherent feature of the distributed and decentralized characteristic of Cassandra. The fact that all nodes have the same functions, communicate in a peer-to-peer manner, are distributed, and the data is replicated makes Cassandra a very tolerant and adaptable solution when nodes fail. The user contacts one node of the cluster. If the node is not responding, then the user will receive an error and contact another node.

cassandra-19 The same architectural flexibility is visible in the way Cassandra scales the capability of the clusters. A cluster is scaled by simply adding nodes, and performance increases linearly with the number of added nodes. New nodes that are added immediately start serving traffic, while existing nodes move some of their responsibilities towards the new added nodes. Both adding and removing nodes is done seamlessly without interrupting cluster operations.

cassandra-20 Cassandra gracefully handles large numbers of writes first by parallelizing writes to all nodes holding a replica of your data.

cassandra-21 One important Cassandra fact; by default, there's no read before write. At the node level, writes are performed in memory, meaning no read before and then flushed on disk. On disk, data is appended in a sequential manner with the data being reconciled later through compaction.

cassandra-22 Be aware that while syntax-wise, there are similarities between CQL and SQL, the resemblance stops here. The ways write and read operations are executed in Cassandra are different than how these are executed in relational databases. 

apache cassandra data model: cassandra-23 Cassandra stores data in tables whose schema defines the storage of the data at cluster and node level. Tables are grouped in key spaces. A keyspace is a logical entity that contains one or more tables. A keyspace also defines a number of options that applies to all the tables it contains, most prominent of which is the replication strategy used by the key space. It is generally encouraged to use one keyspace per application. 

cassandra-24 So tables are the logical entities that organize data storage at cluster and node level. They contain rows of columns. You can create, drop, and alter your tables without impacting the running updates on your data or the running queries. In order to create a table, we need to declare using Cassandra query language a schema. A table schema comprises at least a definition of the table's primary key and the regular columns of the table. Table groups store information regarding several groups, such as groupid, group_name, and for each group, the username and age of their members. You can see that the primary key is composed of two columns, groupid and username. In Cassandra denomination, the groupid column is called the partition key and the username column is called the clustering key.

see cassandra-25 cassandra-26 When data is inserted into the cluster in a table, the data is grouped per partition key into partitions, and the first step is to apply a hash function to the partition key. The partition key hash is used to determine what node and subsequent replicas will get the data. In simpler terms, a partition key determines the data locality in the cluster. The partition is the atom of storage in Cassandra, meaning that one partition's data will always be found on a node and its replicas in the case of a replication factor greater than one. So if we want to answer the query all users in group 12, then the query can address only the fourth node and will get the answer. In big clusters, those consisting of hundreds or thousands of nodes, limiting the number of nodes required to be contacted in order to answer the query is crucial for query performance.

cassandra-27 cassandra-28 cassandra-29 While the partition key is important for data locality, the clustering column specifies the order that the data is arranged in inside the partition that is, ascending or descending, and optimizes the retrieval of similar values column data inside a partition. The clustering key can be a single or multiple column key. In our case, our clustering key contains only one column username, which means that the data inside the group partition is going to be stored by username by default in an ascending order. So when we query all users in a specific group, data will be ordered by default in ascending order by the user's username. In this example, the clustering key was used for one main reason, to add uniqueness to each entry. But actually the clustering key is also very important for improving the read query performance.

cassandra-30 Thus, a query such as give me all users in a certain group by group ID that are aged 32 will just locate the node that contains the partition for group 12 and read from that node just the two sequential records. Reducing the amount of data to be read from a partition is crucial for query time, especially in the case of large partitions in which Cassandra would have to read hundreds of megabytes of data in order to provide an answer from just a few kilobytes of data. 

cassandra-31 Inserting new data in our table will just direct the write to the location of the partition and will increase the partition size from two to three entries. In dynamic tables, partitions grow dynamically with the number of distinct entries due to the presence of the clustering key in the primary key. It's important to note that in this diagram, replication is not taken into account, so only one node holds the partition for group 45 for example. Every write or read for group 45 will be routed to the second node of the cluster. As long as the cluster is not changed, this node will hold the data for group 45. A change such as nodes added or leaving would trigger a new token allocation and subsequently data distribution. 

cassandra-32  When building a primary key for a table, you should take into consideration the following simple rules. First, choose a partition key that starts answering your query, but that also spreads the data uniformly around the cluster. For example, group ID might be a good partition key if there are many groups. Many distinct values for group id and group sizes are similar. And secondly, build a primary key that allows you to minimize the number of partitions read in order to answer a certain query. Remember that data is distributed throughout the cluster. If we would need to read more partitions to answer the query, then we would need to potentially access several nodes in order to answer our query. This would affect the query time and even induce timeouts. Thus, the primary key needs to be designed so that optimally we read one partition in order to answer our query. And one note before we summarize, besides the basic rules discussed here, make sure you build a clustering key that helps you reduce the amount of data that needs to be read even further by ordering your clustering key columns according to your query.

into to cassandra query shell: cql-1 cql-2 cql-3 cql-4 cql-5 cql-6 cql-7 cql-8 cql-9 cql-11 for example, if we have a Cassandra cluster of 8 nodes and 2 Data Centers, one data center with replication factor 2 and the other one with replication factor 3, then overall, replication is 5. If we set CONSISTENCY QUORUM on a write operation: first of all, QUORUM means a majority of nodes out of the replicas, so a majority here would be 3 Write operations will go to all 5 replicas but a minimum 3 nodes (from both data centers) out of the 5 replicas need to answer in order for the operation to be successful.

cql-12 While bulk copy in Cassandra should be done through special procedures, if you would like to just test your model or you're working with a smaller, text delimited dataset, then you can use COPY FROM or COPY TO operations to bring data in, and export data out, of Cassandra. COPY TO exports data from a table into a CSV file. Each row is written to a line in the target file with fields separated by the delimiter. COPY FROM imports data from a CSV file into an existing table. Each line in the source file is imported as a row. All rows in the dataset must contain the same number of fields and have values in the PRIMARY KEY fields. The process verifies the PRIMARY KEY and imports data accordingly. 

see Lab: Using the CQL Shell (cqlsh)

CQL data types: see cql-data-types-1 cql-data-types-2 cql-data-types-3 cql-data-types-4 cql-data-types-5 cql-data-types-6
cql-data-types-7 cql-data-types-8 cql-data-types-9

apache cassandra keyspace operations: keyspace-1 keyspace-3 In CQL, when we create a keyspace, we need to specify the class as being network topology strategy, the only option recommended for production systems, and specify the replication at datacenter level keyspace-4
keyspace-5 keyspace-6 keyspace-7  

Cassandra Data Modeling and Querying Best Practices

This reading will teach you the best practices for Cassandra data modeling and querying.

Cassandra is an open-source, non-relational, or NoSQL distributed database that is continuously available across multiple data centers and cloud availability zones. It provides a reliable data storage engine for applications with a broad range.

Understanding Cassandra's distributed architecture and data storage principles is crucial for designing data models and crafting efficient queries. Let's examine the best practices for achieving high performance, scalability, and efficiency in Cassandra.

Improved read and write performance

An efficient data model enables data to be requested as a single request, reducing complex joins. However, it also enables Cassandra to retrieve data quickly from distributed storage.

Partition correctly

A good partition key restricts hotspots and disproportionately stores the amount of data on a few nodes. It also avoids selecting high-cardinality or frequently updated fields as partition keys and restricts large partitions.

Data duplication

Data duplication supports query patterns and uses primary keys across multiple tables to optimize queries without compromising performance.

Avoid full table scans

Avoiding full table scans minimizes them and range queries without specifying a partition key. Full table scans are resource-intensive, compromising performance for large datasets.

Specify required columns

The data query from Cassandra specifies the SELECT statement's columns. Obtaining only the required columns reduces the usage of the network bandwidth and minimizes the amount of data transferred between nodes.

Prefer batch updates

The batch update achieves atomicity and consistency for a group of write operations. However, it minimizes the overhead associated with network communication and coordination between nodes in the Cassandra cluster.

table operations: Remember that in Cassandra, prior to declaring a table, you first need to create a keyspace. A table will always be created in an existing keyspace. table-1 table-2 table-3 table-4 table-5 table-6 table-7

CRUD operations: crud-1 At cluster level, when a write occurs, the node receiving the write becomes the coordinator of the operation. This means that it will make sure to complete the operation and send the result of the write back to the user. Write operations are directed towards all replicas of the partition into which they are writing, but for the operation to be successful, acknowledgement is expected from at least the minimum number of nodes specified for consistency. Let’s take the example of a keyspace with replication factor 3 and a write operation with consistency set to 2. We assume our partition is located in nodes 1, 2, and 3. In this case the write operation arrives in node 4, and node 4 sends the write to nodes 1, 2, and 3 according to the replication factor. However, node 2 is not available, so the write acknowledgement is sent only by nodes 1 and 3. Coordinator node 4 checks the number of answers and compares it to the expected consistency value, and then returns an okay. crud-2 At the node level there are a few important facts to remember: One important reminder regarding writes in Cassandra: by default, there's no read before the write operation. At the node level, writes are stored in memory and then flushed on disk in files called SSTables. The more writes we perform, the faster the Memtable gets filled and flushes the data on disk. Every flush operation creates a new file called SSTable. On disk data is appended in subsequent SSTables – later on the data will be optimized on disk through a process called compaction. Cassandra attaches a Timestamp to every write operation. Timestamps are used for data reconciliation. The most recent data wins. crud-3 crud-4 crud-5 crud-6 crud-7

crud-8 When a read operation is directed to a node in the cluster, that node becomes the coordinator of the read operation and is responsible for its completion. In our example, node 4 is the coordinator for this read. Reads are sent only to the number of replicas specified by the consistency setting. For example, a consistency of two means that only two nodes of the three replicas will be contacted. The coordinator will reconcile the responses received from the contacted nodes. If there are any inconsistencies, they will be resolved based on the operations time stamps, then the result will be sent back.

crud-9 Syntax-wise, in Cassandra, just like in many databases, reads are performed using the select operation. There are a few rules when it comes to select operations in Cassandra. Always start your query with the partition key to limit your read to only the replicas for your partition. Follow your primary key fields order in your query to get the best performance. For example, if you have a primary key formed of one partition key and two clustering keys, filtering a specific value of the partition key is okay. Note that partition key supports only equal and in operations. Filtering a list of possible values for the partition key is also okay. Filtering the partition key and the first clustering key is okay. Filtering the partition key and the first and second clustering keys will just read one record, which is okay. However, selecting all data from your table is not okay in production systems because this will send the request to all nodes in the cluster. Imagine having a 1,000 node cluster, would you be happy with the performance of your query? Your query will work, but the performance will be really bad, even if no timeout occurs. Filtering a regular column is not okay and will not work. Finally, filtering the clustering keys without the partition key is not okay and will not work.

crud-10 Let's look at some examples based on our groups table where the primary key is composed of group ID and username columns. Filtering the group ID or group ID and username will work and yield a very good performance. Filtering the age column, which is a regular one, will not work, Cassandra will return an error. What you could do in this case is either remodel your table or create an index on column H, and then the select query will work. But while the previous query will work, optimal performance will be obtained by filtering the age column together with a specific partition key group ID. When you do this, you limit your query to only the nodes that host that particular partition key. This brings us again to this very important Cassandra rule: always start your queries with the partition key. 

crud-11 crud-12 crud-13 From the moment an item has been marked with a tombstone, its data is not visible any longer to queries. However, the actual data will still reside on disk for a period of time. 

crud-14 What are tombstones? In Cassandra, a delete is just a write operation that also has a special value appended to indicate that the data has been deleted and the time of the delete. This value is called a tombstone. Here, as you can see in the diagram, our delete was acknowledged by nodes 1 and 3, and now we have a tombstone indicating that our specific primary key data was deleted at T1. Node 2 data is the initial one, the insert done at T0 since the delete operation did not succeed on node 2. Now, when a read operation arrives, the data will be returned by nodes 1 and 2. Node 1 will return no data with time T1 and node 2 will return requested data with time T0. T1 is more recent than T0. So the coordinator node, node 1, will decide that the newest data is the one from node 1 and return no data as part of the read. One other thing to remember is that tombstones are deleted only when a configurable period has elapsed. This is called gc_grace_seconds, which is set to 10 days by default and is set at the table level.

see Cassandra CRUD Operations

