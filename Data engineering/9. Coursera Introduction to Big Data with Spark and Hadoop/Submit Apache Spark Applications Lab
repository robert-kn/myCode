In this lab, you will learn how to submit Apache Spark applications from a Python script. This exercise is straightforward, thanks to Docker Compose.

Learning objectives

In this lab, you will:

Install a Spark Master and Worker using Docker Compose
Create a Python script containing a Spark job
Submit the job to the cluster directly from python (Note: you'll learn how to submit a job from the command line in the Kubernetes lab)

Prerequisites

The only prerequisites for this lab are:

The wget command line tool
A Python development environment

Start the Spark Master

Enter the following commands in the terminal to download the Spark environment:

wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz && tar xf spark-3.3.3-bin-hadoop3.tgz && rm -rf spark-3.3.3-bin-hadoop3.tgz

Run the following commands to set up JAVA_HOME (preinstalled in the environment) and SPARK_HOME (which you just downloaded):

export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64
export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3

Run the following command to create a config file for the master:

touch /home/project/spark-3.3.3-bin-hadoop3/conf/spark-defaults.conf

Paste the following content into the spark-defaults.conf file. This will configure the number of cores and the amount of memory that the Master will allocate to the workers.

spark.executor.memory 4g
spark.executor.cores 2

Navigate to the SPARK_HOME directory:

cd $SPARK_HOME

Run the Spark master by executing the following command:

./sbin/start-master.sh

Once it starts up successfully, verify that the master is running successfull on port 7077

https://robertndungu-8080.theiadockernext-0-labs-prod-theiak8s-4-tor01.proxy.cognitiveclass.ai/

Create code and submit

open new terminal and 

touch submit.py

Paste the following code to the file and save it. Remember to replace the placeholder yourname in the code below with your name as in the Spark master URL.

Run the following commands to set up JAVA_HOME and SPARK_HOME:

export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64
export SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3

Install the required packages to set up the Spark environment.

pip3 install findspark

Type in the following command in the terminal to execute the Python script:

python3 submit.py