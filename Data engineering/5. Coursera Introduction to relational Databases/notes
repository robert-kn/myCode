what will you learn? explore concepts of relational databases. navigate db2, mysql, and postgresql admin and gain 
insight into their distinctive features.

Every good relational database solution begins with a solid design and implementation strategy. A well designed 
database ensures that the users and applications that depend on the data will know that it is;
- accurate; can you rely on the accuracy of the data as new information is added or it is modified?
- easy to access; is the data organised to make it fast, easy, and predictable to query and maintain?
- reliable; can your database design ensure data integrity and maintain consistent and reliable data?
- flexible; can you update or expand on the design to meet future requirements?

this course will teach you basic relational database concepts and learn to think about data in terms of relationships 
and how information can be best organised to produce results you want to achieve. i will also learn about different 
deployment topologies (star schema vs snowflake schema?) and the tradeoffs that impact your design decisions.

you will create entity relationship diagrams (ERD's) for specific usecases that will serve as the blueprint for the 
implementation of my database. i will also learn about the role of relational model contraints in ensuring that my 
data maintains its integrity and achieves a level of perfomance that meets the needs of the users of the data.

i will then learn how to use sql statements and several relational database management systems (rdbms) tools to 
transform a relational database design into a database and its objects such as taables, keys, indexes, and 
constraints.

i will build on hands on experience by developing an erd to map out the data model of a relational database, applying
techniques that help improve the integrity of my data and the performance of my queries

what is the definition of data? see dataDescription1.png

whaat are the different types of data structure? see structureofData1.png structureofData2 structureofData3 
structureofData4 structureofData5 structureofData6 structureofData7

typical data sources? datasources.png

after data has been collected, where is it stored? see datastored1 datastored2 datastored3 datastored4 datastored5 
datastored6 datastored7

what are two key concepts that play an essential role in data organisation? see twoConcepts1.png

what is an information model? see informationModel1.png informationModel2.png 

what is a data model? dataModel1 dataModel2 dataModel3 dataModel4 (They often involve normalization processes to 
ensure data integrity and reduce redundancy)

what are the differences between information models and data models? differences1.png

what is a hierarchical model? see hierarchicalModel.png 

what is the relationship between information models and hierarchical models? see informationVersusHierarchical1.png 
informationVersusHierarchical2.png informationVersusHierarchical3

types of data models? typesOfDataModels.png

of the two, which is the most widely used model in databases? see relationlModel1

more on the entity relationship data model see erdm1.png 

concepts in database management that help to ensure adaptability and efficiency in database management? 
conceptsDatabaseManagement1

recap1.png

what is an ERD? is a graphical representation of entities and relationships between them. it is a modelling technique
used in database design to represent the structure of a database system visually. the primary components of an ER 
diagram include entities, attributes, and relationships. see ERD1 ERD2 ERD3 ERD4 ERD5 ERD6 ERD7 ERD8 ERD9 ERD10 
ERD11 ERD12 ERD13 ERD14 ERD15 ERD16 ERD17

recap2.png
recap3.png 

what are the best practices for designing a relational database? see best-practices1.png best-practices2 
best-practices3 best-practices4

a database represents a singular entity with its columns representing the aattributes of that entity see 
entity-example.png. The data type you assigned to a column controls the data the column can store. see varchar1.png 
varchar2 varchar3. Different database systems may have variations in how they handle common data types. These include 
date, time, float, or decimal. Most databases have date for storing dates, year, month, day, and time for time of day.
They also usually support a DATETIME or TIMESTAMP type for a combination of date and time. Example in MySQL date 
format is year, month, day while TIMESTAMP includes both date and time. Float and decimal are numeric data types used
for numbers with fractional parts. Float is a floating point number with approximate precision. It's used when exact 
precision isn't necessary. Decimal, on the other hand, is used for exact arithmetic calculations. It's more suitable 
for financial calculations where precision matters. Example in SQL server, FLOAT(24) represents a floating point 
number, whereas DECIMAL(5,2) stores a number with five total digits, two of which are after the decimal point. 
Integer types big INT, int, small int store whole numbers. Each type has a different range. For instance, Int 
typically stores numbers from -2,147,483,648 to 2,147,483,647.

Binary data type stores binary data like images or files and types like binary large object, BLOB. These types store 
data as a sequence of bytes, which is ideal for non-textual data. Char is another character data type used for 
fixed-length strings. Unlike Varchar, char always uses the specified number of characters padding the value with 
spaces if necessary. Understanding the nuances of different data types and their implementation across various 
database platforms is key to efficient database design and data storage. Always choose the data type that best suits 
the nature of your data and the requirements of your database system.

Using the appropriate data type provides many advantages. Some of these are when you define the data type, a column 
should hold, you avoid inserting incorrect data into that column. When date, time, and numeric data are correctly 
typed, you can accurately sort that data. Similarly, you can accurately select ranges of data when it is correctly 
typed and you can perform numeric calculations on typed data for example, calculating an order's total cost. see 
advantages-of-using-data-types.png 

The relational model introduced in 1970 offers a powerful approach to organizing and understanding data.  It centers 
around two fundamental concepts, sets and relations. see fundamental-concepts1.png fundamental-concepts2 
fundamental-concepts3 fundamental-concepts4 fundamental-concepts5 fundamental-concepts6 fundamental-concepts7 
fundamental-concepts8 fundamental-concepts9 fundamental-concepts10 fundamental-concepts11 fundamental-concepts12 
fundamental-concepts13 fundamental-concepts14. To comprehend the relational model, let us understand key concepts 
like degree and cardinality see fundamental-concepts15 fundamental-concepts16 

recap4.png

what is a deployment topology and what does it depend on? see deployment-topology1

what are the common deployment topologies? see common-deployment-topologies1

single tier architecture: deploys all components of an application, including the user interface, application logic, 
and data storage on a single server or machine. In this configuration, the application's entire stack operates within
the same environment.

client-server architecture: also referred to as a two-tier architecture. The architecture represents a deployment 
topology that divides the application into two distinct layers. A client layer, responsible for the user interface, 
and a server layer, managing the application logic and data storage. In this scenario, a remote server hosts the 
database and users usually access it from client systems, commonly through a web page or local application. In 
two-tier database set up, the database server and application function are in distinct tiers. The client tier 
application establishes a connection to the database server through a database interface such as an API or framework. 
Depending on the programming language used to write the application, this interface communicates with the database 
server through a database client or API installed on the client system. The server's database management system 
software, DBMS, comprises several layers, broadly categorized as data access layer, database engine layer, database 
storage layer. Communication between the database interface and server occurs through a database client or API 
installed on the client system. The data access layer server provides interfaces for various client types 
encompassing standard APIs like JDBC and ODBC, command line processor, CLP (command line processor) interfaces, and 
vendor-specific or proprietary interfaces. Additionally, the database server incorporates an engine responsible for 
compiling queries, retrieving and processing data, and delivering the result set. see client-server-arch1.png 
client-server-arch2 client-server-arch3 client-server-arch4

three tier architecture: In scenarios involving multiple users, the introduction of a middle tier or application 
server layer often occurs between the application client and the remote database server, forming a three-tier 
architecture commonly deployed in production environments. see three-tier1

Most production environments usually restrict access to database servers, except administrators. This limitation 
arises for several reasons. A primary concern is security. Database servers contain sensitive data. Limiting access 
to authorized personnel is important to protect against unauthorized data access or modification. The next concern is 
performance optimization. Database servers are often critical components of applications, so it is important to avoid 
overloading them with unnecessary traffic. Lastly, maintainability is crucial, and administrators, with their training 
and experience, are entrusted with making changes to the database schema or data to avoid disrupting the application. 
In a three-tier database architecture, the application presentation layer and business logic layer exist in different 
tiers. The presentation layer serves as the user interface, accessible through various platforms like desktop 
applications, web browsers, or mobile apps. The client application interacts with an application server via the 
network. The application server encapsulates both the application and business logic, establishing communication with 
the database server through a database API or driver. see three-tier2 three-tier3

cloud deployment: the database resides within a cloud environment, offering the many advantages inherent to 
cloud-based services. Therefore, it eliminates the need to download or install database software locally and ongoing 
maintenance of supporting infrastructure. Cloud-based databases are easily accessible to users from anywhere at any 
time, and with only an internet connection required. Client applications and users interact with the database through 
an application server layer or interface situated within the cloud environment. The flexibility of cloud deployments 
make them suitable for a wide range of purposes, including development, testing, and full-scale production 
environments. cloud-based.png

In our exploration of database architectures, we've primarily focused on single-server configurations. However, for 
critical or large-scale workloads where high availability or scalability is important, relational database management 
systems, RDBMSs, offer distributed architectures. These distributed database architectures involve clusters of 
machines interconnected through a network, distributing data processing and storage tasks. The approach brings about 
notable benefits including enhanced scalability, fault tolerance, and overall performance improvements. see
distributed-architecture1 distributed-architecture2

types of database architecture? see types-of-architecture

shared disk architecture: involves multiple database servers processing workloads in parallel. Each server 
establishes a connection to shared storage and communications with other servers using high-speed interconnection. 
The shared disk architecture also facilitates the effective distribution of workloads, ensuring scalability as the 
demand for processing power grows. In the event of a server failure, a mechanism is in place to reroute clients 
seamlessly to other servers, maintaining high availability and minimizing service disruptions. see shared-disk-arch1
shared-disk-arch2

shared-nothing-architecture: utilizes either replication or partitioning techniques. The approach allows for the 
effective distribution of client workloads across multiple nodes, promoting parallel processing and efficient 
resource utilization. One of the key advantages lies in enhanced fault tolerance achieved by rerouting clients to 
alternative nodes in the event of a server failure. 

Certain distributed database architectures employ a combination of shared disk, shared nothing, replication or 
partitioning techniques. Additionally, they integrate specialized hardware components to achieve specific goals 
related to availability and scalability. 

techniques for managing data and optimizing performance. Some of the common techniques include database replication, 
database partitioning and sharding.

database replication: is a technique that involves copying changes from one database server to one or more replicas. 
This process distributes the client workload across servers, leading to improved performance. When the replica 
resides in the same location, we call it a high availability, HA, replica. If the primary database server experiences 
a failure due to software or hardware issues, the system redirects clients to HA replica. To mitigate broader 
disasters, organizations establish replicas in geographically distributed locations. This guarantees that during 
instances of complete data center outages, be it due to power loss, fire, earthquake or flood, clients can be rerouted 
to disaster recovery replicas. see replication1

An alternative strategy involves partitioning tables with substantial data into logical segments, each containing a 
subset of the overall data, e.g., sales records for different quarters. This technique, known as sharding, places 
these partitions on separate nodes in a cluster. Each shard possesses its compute resources, processing, memory, and 
storage to operate on its specific subset of data. When a client issues a query, it is processed in parallel across 
multiple nodes or shards, and the results from different nodes are synthesized and returned to the client. As data or 
query workloads increase, additional shards and nodes can be seamlessly added to the database cluster, facilitating 
increased parallel processing and improved overall performance. Database partitioning and sharding are particularly 
prevalent in handling data warehousing and business intelligence workloads that involve extensive volumes of data. 
see partitioning1

database users? see data-users1

data engineers and database administrators see engineers-and-DBAs1

Data engineers and database administrators employ the following mechanisms for performing their tasks. Graphical user
interfaces (GUIs) or web-based management tools, command-line interfaces and utilities, application programming 
interfaces (APIs). Let's explore the utility of these mechanisms. GUIs or web-based management tools serve as common 
means for interacting with databases. Databases typically offer graphical tools, web-based interfaces, particularly 
for cloud databases and mobile applications. If vendor-provided tools lack sufficient functionality or user 
friendliness, users may turn to third-party or specialized alternatives. For example, Oracle SQL Developer is a 
graphical tool provided by Oracle for database management. Command-line interfaces and utilities continue to play a 
crucial role despite the prevalence of GUI tools. Proficiency in command-line usage remains valuable for certain tasks. 
While using these tools, users can issue database commands directly or utilize interactive shells for efficient 
command line interactions. Let's explore examples of each. Command-line interfaces may involve issuing straightforward 
database commands directly from the terminal, such as, `db2 create database sample` or `mysqldump sakila > sakila.sql`. 
Alternatively, interactive command-line shells like sqlplus for Oracle or db2 clp can be utilized as demonstrated by 
db2, db2 > connect to sample. Furthermore, SQL scripts and batch files executed from the shell provide additional
means of interacting with databases. Databases often incorporate programmatic interfaces or APIs designed for 
administrative tasks accessible from applications and tools created by data engineers or third parties. Users commonly
employ APIs in situations requiring automated or programmatic access for tasks like creating and managing database 
objects, setting access controls, and monitoring performance. 

Other key users include data scientists and business analysts. These users engage with databases for data analysis, 
insight derivation, and data-driven predictions. Their typical data access patterns involve reading from existing 
data sources, occasionally necessitating the creation and population of database objects, especially in their sandbox 
environments. Frequently employed tools for tasks in data science and machine learning encompass Jupyter, R Studio, 
Zepplin, SAS, and SPSS. Reporting, dashboarding and business intelligence tasks utilize tools such as Microsoft Excel, 
Microsoft PowerBI, Microsoft Tableau, and Microstrategy. Both data science and business intelligence tools interact 
with relational databases through SQL interfaces and APIs, often abstracting away the direct use of SQL. Additionally, 
users may opt for SQL query tools for ad-hoc querying, commonly provided by databases, or available as third-party 
solutions compatible with various database systems. 

The last users we will discuss are application developers. They seldom access databases directly. They create 
applications that can require both read and write access to databases. Developers use programming languages such as 
C++, C#, Java, JavaScript, .NET, PHP, Perl, Python, and Ruby to write applications. The programming languages 
communicate with the database through SQL interfaces and APIs like ODBC and JDBC. Some databases, especially 
cloud-based ones, also include REST APIs for accessing data. Although programming applications using these lower-level 
APIs is feasible, it is worth noting that this was the conventional approach for developing applications in the past. 
Nowadays, most programmers opt for object relational mapping (ORM) frameworks when working with databases. ORM 
frameworks are tools in software development that facilitate the interaction between a relational database and an 
object-oriented programming language. They are user friendly and can conceal the intricacies of the underlying 
relational database and SQL. Examples of popular ORM frameworks include, ActiveRecord in Ruby applications, Django in 
Python, Entity Framework in .Net, Hibernate in Java, and Sequelize in JavaScript. see application-developers1


In the 1960s, IBM and American Airlines created the IBM Sabre Seat reservation System, the first product recognizable 
as a relational database. In the early 70s, Edgar F. Codd listed 12 rules to define relational databases. In 1976, 
Peter P. Chen introduced the entity relationship (ER) database model. By the late 70s, Ingres from the University of 
California, Berkeley and System R from IBM San Jose were operational. The 1980s marked the commercial success of 
relational database systems, with DB2 becoming IBM's flagship product and structured query language (SQL) emerging as 
the standard query language. Towards the late 80s, an IBM workgroup designed a distributed relational database 
architecture, facilitating network-connected databases to collaborate for SQL requests. By the early 1990s, new client 
tools for application development including Oracle developer, power builder and VB, and tools for personal 
productivity like ODBC, Excel, and Access were becoming popular. In the late 1990s, the database industry experienced 
exponential growth. Average desktop users began to use client server database systems to access computer systems that 
contain legacy data. Some of the most popular relational databases included giants such as Oracle, Microsoft SQL 
Server, and IBM DB2. In the source databases gained momentum, replacing commercial counterparts. MySQL, PostgreSQL, 
and other systems introduced open source solutions to the internet. The 2010s saw the popularity of cloud databases 
soar, with the leading players in the industry being Amazon RDS, IBM DB2 on Cloud, Microsoft SQL Azure, and Oracle 
Cloud. Large corporations developed and maintained the most popular relational database systems for years. They 
utilized the resources and talent to create sophisticated, fully featured solutions that catered to industry needs at 
the time. These commercial relational database management systems RDBMS played a pivotal role in this landscape as 
organizations wanted licensed solutions that offered reliability, scalability, and comprehensive features. Commercial 
licenses continue to drive the popularity of these relational databases, with Oracle, Microsoft SQL Server, and IBM 
DB2 leading the market. Most large corporations offer both on-premises and cloud-based versions of their products. In 
the late 2000s, the popularity of open source licensing for relational database systems such as MySQL, PostgreSQL, and 
SQLite experienced a surge in popularity. Open-source licensing is software licensing that allows the source code of a 
computer program to be free and available to the public. The fundamental principles of open-source licensing include 
allowing users to view, modify and distribute the source code. These principles foster collaboration and transparency 
within the software development community. Examples of popular open-source databases include MySQL, which Oracle 
produces and offers under the general public license version 2. PostgreSQL is produced by the PostgreSQL Global 
Development Group and offered under the free open-source permissive PostgreSQL license. SQLite was created by Dwayne 
Richard Hipp and provided under a public domain license. Let us now discuss the popularity of commercial and 
open-source licensing industry analyst DB Engines conducts a monthly assessment of the popularity of various database 
products. The DB Engines ranking, which evaluates the popularity of relational database systems, relies on a 
multifaceted approach. The DB Engines' ranking bases its assessment on the following mentions on websites such as 
Google and Bing, General interest (Google trends), occurrence of technical discussions on platforms such as stack 
overflow and DBA stack exchange. Number of times mentioned in job postings on Indeed and SimplyHired platforms, 
mentions on LinkedIn profiles, other social media mentions. As of February 2021, here are the ten most popular 
relational database systems. Oracle, MySQL, Microsoft SQL Server, PostgreSQL, MongoDB, Redis, Elasticsearch, IBM Db2, 
SQLite, Microsoft Access. In the last decade, there has been a significant shift in the preference for licensing 
models impacting various software categories, including relational databases. The ascendancy of open-source licensing 
has been remarkable, surpassing the declining popularity of commercial licensing. Both licensing models share an equal 
distribution, with open source slightly leading at 50.1%. A 2023 DB Engine study found that open source systems 
account for 55.3% of the total DB Engine popularity score, up from 35.5% in 2013. This trend reflects the growing 
acceptance and adoption of open-source relational databases within the software landscape. A cloud database is a 
service constructed and accessed through a cloud platform, offering functionalities like traditional databases while 
leveraging the added advantages of cloud computing. Over the past decade, the popularity of cloud databases has more 
than doubled and this upward trajectory is expected to persist. By 2025, experts predict that 80% of all databases 
will deploy on or migrate to a cloud platform. This forecast emphasizes the growing significance of cloud-based 
solutions in the database landscape and reflects the continuous shift towards leveraging the scalability and 
efficiency offered by cloud computing. The primary driver for the consistent growth in the popularity of cloud 
databases is the increasing adoption of the software as a service (SaaS) model, enabling organizations to leverage 
cloud benefits like enhanced scalability. Cloud databases facilitate processing extensive data volumes essential for 
robust data analytics. Cloud platforms provide built-in data backup and disaster recovery solutions, guaranteeing 
critical data protection. Some of the leading cloud databases include Amazon DynamoDB, Microsoft Azure Cosmos DB, 
Microsoft Azure, SQL DB, Google BigQuery, and Amazon Redshift.


Database 2, or Db2, was first released by IBM in 1983 and was an early example of a relational database management 
system. This first release ran on IBM mainframe computers, but over the years, different versions were developed to 
run on many other platforms, including OS2, Unix, Linux, and Windows. After some time, the product was rewritten to 
use the same codebase across the multiple operating systems so that you can easily port applications accessing Db2 
data from one operating system to another. After many iterations of the offering across many platforms and with 
enhanced functionality, today, Db2 is a whole suite of database management products. Including Db2 database, Db2 
warehouse, Db2 on cloud, Db2 warehouse on cloud, Db2 big SQL, and Db2 for z OS. There are many ways to evaluate these 
products. You can use Db2 database community license for free with a 100-gigabyte data limit, or you can download a 
free docker image of Db2 database. You can also use the free lite plan of Db2 on cloud for development and evaluation 
purposes on IBM Cloud. Db2 Warehouse Enterprise Edition and Db2 Big SQL are available in no-cost trial editions. You 
can use Db2 warehouse on cloud for free up to 1GB of data. The Db2 products all use AI-powered functionality to 
simplify the management and querying of your data, both on-premises and in cloud environments. You can use machine 
learning algorithms to improve the efficiency and performance of queries. The column store feature to improve 
performance and reduce overheads for analytic workloads by directing queries to specific columns rather than 
processing an entire data table. And the data skipping feature to reduce overheads by automatically avoiding 
processing data that is not required in a particular query. The common SQL engine across all the Db2 family means 
that you can write a query once and be sure it will work with other products in the family. This simplifies migration 
of applications between products and platforms. The support for all data types, relational, structured, and 
unstructured, means you can access all of your corporate data to make better business decisions. And the Db2 
replication functionality enables you to implement high availability and disaster recovery solutions. Db2 provides 
scalability in a number of ways. For short peaks, you can extend on-premises storage and power levels onto hosted 
cloud deployments. And you can independently scale power and storage in a managed cloud deployment to only use and 
pay for extra resources when you need them. Or in Db2 warehouse, you can use the database partitioning feature to 
transparently split data across partitions and servers to maximize the compute power available and enable massively 
parallel processing. The Db2 family provides you with a range of data management products to work with data 
on-premises or in the cloud. Db2 database is a powerful enterprise-ready on-premises RDBMS optimized for OLTP. It is 
supported on Linux, Unix, and Windows and provides performance, high availability, scalability, and resilience. Db2 
Warehouse is an on-premises data warehouse that provides advanced data analytics, massively parallel processing, and 
machine learning. Db2 on Cloud is a fully managed cloud-based SQL database which provides similar features to Db2 
database performance, high availability, scalability, and resilience. And Db2 warehouse on Cloud is a fully managed 
elastic cloud-based data warehouse that provides similar features to the on-premises Db2 warehouse. You can deploy 
both of the cloud-based products on IBM Cloud and Amazon Web Services.

Db2 Big SQL is an SQL on-Hadoop engine that provides massively parallel processing and advanced query querying 
functionality. You can use it to query a range of data sources, including Hadoop HDFS and web HDFS, RDBMS, NoSQL, and 
other object stores. You can integrate it with Cloudera data platform or use a service on IBM Cloud Pak for data and, 
finally, Db2 for ZOS, an enterprise data server for IBM Z. It provides a mission-critical data solution with 
integration for analytics, mobile, and cloud that supports thousands of customers and millions of users. Cloud pack 
four data is a fully integrated data and AI platform that you can use to work with and manage all of your data. It 
runs on Red Hat OpenShift in a container, so you can deploy it on any private, public, or hybrid cloud. Using Cloud 
Pak four data, you can connect to Db2 or any other data source wherever it may be stored. Use the Watson Knowledge 
catalog to organize your data, work with a range of analytics services to gain insight on your data, and use Watson 
and other services to infuse AI into your systems. Db2 on Cloud is a great way to get started with Db2. It offers 
three plans for lite, standard, and enterprise. The lite plan is free and time unlimited, meaning that you can use it 
in your projects without worrying about a time-limited trial period coming to an end. The plan is limited to 200 
megabytes of data and 15 simultaneous connections. The standard plan provides flexible scaling of compute capability 
and storage as well as built-in three-node high availability clustering. And the enterprise plan provides you with a 
dedicated database instance. Again, with flexible scaling of compute capability and storage and built-in three-node 
high availability clustering. You can deploy Db2 on Cloud on the IBM Cloud platform or on Amazon Web services. Once 
running, you can access your Db2 on cloud databases by using the CLPPlus command line interface, the Db2 on cloud GUI 
console, or standard APIs such as ODBC, JDBC, and rest. You can also easily load data from Excel, CSV, and text files 
from Amazon S3 object storage using the GUI console, and you can programmatically load data from IBM cloud object 
storage. Db2 provides high availability, disaster recovery, or HADR functionality to support high availability 
systems. HADR replicates changes made at a primary database to multiple standby servers. If the primary database fails 
for any reason, hardware, software, or network issues, you can automatically promote one of the standby databases to 
be the primary database. Redirect client applications to this new primary database, and continue to replicate to the 
other standby servers in the group. When the original primary database comes back online, it can either take the place 
of a standby server or be promoted back to the primary position. Db2 warehouse offers massively parallel processing 
and data analytics for BI workloads. At times, you may need to scale the storage capabilities of your system to meet 
peak demand or to reduce costs when demand is low. Data in Db2 warehouse is stored in data nodes. To scale up your 
storage capacity, you just need to add a node to your deployment. The partitions and their workloads are then 
automatically rebalanced across the new node setup. Similarly, to scale down, you just remove a node to return to 
your original state.

what is mysql? mysql1 mysql2 mysql3 mysql4; The default MySQL storage engine is InnoDB mysql6; also supports MyISAM engine 
mysql7 mysql8 mysql9 mysql10 mysql11 mysql12

what is postgresql? originates from the POSTGRES project at the University of California more than 30 years ago. 
POSTGRES was used for many research and production applications across a range of industries, including financial 
services, aviation, and medicine. In 1994, the open source Postgres95 was released, which included an SQL language 
interpreter. This was soon renamed PostgreSQL, and is today generally pronounced as simply Postgres. You can use it 
as part of the LAPP, Linux, Apache, PostgreSQL, and PHP stack for web applications and websites. And you can also use
independently developed extensions for additional functionality such as PostGIS for geographic and spatial data 
handling. see postgresql1

When using Postgres, you can use all the standard relational database constructs such as keys, transactions, views, 
functions, and stored procedures. And you can also use some of the NoSQL functionality such as JSON for structured 
data and HSTORE for non-hierarchical data. see postgresql2 postgresql2 

For even greater flexibility for scaling applications, you can use commercial additions such as EDB PostgreSQL 
replication server, which provide multi-master read/write replication. This enables you to run multiple read/write 
databases that all replicate changes with each other. If one fails, users can easily be redirected to another 
instance until it is available again. see postgresql4

Other technologies that have been added to PostgreSQL in recent releases to enhance scalability and working with 
larger data sets include partitioning, which enables you to split a large table into multiple smaller sections or 
partitions to improve query performance. And sharding, which enables you to store horizontal partitions across 
multiple remote servers. 

In the world of managing databases, it's super important to understand some fancy concepts to make databases work 
well. Three big ones are functional dependencies, multi-valued dependencies, and candidate keys. These are like the 
building blocks for organizing and speeding up databases.

1. Functional Dependencies (FDs)

FDs are fundamental building blocks for ensuring data integrity and consistency in relational databases. They 
represent a specific type of relationship between attributes in a relation, where the value of one attribute 
(determinant) uniquely determines the value of another attribute (dependent). In simpler terms, if you know the value 
of the determinant, you can always identify the exact value of the dependent attribute.

For example, if you know an employee's ID number, you can look it up in the table and find out their first and last 
name. In this case, the employee ID "determines" the first and last name. This is a functional dependency. Think of it
as a special kind of connection between pieces of information. The determining attribute acts like a key that unlocks
the related information.

Properties of FDs:

- Notation: FDs are typically written as X -> Y, where X is the determinant and Y is the dependent attribute.

Properties:

- Reflexivity: X -> X (every attribute determines itself).
- Transitivity: If X -> Y and Y -> Z, then X -> Z (dependencies can chain together).
- Closure: The minimal set of FDs that implies all other FDs in a relation.

Key points

- FDs are essential for maintaining data accuracy and minimizing redundancy.
- They play a crucial role in database normalization, ensuring the efficient organization of data.
- FDs help in eliminating unnecessary repetition and ensuring the correctness of data.

2. Multi-Valued Dependencies (MVDs)

MVDs are more complex than FDs. With an MVD one attribute, the determinant, determines a set of possible values for 
another attribute, the dependent. In other words, knowing the value of the determinant narrows down the potential 
values the dependent attribute can hold.

Here's how it works:

- If you know an employee's ID (let's say ID 123), you can't just look it up and find their one assigned project.
- With MVD, knowing ID 123 tells you there will be multiple rows in the table, each listing a different project that 
employee 123 works on.
- So, the employee ID "multi-determines" the project because it doesn't give you a single answer, but lets you know 
there are potentially several project assignments for that employee in separate rows. This is different from a regular 
dependency, where one piece of information leads to one or two specific others.

Properties of MVDs:

Notation: 

- MVDs are written as X -> {Y1, Y2, …, Yn}, where X is the determinant and Y1, Y2, …, Yn are the possible values for the dependent attribute.

Properties:

- MVDs have similar properties to FDs, like reflexivity and transitivity. However, they lack the closure property.
- dentifying MVDs helps understand the complex relationships between attributes and ensures data consistency within those relationships. Violations of MVDs can lead to invalid data entries.

Key points:

- MVDs are essential for organizing complex relationships between sets of attributes.
- They help in avoiding mix-ups and ensuring proper organization of data.
- Understanding MVDs is crucial for maintaining data integrity and optimizing database performance.

3. Candidate Keys

Candidate keys uniquely identify each row in a relation. They represent a minimal set of attributes that collectively 
guarantee no duplicates exist in the table. In other words, if you know the values of the candidate key, you can 
pinpoint a specific row and only that row. The keys are unique within that table.

Imagine going back to the basic table with just employee ID, first name, and last name.

- An employee ID is a strong candidate key on its own. Why? Because every employee has a unique ID, knowing the ID 
instantly tells you which record belongs to that particular employee.
- Here's the twist: This table might have another candidate key! Think about it: what if two employees have the same 
first and last name (less common, but possible)? In that case, the combination of first name and last name wouldn't 
be enough to distinguish between them.
- But the employee ID, by itself, will always be unique and pinpoint the exact employee. That's why a single 
attribute (employee ID) can be a candidate key in this scenario.

Key points:

- Uniqueness: Each combination of values in the candidate key must uniquely identify a distinct row.
- Minimality: No proper subset of the candidate key should be able to uniquely identify a row. This ensures that the 
candidate key is the smallest possible set of attributes needed for unique identification.
- Performance: Having well-defined candidate keys significantly improves query performance. Queries searching for 
specific rows can utilize indexing on the candidate key for faster data retrieval. It also helps maintain data 
integrity by preventing duplicate entries.
- Multiple keys: A relation can have multiple candidate keys, meaning different minimal sets of attributes can 
uniquely identify each row.

Here's the important thing: A table can have multiple candidate keys. As long as a set of attributes guarantees 
finding one specific employee record and no others, it qualifies as a candidate key. The key thing is that it 
uniquely identifies a row in the table.

Practical scenario

Let's consider a hypothetical scenario where we have a database for tracking employee projects. Each employee can work 
on multiple projects, and each project can involve multiple employees. We'll create a table to represent this 
scenario:

EmployeeID	ProjectID	EmployeeName	ProjectName	    Department
1	        101	        Alice	        Project X	    HR
1	        102	        Alice	        Project Y	    Finance
2	        101	        Bob	            Project X	    HR
3	        101	        Charlie	        Project X	    IT
3	        102	        Charlie	        Project Y	    Finance

In this table:

EmployeeID: Unique identifier for each employee.
ProjectID: Unique identifier for each project.
EmployeeName: Name of the employee.
ProjectName: Name of the project.
Department: Department to which the project belongs.

Functional Dependencies (FDs)

EmployeeID -> EmployeeName

- Knowing the EmployeeID uniquely determines the EmployeeName.
- For instance, if you look up EmployeeID 1 in the table, it uniquely identifies the EmployeeName as "Alice".
    EmployeeID -> EmployeeName
    1 -> Alice

ProjectID -> ProjectName

Knowing the ProjectID uniquely determines the ProjectName.
In the table, ProjectID 101 corresponds to "Project X" and ProjectID 102 corresponds to "Project Y", demonstrating a one-to-one relationship.
    ProjectID ↔ ProjectName
    101 ↔ Project X
    102 ↔ Project Y


Multi-Valued Dependencies (MVDs)

{EmployeeID} ->> {ProjectID}

- Knowing the EmployeeID tells us all the projects that employees are working on, independent of each other.

Let's revisit the example of EmployeeID 1 (Alice) from the table:

EmployeeID 1 is associated with ProjectID 101 (Project X) in the HR department.
EmployeeID 1 is also associated with ProjectID 102 (Project Y) in the Finance department.

EmployeeID – {associated with} –> ProjectID (department)
1———–> 101 (HR)
1———–> 102 (Finance)

Knowing only EmployeeID 1 doesn't tell you which specific project Alice works on. It indicates there are potentially 
several project assignments for her, reflected in separate rows in the table. This highlights the many-to-many 
relationship between EmployeeID and ProjectID.

Candidate Keys

There are two candidate keys (CKs):

EmployeeID: As discussed earlier, each employee has a unique identifier (EmployeeID) that pinpoints their specific 
record. You can identify any employee solely based on their EmployeeID, making it a candidate key.

EmployeeID -> EmployeeName
1 -> Alice

Combination of EmployeeID, ProjectID: This might seem surprising, but consider a scenario where every employee has a 
distinct name within a project (no duplicates within projects). In that case, the combination of EmployeeName and 
ProjectID would uniquely identify each employee record.

EmployeeID and ProjectID determine the Department
EmployeeID -> ProjectID ==> Department
1 -> 101 => HR

For instance, if Alice was assigned to Project X and Charlie to Project Y (assuming unique names within projects), 
then the combination of "John" and "Project X" or "Mary" and "Project Y" would uniquely identify their respective rows.

Summary

Aspect	    Functional Dependencies	                            Multi-Valued Dependencies (MVDs)	                    Candidate Keys
Definition	Describe relationships between attributes	        Extend the concept to groups of attributes	            Sets of attributes uniquely identifying rows
Essence	    Values of certain attributes determined by others	Dependencies between sets of attributes	                Uniquely identify each row in a table
Example	    Knowing one attribute allows finding another	    Describe how sets of attributes determine each other	Combination of attributes uniquely identifying each record
Purpose	    Ensure data accuracy and minimize redundancy	    Organize data effectively, avoiding mix-ups	            Enforce entity integrity constraints
Usage	    Essential for database normalization	            Crucial for maintaining data integrity	                Establish relationships between tables

Now, let's apply the concepts learned in this module to a real-world example of a database.

After completing this reading, you will be able to evaluate your knowledge of Advanced relational model concepts.

Here you are going to:

Apply advanced relational concepts like functional dependencies, multi-valued dependencies, and candidate keys to the
"Car Dealership" database schema.
Identify constraints within the schema based on these concepts.
Understand the impact of these concepts on data integrity and manipulation.

Exercise

In this exercise, we will work on a relational database schema called Car Dealership, designed to keep track of 
automobile sales in a car dealership.

Schema diagram for the Car Dealership relational database see Car_dealership 1.png

Relational instance of SALE: Car_dealership 2.png

Now, let's go through some questions based on the above database schema of Car Dealership and the relational instance of SALE:

1. Identify FDs in the Car Dealership schema:

A. Analyze each pair of attributes in each relation (Car, Sale, Salesperson, Customer).

B. For each pair, consider if the value of one attribute always determines the value of the other.

C. List all identified FDs for each relation.

Car:

Serial_no -> (Model, Manufacturer, Price)

Model -> Manufacturer

Sale:

Serial_no -> Date

Serial_no -> Sale_Price

Salesperson:

Salesperson_id -> Name

Salesperson_id -> Phone

2. Explore MVDs:

A. Consider if any attribute in the schema determines a set of possible values for another.

B. For example, does "Car Model" determine a set of possible values for "Sale Price"?

C. List any identified MVDs for the schema.

No MVDs are explicitly identified in the given schema.

3. Determine candidate keys:

A. Analyze each relation and identify any subset of attributes that uniquely identifies each row.

B. Remember, a candidate key must not contain any redundant attributes.

C. List all identified candidate keys for each relation.

Car: Serial_no

Sale: Serial_no

Salesperson: Salesperson_id

4. Discuss the implications:

A. How do the identified FDs and MVDs impact data integrity and manipulation in the schema?

B. Could any data inconsistencies arise due to violating these constraints?

C. How do candidate keys affect query optimization and data retrieval?

A. Data Integrity: The identified FDs and candidate keys help ensure data integrity by preventing inconsistencies:

Changing Serial_no automatically updates dependent attributes in Car.

Serial_no and Date prevents duplicate sales and ensures association with correct car and salesperson.

Unique keys (Serial_No, Salesperson_id ) guarantee distinct, identifiable entities.

Data Manipulation: FDs guide proper data updates. Modifying VIN requires cascading changes to dependent attributes.

B. Potential Inconsistencies due to Constraint Violations:

Incorrect data updates: Forgetting to update dependent attributes when modifying a determining attribute (e.g., changing Serial_no without updating Model) can lead to inconsistencies.

Duplicate data: if Serial No doesn't determine Price, multiple entries with the same car could have different prices.

Inaccurate queries: A salesperson selling a non-existent car if Salesperson_id doesn't determine Serial_no.

C. Effects on query optimization and data retrieval:

Impact of candidate keys: Unique candidate keys (Serial_no for Car, Serial_no for Sale, etc.) significantly improve query performance by creating efficient indexing mechanisms.

Fast lookups: Queries using candidate keys can quickly locate specific rows without scanning the entire table, resulting in faster data retrieval.

Reduced redundancy: Candidate keys eliminate redundant identifiers, resulting in smaller data storage requirements and potentially faster table scans for non-indexed searches.

SQL Statements are used for interacting with Entities (that is, tables), Attributes (that is, columns) and their tuples
(or rows with data values) in relational databases. SQL statements fall into two different categories: Data Definition 
Language statements and Data Manipulation Language statements. 

Data Definition Language (or DDL) statements are used to define, change, or drop database objects such as tables. 
Common DDL statement types include CREATE, ALTER, TRUNCATE, and DROP. CREATE: which is used for creating tables and 
defining its columns; ALTER: is used for altering tables including adding and dropping columns and modifying their 
datatypes; TRUNCATE: is used for deleting data in a table but not the table itself; DROP: is used for deleting tables.
see DDL1.png

Data Manipulation Language (or DML) statements are used to read and modify data in tables. These are also sometimes 
referred to as CRUD operations, that is, Create, Read, Update and Delete rows in a table. Common DML statement types 
include INSERT, SELECT, UPDATE, and DELETE. INSERT: is used for inserting a row or several rows of data into a table; 
SELECT: reads or selects row or rows from a table; UPDATE: edits row or rows in a table; And DELETE: removes a row or 
rows of data from a table. see DML1.png

what about creating tables? see creating-tables1 creating-tables2 creating-tables3 creating-tables4 creating-tables5 
creating-tables6 creating-tables7 creating-tables8 creating-tables9 creating-tables10 creating-tables11 
creating-tables12 creating-tables13 creating-tables14 creating-tables15 creating-tables16 creating-tables17
creating-tables18 creating-tables28 creating-tables19 creating-tables20 creating-tables21 creating-tables22 creating-tables23
creating-tables24 creating-tables25 creating-tables26 creating-tables27

Data engineers and database administrators often need to move data into and out of an existing database. There are 
multiple reasons for this action. These include, populating the database and its objects like tables, creating a 
duplicate database for development and testing needs. Creating a snapshot of the state of the database for disaster 
recovery, generating a new table using data from external sources or files, adding data into one or more existing 
tables. see data-movement1

Most databases support different ways to move data in and out of the database using multiple file formats. Each 
database has its tools and utilities for data movement, but for this lesson we will classify them into three 
categories, backup and restore, import and export, and load. see data-movement2 data-movement3 data-movement4 
data-movement5 data-movement6 (create copies of the database for development and test purposes) data-movement7
data-movement8 data-movement9 data-movement10 data-movement11 data-movement12 data-movement13

Using SQL statements such as insert to add data into tables can work well for a few rows or when you are developing 
and testing your database systems. However, it's often not practical to use this method to load hundreds or thousands 
of rows of data. see loading-data1

Instead, most relational database management systems, or RDBMS, provide a method of directly loading large amounts of 
data into tables in a quick, efficient, and scalable manner. Your data might originate from a range of different 
sources in a variety of formats. For example, you might want to populate your table with information exported from 
another database into limited text files or object data output from a bespoke application. see loading-data2
loading-data3

hands on lab: Create Tables and Load Data in Datasette

Objectives

After completing this lab, you will be able to:

Create and load data into a table from a CSV file
Create and load data into a table from a SQL script file

What did you learn?

- how to load a csv file from n S3 bucket into a dataset in the datasette tool, fairly simple and straightforward.
- loaded data from the below SQL script file

-- Drop the tables in case they exist

DROP TABLE IF EXISTS BookShop;
DROP TABLE IF EXISTS BookShop_AuthorDetails;

-- Create the table

CREATE TABLE BookShop (
	BOOK_ID VARCHAR(4) NOT NULL, 
	TITLE VARCHAR(100) NOT NULL, 
	AUTHOR_NAME VARCHAR(30) NOT NULL, 
	AUTHOR_BIO VARCHAR(250),
	AUTHOR_ID INTEGER NOT NULL, 
	PUBLICATION_DATE DATE NOT NULL, 
	PRICE_USD DECIMAL(6,2) CHECK(Price_USD>0) NOT NULL
	);

-- Insert sample data into the table

INSERT INTO BookShop VALUES
('B101', 'Introduction to Algorithms', 'Thomas H. Cormen', 'Thomas H. Cormen is the co-author of Introduction to Algorithms, along with Charles Leiserson, Ron Rivest, and Cliff Stein. He is a Full Professor of computer science at Dartmouth College and currently Chair of the Dartmouth College Writing Program.', 123 , '2001-09-01', 125),
('B201', 'Structure and Interpretation of Computer Programs', 'Harold Abelson', 'Harold Abelson, Ph.D., is Class of 1922 Professor of Computer Science and Engineering in the Department of Electrical Engineering and Computer Science at MIT and a fellow of the IEEE.', 456, '1996-07-25', 65.5),
('B301', 'Deep Learning', 'Ian Goodfellow', 'Ian J. Goodfellow is a researcher working in machine learning, currently employed at Apple Inc. as its director of machine learning in the Special Projects Group. He was previously employed as a research scientist at Google Brain.', 369, '2016-11-01', 82.7),
('B401', 'Algorithms Unlocked', 'Thomas H. Cormen', 'Thomas H. Cormen is the co-author of Introduction to Algorithms, along with Charles Leiserson, Ron Rivest, and Cliff Stein. He is a Full Professor of computer science at Dartmouth College and currently Chair of the Dartmouth College Writing Program.', 123, '2013-05-15', 36.5),
('B501', 'Machine Learning: A Probabilistic Perspective', 'Kevin P. Murphy', '', 157, '2012-08-24', 46);

-- Retrieve all records from the table

SELECT * FROM BookShop;

At this point, you know that:

DDL (Data Definition Language) statements, such as CREATE, ALTER, and DROP, are used for defining, modifying, and 
deleting objects in a database, like tables and their attributes.

When creating tables, key considerations involve selecting a schema, defining columns with names, datatypes, and 
optional values like primary key constraints, and referencing an Entity Relationship Diagram (ERD).

Methods for creating tables include using visual interfaces like the Db2 on Cloud console, SQL statements, and 
administrative APIs.

Post-creation actions may include generating SQL code, modifying table structure, exploring dependencies, and 
dropping the table if necessary.

Data manipulation involves DML (Data Manipulation Language) statements for working with data within tables, such as 
inserting, updating, deleting, or querying records.

Data movement tasks include populating databases initially, adding or appending data, making copies for development 
tests or disaster recovery, and ensuring scalability.

Utilities like BACKUP and RESTORE are used to create and recover copies of entire databases, including tables, views, 
constraints, and their data.

The IMPORT utility facilitates inserting data into tables from different formats like DEL/CSV (delimited CSV), ASC, and IXF, while 
the EXPORT utility saves table data into various formats like CSV.

LOAD utilities enable high-performance insertion of large volumes of data into specified tables, offering quicker and 
more efficient data loading compared to multiple INSERT statements.

Loading data can be done from various sources, including delimited text files and Cloud Object Storage, and can be 
managed through user-friendly interfaces like the Load Data utility in the Db2 Web Console.

Relational database management systems RDBMSs contain many objects that database engineers and database administrators 
must organize. Storing tables, constraints, indexes, and other database objects in a hierarchical structure allows 
database administrators to manage security, maintenance, and accessibility. This hierarchy gives you an overview of 
how RDBMSs are structured. Although slight variations may occur between products, most RDBMSs begin with an instance, 
a single way of organizing the database and everything it contains. Many RDBMSs permit more than one database within a 
single instance. You will generally find at least one schema at some level in the hierarchy. A schema is a logical 
grouping of objects within a database. Schemas define how database objects are named and prevent ambiguous references. 
Some RDBMSs consider the schema a parent object of a database, and others consider it a database object. There are 
database objects within a schema, including tables, constraints, and indexes. An instance is a logical boundary for a 
database, or set of databases where you organize database objects and set configuration parameters. Every database 
within an instance is assigned a unique name, has its own set of system catalog tables, which keep track of the objects 
within the database, and has its own configuration files. You have the option to generate multiple instances on a 
single physical server, ensuring a distinct database server environment for each instance. The databases and other 
objects within one instance are isolated from those in any other instance. It is possible to establish multiple 
instances on a single physical server, thereby creating distinct database server environments for each instance. Not 
all RDBMSs use the concept of instances, often managing database configuration information in a special database 
instead.

In cloud based RDBMs, the term instance means a specific running copy of a service. A relational database comprises 
objects designed for the storage, management, and retrieval of data. These objects include tables, views, indexes, 
functions, triggers, and packages. Database objects can be either defined by the system built-in objects, or defined 
by the user; User defined objects. in a relational database, database engineers establish relationships between tables 
to reduce redundant data and improve data integrity. A distributed relational database shares tables and other objects 
across different but interconnected computer systems. A schema is a specialized database object that provides a way 
to group other database objects logically. A schema has the capability to include tables, views, nicknames, triggers, 
functions, packages, and various other objects. When you create a database object, you can assign it to a schema. If 
you want to assign the object to a specific schema, you can explicitly include the schema name. If you don't include 
the schema name, the object is implicitly assigned to the current schema. In most RDBMSs, the default schema is the 
user schema for the currently logged on user. A schema also provides a naming context, using the schema name as a name 
qualifier enables you to distinguish between objects with the same name in different schemas. For example, the schema 
names internal and external make it easy to distinguish two different sales tables, internal sales in the internal 
schema, and external sales in the external schema. Thus, schemas enable multiple applications to store data in a 
single database without encountering namespace conflicts. Many RDBMSs use a specialized schema to hold configuration 
information and metadata about a particular database. For example, tables in a system schema can store lists of 
database users and their access permissions, information about the indexes on tables, details of any database 
partitions that exist, and user-defined data types. A partitioned relational database is one where data is distributed 
and managed across multiple database partitions within the relational database system. You can partition tables that 
need to contain large quantities of data into multiple logical partitions, with each partition containing a subset of 
the overall data. Database partitioning is used in scenarios that involve large volumes of data, such as data 
warehousing and data analysis for business intelligence. Database objects encompass items present within the database. 
The process of database design includes defining database objects and their relationships with each other. In most 
RDBMSs, you can create objects such as tables, these are logical structures consisting of rows and columns that store 
data. Constraints within any business, data is often subject to certain restrictions or rules. For example, it is 
imperative for an employee number to be distinct or unique. Constraints provide a way to enforce such rules. Indexes, 
an index is a set of pointers used to improve performance and ensure the uniqueness of the data. Views, a view 
provides a different way of representing the data in one or more tables. It's important to note that a view is not an 
actual table, and does not demand permanent storage. Aliases, an alias serves as an alternative name for an object 
such as a table. It can be used to provide a shorter, simpler name to reference an object. You can create and manage 
database objects through graphical database management tools, scripting or accessing the database through an API. If 
you are using SQL to create or manage the object, you will use data definition language statements like create or 
alter. 

You can use the primary key to uniquely identify every row in a table. In some tables, the choice of the primary key 
is easy because it is a naturally occurring unique attribute of an entity. For example, the book_id of a book or the 
employee_id number of a staff member. If your table doesn't have an existing unique attribute, you can add a column 
to the table to serve as the primary key. Or if a combination of the two attributes uniquely identifies each row, you 
can create a primary key across the two columns. For example, where employees have a unique identifier within their 
work site, you can use the combination of their site_id and employee_id. Note that each table can only have one 
primary key. You can create a primary key when you create the table by using the PRIMARY KEY clause and the CREATE 
TABLE statement. In the parentheses for the PRIMARY KEY clause, state the name of the column or columns that will be 
the primary key see primary-key1. Alternatively, you can create a primary key on the existing table by using the ADD 
PRIMARY KEY clause of the ALTER TABLE statement see primary-key2. Again, in the parentheses state the name of the 
column or columns, that will be the primary key.

You use primary and foreign keys to define the relationships between your tables. A foreign key is a table column that 
holds information identical to the primary key in another table. For example, the copy table might list all books that 
the library owns. Therefore, the book_id of a copy of an individual book must exist in the book table as a valid book. 
see foreign-key1. When the library owns multiple copies of a popular book, the book_id of that particular book will 
appear multiple times in the copy table. You can also specify that whenever you add a row to the copy table, the 
book_id you use must already exist in the book table. Similar to primary keys, you can create a foreign key when you 
create the table by using the CONSTRAINT name FOREIGN KEY clause of the create table statement. In the parentheses for 
the FOREIGN KEY clause state the name of the column or columns that will be the foreign key and then reference the 
table and primary key column that the foreign key links to see foreign-key2. You can also use the rule clause to 
define what action to take if a row in the parent table, the table with the primary key, is updated or deleted. For 
updates and deletes, you can specify to take no action, in which case the update or delete option on the parent table 
may fail see foreign-key3. For deletions, you can also specify to cascade the delete, that is to delete the related 
child row in the dependent table or to set the values in the foreign key column of the child table to null see 
foreign-key4. Alternatively, you can create a primary key on an existing table by using the ADD PRIMARY KEY clause of 
the alter table statement. Again, in the parentheses state the name of the column or columns, that will be the 
primary key. Note to use a comma to separate the column names when using multiple columns for a primary key.

Imagine you have a library with a collection of books. If you want to find a specific book, you could go through each 
shelf individually until you find it. However, this approach can be quite time consuming and inefficient. A better way 
to locate the book would be to use the library's catalog. The catalog is an index of all the books in the library, and 
it lists each book by its title, author, and other relevant information. 

When you use the catalog to find a book, you navigate through the index to locate its entry. Indexing a table in a 
database works similarly. An index is a data structure that allows you to find specific rows in a table based on 
certain criteria quickly. For example, if you have a customer data table, you could create an index in the customer's 
name. Column indexing would allow you to find all the table rows belonging to a specific customer quickly. Here are 
some real world examples of how databases use indexing. Some of these examples include online shopping websites, 
airline reservation systems, and bank ATM's. When you search for a product on an online shopping website, the website 
uses indexes to locate products that align with your search criteria efficiently. When you search for a flight on an 
airline website, the website uses indexes to find the flights that meet your criteria quickly. Lastly, when you 
withdraw money from an ATM, the ATM uses indexes to retrieve your account information quickly. 

Indexes can significantly improve the performance database queries. They allow the database to quickly find the 
relevant rows to the query without scanning the entire table. Usually, when you add data to a table, it is added to 
the end of the table. However, this action has no guarantee or inherent order to the data. When you select a 
particular row from that table, the processor must check each row until it finds the one you want. On a large table, 
it can significantly slow down the process of locating a specific row. Also, when you select multiple rows, the result 
may lack a specific order unless you specify a sort order in your select statement. As you might want to return the 
rows in a particular order or select a subset of sequential rows, you can create an index on a table to locate the 
row or set of rows that you need quickly. An index works by storing pointers to 
each row in the table, so that when you request a particular row, the SQL processor can use the index to locate the 
row quickly. It is similar to how you use the index in a book to find a particular section quickly. The unique key 
dictates the order of the index on which values rely see index-1. By default, when you create a primary key on a table, 
the system automatically generates an index on that key. However, you can also create your indexes on regularly 
searched columns see index-2. Use the create index statement to define the index, specifying the index name, its 
uniqueness, and the table and column on which to base it. Indexes provide the database user with many benefits. These
include improved performance of select, reduced need to sort data, and guaranteed uniqueness of rows. Improved 
performance of select queries when searching on an indexed column. Because the index offers a quick route to locate 
rows matching the search term, it 
returns results faster than when it needs to check every row in the table. Reduced need to sort data. If you 
constantly need rows in a specific order, employing an index can eliminate the necessity to sort the rows after their 
retrieval. Guaranteed uniqueness of rows. If you use the unique clause when you create the index, you ensure that 
updates and insertions won't result in duplicate entries in that column. There are, however, a few disadvantages of 
indexes as well. These include use of disk space. Each index you create uses disc space in the same way that adding 
indexes increases the number of pages in a book. Decreased performance of insert, update, and delete queries. Because 
the rows in an indexed table are sorted according to the index, adding or removing rows can take longer than a non 
indexed table.

When you keep records of data such as books in a bookshop, you will inevitably have some inconsistencies and duplicate 
information. Such duplication can cause extra work and inconsistencies if the data is updated, because you must change 
it in more than one place. Normalization is the process of organizing your data to reduce redundant data, often by 
dividing larger tables into multiple relatable tables. Normalization helps speed up transactions because you perform 
updates, additions, and deletes only once on a normalized database. It also improves data integrity because it reduces 
the chance of a change being made in one place but not in another. As you begin the normalization process, it's 
important to recognize that you focus on normalizing each table until you've reached the required normal form level.

Normalization usually results in creating more tables, and once all the tables are normalized, you will have a 
normalized database. There are several forms of normalization, and most data engineers will need to be familiar with 
the first normal form, second normal form, and third normal form. For a table to be in the first normal form, each 
row must be unique and each cell must contain only a single value. First normal form is also called 1NF. Let's look 
at how you would normalize a simple table. In this example, the book table contains some basic information about books, 
including titles, formats, and authors see normalisation1. To adhere to the requirements of the first normal form, it is essential that 
each cell contains a singular value rather than a list. In this example, you can observe that all variations of a book's 
format are recorded within the same cell. To normalize this table, you can add an extra row and split the two formats 
of patterns and software into their own row see normalisation2. So now you have a row for the paperback version and a row for the 
hardback version. Each cell in the table now has only one entry, and so the table is in the first normal form.

To be in the second normal form, the database must already be in first normal form, which involves ensuring that every 
row in the table is unique, and that each cell contains only a single value. Second normal form specifies that you 
should separate groups of values that apply to multiple rows by creating new tables. Second normal form is also 
referred to as 2NF. For clarity, this example shows just a subset of the data in the book table see normalisation3. 
Book 401 comes in both paperback and hardback format, so in its current form, it must be listed twice, once for each 
format. In this case, the format column contains values that apply to both rows that reference book 401, so there is 
some data duplication. To meet the requirements for the second normal form and achieve just one row for book 401, 
You can split the book table so that the format information for the book is separated from unrelated information such 
as title and author, each resulting table is in 1NF see normalisation4. To maintain a relationship between the two 
tables, identify a primary key for one table that will be used as a foreign key in the other. In this example, the 
book ID is unique to each book, so you can make that the primary key in the book table and include it as a foreign key 
in the format table see normalisation5. Now you can use it to link between the two tables to find the different formats 
of each of the unique books. The database must already be in the first and second normal forms to meet the requirements 
for the third normal form. 

Next, you must eliminate any columns that do not depend on the key. Third normal form is also referred to as 3NF. 
Let's consider some additional data about books, the publisher, and where the book ships from. Each publisher ships 
books from warehouses in their own location. So where the book ships from depends on the publisher, not the book ID. 
see normalisation6. Therefore, the book table is not in 3NF because the ships from data does not depend on the primary 
key. To fulfill the criteria of the third normal form, 3NF, it is necessary to segregate the publisher and ship's 
details into a dedicated publisher's table see normalisation7. Both tables are now in third normal form, which is as 
far as most relational databases go. There are also higher normal forms such as Boyce-Codd normal form, or BCNF, which 
is an extension to the third normal form, as well as fourth and fifth normal forms, which may be needed for specific 
scenarios. In transactional systems, OLTP, where data is both read and written frequently, you typically normalize 
the data to 3NF. OLTP systems need to process and store transactions as well as query transactional data, and 
normalizing the data to 3NF helps the database to process and store individual transactions efficiently. In analytical 
OLAP systems, where users primarily read data, databases prioritize read performance over write integrity. Hence, the 
data may have undergone some denormalization to a lower normal form before being loaded into the analytical system, 
such as a data warehouse. In data warehousing, data, engineers focus on performance, which can benefit from having 
fewer tables to process.

In a business, a data relational model needs to adhere to certain restrictions or rules. In a relational data model, 
data integrity can be implemented using integrity rules or constraints. The following six constraints are defined in 
a relational database model, entity integrity constraint, referential integrity constraint, semantic integrity 
constraint, domain constraint, null constraint, and check constraint. To identify each tuple in a relation, the 
relation must have a primary key. The primary key is a unique value that identifies each tuple or row in a table. This 
is the entity integrity constraint. The terms primary key constraint or unique constraint are also used. This 
constraint prevents duplicate values in a table. To implement these constraints, indexes are used. According to the 
entity integrity constraint, no attribute that is a part of the primary key of a relation should have null values. The 
value null indicates that the value is unknown. In the entity integrity constraint, the primary key cannot have an 
unknown value. For example, in the relation author, author ID is the primary key. The primary key identifies each 
tuple in the relation. The author ID A1, points to author Raul Chong from Toronto. If you replace the value A1 with 
null, you can still identify the author as Raul Chong. However, if you also replace author ID A4 with null, now you 
do not know which null value identifies which tuple see constraints1. With the entity integrity constraint, no 
attribute participating in the primary key is allowed to accept null values. Relationships between tables are defined 
by the referential integrity constraint. This constraint endorses the relationships between tables. The validity of 
the data is enforced using a combination of primary keys and foreign keys. As mentioned previously, for a book to 
exist, it has to be written by at least one author. The correctness of the data in a table is enforced by the semantic 
integrity constraint. For example, in the relation author, if the attribute or column city contains a garbage value 
instead of Toronto, the garbage value does not have any meaning see constraints2. The semantic integrity constraint is
related to the correctness of the data. A domain constraint checks for valid values for a given attribute. For example, 
in the relation author, the attribute country must contain a two letter country code, such as CA for Canada or IN for 
India. If a number value of 34 is entered for the country attribute, instead of a two-letter country code, the value 
34 does not have any meaning see constraints3. As observed earlier, the entity integrity constraint states that no 
attribute that is part of the primary key should accept null values. The null constraint ensures that attribute values 
should not be null. For example, in the relation author, if either last name or first name contains a null value, it 
could be difficult to identify the correct author see constraints4. In this example, first name and last name 
attribute values cannot be null. An author must have a name. The check constraint enforces domain integrity by 
limiting the values that are accepted by an attribute. In the relation book, the attribute year is the year in which 
a particular book is published. If this was still the year 2010, it would not be meaningful to have a year greater 
than the current year. The check constraint would enforce the domain integrity by limiting the values that are 
accepted by the attribute year see constraints5.

Hands-on Lab: Normalization, Keys, and Constraints in Relational Database

Instructions

In this lab, you will explore normalization, keys, and constraints in Datasette. Initially, you will learn how to 
minimize data redundancy and inconsistency in a database by normalizing tables. Next, you will learn how to use keys 
to uniquely identify a record in a table, to establish a relationship between tables, and to identify the relation 
between them. Finally, you will learn about different kinds of relational model constraints that help to maintain 
data integrity in a relational data model.

Objectives

After completing this lab, you will be able to:

Minimize data redundancy and inconsistency in a database by using normalization.
Use keys to uniquely identify a record in a table, establish a relationship between tables, and identify the 
relation between them.
Maintain data integrity in a relational data model using constraints.

Exercise 1: Normalization

In this exercise, you will learn about first normal form (1NF) and implement second normal form (2NF).

Task A: First normal form (1NF)

In this task of normalization, you will be working with the BookShop table. The following image shows the 
BookShop table: see BookShop_table_Not_1NF

You will answer some questions to determine if the table above is in 1NF.

Does the above table have unique rows? Yes, you can uniquely identify each row.

Does each cell of the above table have single/atomic value? No. The columns AUTHOR_NAME and AUTHOR_ID contain multi 
valued cell.

By definition, a table is in 1NF if every attribute in that relation contains single valued data and every tuple in 
that relation is unique. Does the above table fall in first normal form? No, the table is not in 1NF since it has 
unique rows but not all single valued cell.

If your answer to question 3 is No, how can you normalize the table to ensure first normal form? To normalize this 
table, add an extra row, and split the multiple author names as well as multiple author IDs of the row containing 
multi-valued data into their own row.

Task B: Second normal form (2NF)

Download the BookShop-CREATE-INSERT.sql script below, copy and paste it to the Datasette lab, and run it. The script 
will drop any previous BookShop table that exists, create the new BookShop table, and populate it with the sample 
data required for this lab. see BookShop_table

By definition, a relation is in second normal form if it is already in 1NF and does not contain any partial 
dependencies. If you look at the BookShop table, you will find every column in the table is single or atomic valued, 
but it has multiple books by the same author. This means that the AUTHOR_ID, AUTHOR_NAME and AUTHOR_BIO details for 
BOOK_ID B101 and B401 are the same. As the number of rows in the table increase, you will be needlessly storing more 
and more occurrences of these same pieces of information. And if an author updates their bio, you must update all of 
these occurrences. see exercise1_B_2a

In this scenario, to enforce 2NF you can take the author information such as AUTHOR_ID, AUTHOR_NAME and AUTHOR_BIO 
out of the BookShop table into another table, for example a table named BookShop_AuthorDetails. You then link each 
book in the BookShop table to the relevant row in the BookShop_AuthorDetails table, using a unique common column such 
as AUTHOR_ID to link the tables. To create the new BookShop_AuthorDetails table, copy the code below and paste it to 
datasette text area. Click on Submit query button.

Now you are only storing the author information once per author and only have to update it in one place; reducing 
redundancy and increasing consistency of data. Thus 2NF is ensured.

Exercise 2: Keys

In this exercise, you will learn how to utilize a primary key to uniquely identify records in a table, use a foreign 
key to establish relationships between tables, and discern the relations between them.

Task A: Primary Key

By definition, a primary key is a column or group of columns that uniquely identify every row in a table. A table 
cannot have more than one primary key. The rules for defining a primary key include:

No two rows can have a duplicate primary key value.
Every row must have a primary key value.
No primary key field can be null.

To uniquely identify every row in the BookShop and BookShop_AuthorDetails tables, you will create a primary key. Set 
the BOOK_ID column of the BookShop table and the AUTHOR_ID column of the BookShop_AuthorDetails table as primary keys 
for their respective tables. Both columns were declared as NOT NULL when the tables were created (verify this in the 
SQL script or table definition). The BookShop_AuthorDetails table inherits data types and column constraints, 
including NOT NULL, from the BookShop parent table).

To set the BOOK_ID column of the BookShop table as a primary key for each of the tables, copy the code below and 
paste it to datasette text area. Click on Submit query button.

--Drop the table.

DROP TABLE IF EXISTS BookShop;

-----Recreate it with Primary Key -------

CREATE TABLE BookShop (
 BOOK_ID VARCHAR(4) NOT NULL, 
 TITLE VARCHAR(100) NOT NULL, 
 AUTHOR_NAME VARCHAR(30) NOT NULL, 
 AUTHOR_BIO VARCHAR(250),
 AUTHOR_ID INTEGER NOT NULL, 
 PUBLICATION_DATE DATE NOT NULL, 
 PRICE_USD DECIMAL(6,2) CHECK(Price_USD>0) NOT NULL,PRIMARY KEY (BOOK_ID));

INSERT INTO BookShop VALUES
('B101', 'Introduction to Algorithms', 'Thomas H. Cormen', 'Thomas H. Cormen is the co-author of Introduction to Algorithms, along with Charles Leiserson, Ron Rivest, and Cliff Stein. He is a Full Professor of computer science at Dartmouth College and currently Chair of the Dartmouth College Writing Program.', 123 , '2001-09-01', 125),
('B201', 'Structure and Interpretation of Computer Programs', 'Harold Abelson', 'Harold Abelson, Ph.D., is Class of 1922 Professor of Computer Science and Engineering in the Department of Electrical Engineering and Computer Science at MIT and a fellow of the IEEE.', 456, '1996-07-25', 65.5),
('B301', 'Deep Learning', 'Ian Goodfellow', 'Ian J. Goodfellow is a researcher working in machine learning, currently employed at Apple Inc. as its director of machine learning in the Special Projects Group. He was previously employed as a research scientist at Google Brain.', 369, '2016-11-01', 82.7),
('B401', 'Algorithms Unlocked', 'Thomas H. Cormen', 'Thomas H. Cormen is the co-author of Introduction to Algorithms, along with Charles Leiserson, Ron Rivest, and Cliff Stein. He is a Full Professor of computer science at Dartmouth College and currently Chair of the Dartmouth College Writing Program.', 123, '2013-05-15', 36.5),
('B501', 'Machine Learning: A Probabilistic Perspective', 'Kevin P. Murphy', '', 157, '2012-08-24', 46);

-- Retrieve all records from the table

SELECT * FROM BookShop;

To set the AUTHOR_ID column of the BookShop_AuthorDetails table as a primary key for each of the tables, copy the 
code below and paste it to datasette text area. Click on Submit query button.

--Drop the table.

DROP TABLE IF EXISTS BookShop_AuthorDetails;

-----Rereate another table BookShop_AuthorDetails with author id as the primary key

CREATE TABLE BookShop_AuthorDetails(AUTHOR_ID INTEGER NOT NULL,AUTHOR_NAME VARCHAR(30) NOT NULL,AUTHOR_BIO VARCHAR(250),PRIMARY KEY (AUTHOR_ID)) ;

-----Insert the records of Bookshop to this table.

insert into BookShop_AuthorDetails select DISTINCT AUTHOR_ID, AUTHOR_NAME, AUTHOR_BIO FROM BookShop;

select * from BookShop_AuthorDetails;

After adding primary key lets try add the same record in BookShop table, copy the code below and paste it to 
datasette text area. Click on Submit query button.

INSERT INTO BookShop VALUES
('B101', 'Introduction to Algorithms', 'Thomas H. Cormen', 'Thomas H. Cormen is the co-author of Introduction to Algorithms, along with Charles Leiserson, Ron Rivest, and Cliff Stein. He is a Full Professor of computer science at Dartmouth College and currently Chair of the Dartmouth College Writing Program.', 123 , '2001-09-01', 125)

---You will get unique constraint violation.

select * from BookShop;

Now you can use the BOOK_ID column to uniquely identify every row in the BookShop table and the AUTHOR_ID column to 
uniquely identify every row in the BookShop_AuthorDetails table.

Task B: Foreign Key

By definition, a foreign key is a column that establishes a relationship between two tables. It acts as a 
cross-reference between two tables because it points to the primary key of another table. A table can have multiple 
foreign keys referencing primary keys of other tables. Rules for defining a foreign key:

A foreign key in the referencing table must match the structure and data type of the existing primary key in the 
referenced table.
A foreign key can only have values present in the referenced primary key
Foreign keys do not need to be unique. Most often they are not.
Foreign keys can be null.

To create a foreign key for the BookShop table, set its AUTHOR_ID column as a foreign key, to establish a 
relationship between the BookShop and BookShop_AuthorDetails tables. Copy the code below and paste it to datasette 
text area. Click on Submit query button..

Note: ON DELETE clause along with Foreign key is used to configure actions that takes place while deleting rows from 
referencing table. ON UPDATE along with the foreign key are used to take the set actions while modifying the 
referencing key values of existing rows.

NO ACTIONS simply means that when a parent key is updated, modified or deleted from the database, there will be no 
special action taken.

If the configured action is set to RESTRICT then the application is prohibited for deleteing and modifying a parent 
key where one or more chil keys are already present.

On configuring the action to SET NULL when a parent key is deleted or updated then the column of all child keys that 
are mapped to parent key will set to contain SQL NULL values.

SET DEFAULT is similar to SET NULL except that the child keys columns will set to contain value as default instead of 
null.

Now that you have created the relationship, each book in the BookShop table is linked to the relevant row in the 
BookShop_AuthorDetails table through AUTHOR_ID.

Exercise 3: Constraints

In this exercise, you will review different kinds of relational model constraints crucial for maintaining data 
integrity in a relational data model.

Entity Integrity Constraint: Entity integrity ensures that no duplicate records exist within a table and that the 
column identifing each record within the table is not a duplicate and not null. The existence of a primary key in 
both the BookShop and BookShop_AuthorDetails tables satisfies this integrity constraint because a primary key 
mandates NOT NULL constraint as well as ensuring that every row in the table has a value that uniquely denotes the 
row.

Referential Integrity Constraint: Referential integrity ensures the existence of a referenced value if a value of one 
column of a table references a value of another column. The existence of the foreign Key (AUTHOR_ID) in the BookShop 
table satisfies this integrity constraint because a cross-reference relationship between the BookShop and 
BookShop_AuthorDetails tables exists. As a result of this relationship, each book in the BookShop table is linked to 
the relevant row in the BookShop_AuthorDetails table through the AUTHOR_ID columns.

Domain Integrity Constraint: Domain integrity ensures clarity of column purpose and consistency of valid values. The 
BookShop table adheres to this constraint through the specification of data types, length, date format, check 
constraints, and null constraints in its CREATE statement. This comprehensive approach guarantees that the values in 
each column are not only valid but also conform to the specified domain constraints.

what is mysql? see mysql-rdbms1 You can download and install the free community edition under a GNU, general public 
license and embed this in your applications. Alternatively, commercial editions like standard, enterprise, and cluster 
versions with added features are available for purchase. MySQL is also accessible in the cloud, allowing self management 
in virtual machines or containers. Managed services like IBM Cloud, Amazon RDS, Azure Database, and Google Cloud SQL offer 
convenient options for MySQL deployment. see mysql-rdbms2 mysql-rdbms3 mysql-rdbms4 mysql-rdbms5 mysql-rdbms6 mysql-rdbms7
mysql-rdbms8 mysql-rdbms9 mysql-rdbms10 mysql-rdbms11 mysql-rdbms12 mysql-rdbms13 mysql-rdbms14 mysql-rdbms15 mysql-rdbms16
mysql-rdbms17

As a data engineer or database administrator, you will often need to populate databases and tables. One method of doing this 
is to back up an existing database containing your data and restore it to your new destination. You can use mysqldump to back 
up a database to a .SQL file containing all of the statements needed to recreate the contents of the database. see mysql-backup1
If you're already at the MySQL command prompt, you can restore a dump file by using the source command with the name of the dump 
file. This method can also be used to execute SQL scripts from file. see mysql-backup2 mysql-backup3 Alternatively, if you only want 
to populate a small number of rows in an individual table, instead of populating an entire database, you can use the phpMyAdmin tool 
to manually enter rows or run SQL INSERT statements. see mysql-loading1. when using myaqlimport utility, passing the name of the database 
that the table resides in and the name of the CSV file the table name is inferred from the name of the CSV file. You must ensure that 
your file name matches your table name exactly.

Hands-on Lab: Getting Started with MySQL Command Line

In this lab, you will use the MySQL command line interface (CLI) to create a database, restore the structure and contents of tables, 
explore and query tables, and finally, learn how to dump/backup tables from the database.

Objectives

After completing this lab, you will be able to use the MySQL command line to:

Create a database.
Restore the structure and data of a table.
Explore and query tables.
Dump/backup tables from a database.

Software Used in this Lab

In this lab, you will use MySQL. MySQL is a Relational Database Management System (RDBMS) designed to efficiently store, manipulate, and 
retrieve data. https://www.mysql.com/

Database Used in this Lab

The Sakila database used in this lab comes from the following source: https://dev.mysql.com/doc/sakila/en/ under New BSD license 
[Copyright 2021 - Oracle Corporation].

You will use a modified version of the database for the lab, so to follow the lab instructions successfully please use the database 
provided with the lab, rather than the database from the original source.

The following entity relationship diagram (ERD) shows the schema of the Sakila database: see sakila-ERD.png

Task A: Create a database

1. Go to Terminal > New Terminal to open a terminal

2. Copy the command below to fetch the sakila_mysql_dump.sql file

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0110EN-SkillsNetwork/datasets/sakila/sakila_mysql_dump.sql

3. Start the MySQL service session

4. Initiate the mysql command prompt session using the command below in the terminal (replace the host and password values):

mysql --host=172.21.10.8 --port=3306 --user=root --password=ZDvq3NXjxUImP9F18HSE0xGM

5. Create a new database sakila using the command below in the terminal and proceed to Task B:

create database sakila;

Task B: Restore the structure and data of a table

1. To use the newly created empty sakila database, use the command below in the terminal:

use sakila;

2. Restore the sakila mysql dump file (containing the sakila database table definitions and data) to the newly created empty sakila 
database. A dump file is a text file that contains the data from a database in the form of SQL statements. This file can be imported 
using the command line with the following command:

source sakila_mysql_dump.sql;

Note: You can use the source command to restore the database dump file within the mysql command prompt. To restore the database dump file
outside of the mysql command prompt, you can use the mysql --host=mysql --port=3306 --user=root --password sakila < sakila_mysql_dump.sql 
command after quitting the mysql command prompt session with command \q.

Task C: Explore and query tables

1. To list all the tables names from the sakila database, use the command below in the terminal:

SHOW FULL TABLES WHERE table_type = 'BASE TABLE';

The Table_type for these tables is BASE TABLE. BASE TABLE means that it is a table as opposed to a view (VIEW) or an INFORMATION_SCHEMA 
view (SYSTEM VIEW).

2. Explore the structure of the staff table using the command below in the terminal:

DESCRIBE staff;

To understand the output, see the following table:

Column-Name			Definition
	
Field				Name of the column.
Type				Data type of the column.
Null				Displays YES if column can contain NULL values and NO if not. Notice how the primary key displays NO.
Key					Displays the value PRI if the column is a primary key, UNI if the column is a unique key, and MUL if the column is a 
					non-unique index in which one value can appear multiple times. If there is no value displayed, then the column isn't 
					indexed or it's indexed as a secondary column. Please note, that if more than one of these values applies to the 
					column, the value that appears will be displayed based on the following order: PRI, UNI, and MUL.
Default				The default value of the column. If the column's value has specifically been set as NULL, then the value that appears 
					will be NULL.
Extra				Any additional information about a column.

3. Now retrieve all the records from the staff table using the command below in the terminal:

SELECT * FROM staff;


4. Quit the MySQL command prompt session using the command below in the terminal and proceed to Task D:

\q


Task D: Dump/backup tables from a database

1. Finally, dump/backup the staff table from the database using the command below in the terminal:

mysqldump --host=[replace-me] --port=3306 --user=root --password sakila staff > sakila_staff_mysql_dump.sql

This command will backup the staff table from the sakila database into a file called sakila_staff_mysql_dump.sql.

using keys and constraints in MySQL see keys-constraints1 keys-constraints2 keys-constraints3 keys-constraints4 keys-constraints5
keys-constraints6 keys-constraints7 keys-constraints8

Getting started with postgres see postgresql-intro1 popular database choice for OLTP, data analytics, and geographic information systems.
postgresql-intro2 postgresql-intro3 postgresql-intro4 postgresql-intro5 postgresql-intro6 postgresql-intro7 postgresql-intro8
postgresql-intro9 postgresql-intro10 postgresql-intro11

As with many RDBMs, you can create databases and tables and load data PostgreSQL by using a command line interface, a graphical user 
interface or API calls. see psql-1 At the command line, you can use PSQL to restore a database that was previously backed up using 
pg_dump. see psql-2

see pg-admin-1 pg-admin-2 pg-admin-3 pg-admin-4 This step runs the SQL statements contained in that file to recreate the database objects 
and data in the current database. see pg-admin-5 pg-admin-6 pg-admin-7 pg-admin-8 pg-admin-9 pg-admin-10

see pg-dump-1.png 

Hands-on Lab: Getting Started with the PostgreSQL Command Line

In this lab, you will use the PostgreSQL command line interface (CLI) to create a database and restore the structure and contents of its 
tables. Then, you will learn how to explore and query tables. Finally, you will learn how to dump/backup tables from a database.

Software used in this lab

In this lab, you will use a PostgreSQL Database. PostgreSQL is a relational database management system (RDBMS) designed to store, 
manipulate, and retrieve data efficiently. https://www.postgresql.org/

Database used in this lab

The Sakila database used in this lab comes from the following source: https://dev.mysql.com/doc/sakila/en/ under New BSD license 
[Copyright 2021 - Oracle Corporation].

You will use a modified version of the database for the lab. To follow the lab instructions successfully, please use the database 
provided by the lab rather than the database from the source.

The following entity relation diagram (ERD) shows the structure of the schema of the Sakila database: see postresql-schema.png

Objectives

After completing this lab, you will be able to use the PostgreSQL command line to:

Create a database
Restore the structure and data of a table
Explore and query tables
Dump/backup tables from a database

Lab structure

In this exercise, you will go through several subtasks where you will use the PostgreSQL command line interface (CLI) to create a 
database and restore the structure and contents of tables. Then, you will learn how to explore and query tables. Finally, you will learn 
how to dump/backup tables from a database.

Task A: Create a database

1. Open a new command terminal 
2. Copy the command below to fetch the sakila_pgsql_dump.sql file

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0110EN-SkillsNetwork/datasets/sakila/sakila_pgsql_dump.sql

3. open the PostgreSQL Command Line Interface (CLI) 
4. Create a new database named sakila using the following command in the terminal:

create database sakila;

Note: You are using the create database command to create a new database within the PostgreSQL CLI. To create a new database named sakila 
outside the command line interface, you can use the following command directly in a terminal window: 

createdb --username=postgres --host=postgres --password sakila, after quitting the psql command prompt session with the command \q.

Task B: Restore the structure and data of a table

1. To connect to the newly created empty sakila database, use the following command in the terminal and enter your PostgreSQL service 
session password:

\connect sakila;

2. Restore the sakila PostgreSQL dump file (containing the sakila database table definitions and data) to the newly created empty sakila 
database by using the following command in the terminal:

\include sakila_pgsql_dump.sql;

Note: You are using the \include command to restore the database dump file within the PostgreSQL CLI. To restore the database dump file 
outside of the Command Line Interface, you can use the command:

pg_restore --username=postgres --host=postgres --password --dbname=sakila < sakila_pgsql_dump.tar 

after quitting the CLI prompt session with the command \q. Non-text format .tar dumps are restored using the pg_restore command. So, 
before using the pg_restore command, first, fetch the .tar version of this dump file using the command 
wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0110EN-SkillsNetwork/datasets/sakila/sakila_pgsql_dump.tar

3. Repeat Step 1 to reconnect to the sakila database after restoring the dump file.

Task C: Explore and query tables

1. To list all the table names from the sakila database, use the following command in the terminal:

\dt

2. Explore the structure of the store table using the following command in the terminal:

\d store;

3. Retrieve all the records from the store table using the following command in the terminal:

select * from store;

4. Quit the PostgreSQL command prompt session using the following command in the terminal.

\q


Task D: Dump/backup tables from a database

1. Finally, to dump/backup the store table from the database, use the following command in the terminal and enter your PostgreSQL service 
session password:

pg_dump --username=postgres --host=postgres --password --dbname=sakila --table=store --format=plain > sakila_store_pgsql_dump.sql

what are views? see views-1 

what is a materialised view? see materialised-view

see recap-1.png 

why is database design important? see database-design-importance1 database-design-importance2 database-design-importance3
database-design-importance4 database-design-importance5 database-design-importance6. The logical design phase involves transferring the 
output from the requirements analysis phase into entities, attributes, and relationships without specifying technical implementation 
details. database-design-importance7 database-design-importance8. When analyzing the requirements, you identified that a person borrows a 
book see database-design-importance9. However, a person might borrow many books and many people might borrow a book. The scenario creates 
a many-to-many relationship, potentially causing ambiguity in a database. The easiest way to solve this is by creating two separate 
one-to-many relationships, and introducing an associative entity between the existing ones. In the book-to-person scenario see 
database-design-importance10, you can add in a loan entity, a person can acquire multiple loans, and a book can undergo multiple loan 
instances. The loan entity will contain attributes from the book table and the person table, as well as some loan-specific ones.

After identifying entities and attributes, consider normalization. see database-design-importance11

Adhering to the first normal form involves eliminating the possibility of multiple author names listed in a single attribute, ensuring a 
more organized and efficient database structure. Additionally, creating a distinct author entity, instead of splitting into authors one 
and two, which violates the second normal form, is a critical step. Establishing a many-to-one relationship between the authors and book 
entities is key to ensuring a more flexible and normalized database structure. see database-design-importance12

When completing normalizing your entities, you can move on to the physical design stage, how your database will look. At this stage, 
consider the impact of your chosen database management system, including supported data types, naming rules, indexes, and constraints. 
When considering naming rules, you should also consider implementing your convention so that anyone working with your data understands 
your schema. In your physical design, the person entity from your logical design transforms into a person table. Each attribute becomes a 
typed column, and you define the keys. To facilitate this process, an entity relationship diagrams, ERD designer, proves invaluable. An 
entity relationship diagrams, ERD designer, can create entity relationship diagrams. pgAdmin includes the ERD tool where you can design 
your ERD and then generate an SQL script to create your database and objects based on your design. see database-design-importance13

Final Project: Database Design and Implementation

Scenario

In this scenario, you have recently been hired as a Data Engineer by a New York-based coffee shop chain looking to expand nationally by 
opening several franchise locations. They want to streamline operations and revamp their data infrastructure as part of their expansion 
process.

Your job is to design their relational database systems for improved operational efficiencies and make it easier for their executives to 
make data-driven decisions.

Currently, their data resides in several systems: accounting software, supplier databases, point of sales (POS) systems, and even 
spreadsheets. You will review the data in all of these systems and design a central database to house all of the data. You will then 
create the database objects and load them with source data. Finally, you will create subsets of data your business partners require, 
export them, and load them into staging databases using several RDBMS.

In this project, you will use PostgreSQL Database, MySQL. These are all relational database management systems (RDBMS) designed to store, 
manipulate, and retrieve the data efficiently.

Data used in this project

In this project, you will be working with a subset of data from the Coffee shop sample data. https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2019/07/12/beanie-coffee-1113?utm_source=Exinfluencer&utm_content=000026UJ&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDB0110ENSkillsNetwork24601058-2021-01-01&utm_medium=Exinfluencer&utm_term=10006555

You will use a modified version of the data for the project, so to succeed in the project, download the linked files when prompted in the 
instructions. You do not need to use any data from the source.

In your scenario, you will be working with data from the following sources:

Staff information held in a spreadsheet at headquarters (HQ)
Sales outlet information held in a spreadsheet at HQ
Sales data output as a CSV file from the POS system in the sales outlets
Customer data output as a CSV file from a custom customer relationship management system
Product information maintained in a spreadsheet exported from your supplier's database

Objectives

After completing this lab, you will be able to:

Identify entities
Identity attributes
Create an entity relationship diagram (ERD) using the pgAdmin ERD tool
Normalize tables
Define keys and relationships
Create database objects by generating and running the SQL script from the ERD tool
Create a view and export the data
Create a materialized view and export the data
Import data into a MySQL database using phpMyAdmin GUI tool

Task 1: Identify entities

The first step when designing a new database is to review any existing data and identify the entities for your new system.

The following image shows sample data from each source you will be working with to design your new central database. Review the image and 
identify the entities you plan to create. see existing_data.png 

Make a list of the entities you have identified. Take a screenshot and save it as Task1.jpg or Task1.png.

staff member
sales outlet
transaction
customer 
product

Task 2: Identify attributes

In this task, you will identify the attributes of one of the entities you plan to create.

Using the information from the sample data in the image from Task 1, identify the entity's attributes that will store the sales 
transaction data.

Make a list of the sales transaction attributes that you identified. Take a screenshot and save it as Task2.jpg or Task2.png.

transaction_id
transaction_date
transaction_time
sales_outlet_id
staff_id
customer_id
product_id
quantity
price

Task 3: Create an ERD

Now that you have defined some of your attributes and entities, you can determine the tables and columns for them and create an 
entity-relationship diagram (ERD).

connect to the postgresql instance

Create a new database named COFFEE, view the schemas in the new COFFEE database, and then start a new ERD project.

Add a table to the ERD for the sale transactions entity using the information in the following table. Consider the naming convention to 
use so that your colleagues can understand your data and ensure that the names are valid in other RDBMS. Use the sample data shown in the 
image in Task 1 to determine appropriate data types for each column.

Take a screenshot of your ERD and save it as Task3A.png or Task3A.jpg.

Add a table to the ERD for the product entity using the information in the following table.

Take a screenshot of your ERD and save it as Task3B.png or Task3B.jpg.

Task 4: Normalize tables

When reviewing your ERD, you notice it does not conform to the second normal form. In this task, you will normalize some of the tables 
within the database.

Review the data in the sales transaction table. Note that the transaction id column does not contain unique values because some 
transactions include multiple products.

Determine which columns should be stored in a separate table to remove the repeating rows and to put this table into second normal form.

Add a new table named sales_detail to the ERD, define the columns in the new table, and delete the moved columns from the sales 
transaction table, leaving a matching column in each of the two tables to create a relationship between them later.

-- This script was generated by the ERD tool in pgAdmin 4.
-- Please log an issue at https://github.com/pgadmin-org/pgadmin4/issues/new/choose if you find any bugs, including reproduction steps.
BEGIN;


CREATE TABLE IF NOT EXISTS public."Sales_transaction"
(
    transaction_id integer NOT NULL,
    transaction_data date NOT NULL,
    transaction_time timestamp without time zone NOT NULL,
    sales_outlet_id integer NOT NULL,
    staff_id integer NOT NULL,
    customer_id integer,
    PRIMARY KEY ("transaction id")
);

CREATE TABLE IF NOT EXISTS public."Product"
(
    product_id integer NOT NULL,
    category character varying(30),
    price double precision,
    PRIMARY KEY (product_id)
);

CREATE TABLE IF NOT EXISTS public."Sales_detail"
(
    sales_detail_id integer NOT NULL,
    "transaction_id " integer NOT NULL,
    product_id integer,
    quantity integer,
    price double precision,
    PRIMARY KEY (sales_detail_id)
);

CREATE TABLE IF NOT EXISTS public.product_type
(
    product_type_id integer NOT NULL,
    type character varying(30),
    product character varying(30),
    description character varying(100),
    PRIMARY KEY (product_type_id)
);

ALTER TABLE IF EXISTS public."Product"
    ADD FOREIGN KEY (product_id)
    REFERENCES public.product_type (product_type_id) MATCH SIMPLE
    ON UPDATE NO ACTION
    ON DELETE NO ACTION
    NOT VALID;


ALTER TABLE IF EXISTS public."Sales_detail"
    ADD FOREIGN KEY (sales_detail_id)
    REFERENCES public."Sales_transaction" (transaction_id) MATCH SIMPLE
    ON UPDATE NO ACTION
    ON DELETE NO ACTION
    NOT VALID;


ALTER TABLE IF EXISTS public."Sales_detail"
    ADD FOREIGN KEY (product_id)
    REFERENCES public."Product" (product_id) MATCH SIMPLE
    ON UPDATE NO ACTION
    ON DELETE NO ACTION
    NOT VALID;

END;