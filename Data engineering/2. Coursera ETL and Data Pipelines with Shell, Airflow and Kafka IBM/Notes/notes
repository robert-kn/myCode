What will you learn about? see learning1.png 

Are you an aspiring Big Data Engineer or Developer interested in creating Data Pipelines for serving Data Warehouses
and Data Analytics platforms? Would you like to learn all about ETL and ELT data pipelines and how to build them 
using Bash scripting and open-source tools such as Apache Airflow and Apache Kafka? This course may be just right 
for you. 

ETL stands for Extract, Transform, and Load. It refers to the process of curating data from multiple sources and 
preparing the data for integration and loading into a destination platform such as a data warehouse or analytics 
environment. ELT is similar but loads the data in its raw format, reserving the transformations for people to apply 
themselves in a ‘self-serve analytics’ destination environment. Both methods are typical examples of data pipeline 
deployments. 

In this course, you will explore the fundamental principles and techniques behind ETL and ELT processes. You will 
learn how to construct a basic ETL data pipeline from scratch using Bash shell-scripting. You will also learn about 
the tools, technologies, and use cases for the two main paradigms within data pipeline engineering: batch and 
streaming data pipelines.  You will further cement this knowledge by exploring and applying two popular open-source 
data pipeline tools: Apache Airflow and Apache Kafka.

You will learn all about Apache Airflow and use it to build, put into production, and monitor a basic batch ETL 
workflow. You will implement this data pipeline using Airflow’s central construct of a directed acyclic graph (DAG),
consisting of simple Bash tasks, Python function and their dependencies.

You will also learn about Apache Kafka and use it to get hands-on experience with streaming data pipelines, 
implementing Kafka’s message producers and consumers, and creating a Kafka weather topic.

What is ETL? see ETL1.png ETL2.png ETL3.png ETL4.png

What is ELT? see ELT1.png ELT2.png ELT3.png ELT4.png

Comparing ETL and ELT? see comparingETLandELT1.png comparingETLandELT2.png comparingETLandELT3.png 
comparingETLandELT4.png comparingETLandELT5.png

What are the data extraction techniques? see rawDataSourcesExamples1 rawDataSourcesExamples2 
techniquesForExtractingData1 techniquesForExtractingData2 usecases.png summaryDataExtractionTechniques.png

What are some data transformation techniques (formating data to suit the application)? see 
dataTransformationTechniques1.png dataTransformationTechniques2 dataTransformationTechniques3
dataTransformationTechniques4 dataTransformationTechniques5 dataTransformationTechniques6 
dataTransformationTechniques7

Examples of data loading techniques? see dataLoadingTechniques1.png dataLoadingTechniques2 dataLoadingTechniques3
dataLoadingTechniques4 dataLoadingTechniques5 dataLoadingTechniques6 dataLoadingTechniques7 dataLoadingTechniques8
dataLoadingTechniques9

ETL Techniques

ETL stands for Extract, Transform, and Load, and refers to the process of curating data from multiple sources, 
conforming it to a unified data format or structure, and loading the transformed data into its new environment. 

see ETLTechniques.png

Extract 

Data extraction is the first stage of the ETL process, where data is acquired from various source systems. The data 
may be completely raw, such as sensor data from IoT devices, or perhaps it is unstructured data from scanned medical
documents or company emails. It may be streaming data coming from a social media network or near real-time stock 
market buy/sell transactions, or it may come from existing enterprise databases and data warehouses.

Transform 
The transformation stage is where rules and processes are applied to the data to prepare it for loading into the 
target system. This is normally done in an intermediate working environment called a “staging area.” Here, the data 
are cleaned to ensure reliability and conformed to ensure compatibility with the target system.  

Many other transformations may be applied, including:  

Cleaning: fixing any errors or missing values  

Filtering: selecting only what is needed  

Joining: merging disparate data sources  

Normalizing: converting data to common units  

Data Structuring: converting one data format to another, such as JSON, XML, or CSV to database tables 

Feature Engineering: creating KPIs for dashboards or machine learning   

Anonymizing and Encrypting: ensuring privacy and security 

Sorting: ordering the data to improve search performance 

Aggregating: summarizing granular data 

Load 

The load phase is all about writing the transformed data to a target system. The system can be as simple as a 
comma-separated file, which is essentially just a table of data like an Excel spreadsheet. The target can also be a 
database, which may be part of a much more elaborate system, such as a data warehouse, a data mart, data lake, or 
some other unified, centralized data store forming the basis for analysis, modeling, and data-driven decision making
by business analysts, managers, executives, data scientists, and users at all levels of the enterprise.

In most cases, as data is being loaded into a database, the constraints defined by its schema must be satisfied for 
the workflow to run successfully. The schema, a set of rules called integrity constraints, includes rules such as 
uniqueness, referential integrity, and mandatory fields. Thus such requirements imposed on the loading phase help 
ensure overall data quality. 

ETL Workflows as Data Pipelines 

Generally, an ETL workflow is a well thought out process that is carefully engineered to meet technical and end-user requirements.  

Traditionally, the overall accuracy of the ETL workflow has been a more important requirement than speed, although 
efficiency is usually an important factor in minimizing resource costs. To boost efficiency, data is fed through a 
data pipeline in smaller packets (see Figure 2). While one packet is being extracted, an earlier packet is being 
transformed, and another is being loaded. In this way, data can keep moving through the workflow without 
interruption. Any remaining bottlenecks within the pipeline can often be handled by parallelizing slower tasks. 

With conventional ETL pipelines, data is processed in batches, usually on a repeating schedule that ranges from 
hours to days apart. For example, records accumulating in an Online Transaction Processing System (OLTP) can be 
moved as a daily batch process to one or more Online Analytics Processing (OLAP) systems where subsequent analysis 
of large volumes of historical data is carried out. 

Batch processing intervals need not be periodic and can be triggered by events, such as  

when the source data reaches a certain size, or  

when an event of interest occurs and is detected by a system, such as an intruder alert, or  

on-demand, with web apps such as music or video streaming services 

Staging Areas 

ETL pipelines are frequently used to integrate data from disparate and usually siloed systems within the enterprise.
These systems can be from different vendors, locations, and divisions of the company, which can add significant 
operational complexity. As an example, (see Figure 3) a cost accounting OLAP system might retrieve data from 
distinct OLTP systems utilized by the separate payroll, sales, and purchasing departments.

ETL Workflows as DAGs 

ETL workflows can involve considerable complexity. By breaking down the details of the workflow into individual 
tasks and dependencies between those tasks, one can gain better control over that complexity. Workflow orchestration
tools such as Apache Airflow do just that.

Airflow represents your workflow as a directed acyclic graph (DAG). A simple example of an Airflow DAG is illustrated
in Figure 4. Airflow tasks can be expressed using predefined templates, called operators. Popular operators include 
Bash operators, for running Bash code, and Python operators for running Python code, which makes them extremely 
versatile for deploying ETL pipelines and many other kinds of workflows into production. 

Popular ETL tools 

There are many ETL tools available today. Modern enterprise grade ETL tools will typically include the following 
features: 

Automation: Fully automated pipelines 

Ease of use: ETL rule recommendations 

Drag-and-drop interface: “0-code” rules and data flows 

Transformation support: Assistance with complex calculations 

Security and Compliance: Data encryption and HIPAA, GDPR compliance 

Some well-known ETL tools are listed below, along with some of their key features. Both commercial and open-source 
tools are included in the list. 

Talend Open Studio

Supports big data, data warehousing, and profiling

Includes collaboration, monitoring, and scheduling

Drag-and-drop GUI for ETL pipeline creation

Automatically generates Java code

Integrates with many data warehouses

Open-source


AWS Glue

ETL service that simplifies data prep for analytics 

Suggests schemas for storing your data

Create ETL jobs from the AWS Console


IBM InfoSphere DataStage 

A data integration tool for designing, developing, and running ETL and ELT jobs

The data integration component of IBM InfoSphere Information Server

Drag-and-drop graphical interface

Uses parallel processing and enterprise connectivity in a highly scalable platform

Alteryx 

Self-service data analytics platform 

Drag-and-drop accessibility to ETL tools

No SQL or coding required to create pipelines


Apache Airflow and Python

Versatile “configuration” as code data pipeline platform

Open-sourced by Airbnb

Programmatically author, schedule, and monitor workflows

Scales to Big Data

Integrates with cloud platforms

The Pandas Python library 

Versatile and popular open-source programming tool 

Based on data frames – table-like structures

Great for ETL, data exploration, and prototyping 

Doesn’t readily scale to Big Data


What are data pipelines? see introductiontodatapipelines1.png introductiontodatapipelines2 introductiontodatapipelines3
introductiontodatapipelines4 introductiontodatapipelines5 introductiontodatapipelines6 introductiontodatapipelines7
introductiontodatapipelines8

What are the key data pipeline processes? see keydatapipelineprocesses1.png keydatapipelineprocesses2 
keydatapipelineprocesses3 keydatapipelineprocesses4 keydatapipelineprocesses5 keydatapipelineprocesses6 
keydatapipelineprocesses7 keydatapipelineprocesses8 

what are differences between batch versus streaming data pipelines? see batchVersusStreaming1.png batchVersusStreaming2
batchVersusStreaming3 batchVersusStreaming4 batchVersusStreaming5 batchVersusStreaming6 batchVersusStreaming7
batchVersusStreaming8 batchVersusStreaming9 batchVersusStreaming10

What are the features of modern data pipeline tools? see featuresOfModernDataPipelines1 featuresOfModernDataPipelines2
featuresOfModernDataPipelines3 featuresOfModernDataPipelines4 featuresOfModernDataPipelines5 featuresOfModernDataPipelines6
featuresOfModernDataPipelines7 featuresOfModernDataPipelines8 featuresOfModernDataPipelines9 featuresOfModernDataPipelines10
featuresOfModernDataPipelines11


what is apache airflow? see airflow1.png airflow2.png

Airflow comes with a built-in scheduler, which handles the triggering of all scheduled workflows. The scheduler is 
responsible for submitting individual tasks from each scheduled workflow to the executor. The executor handles the 
running of these tasks by assigning them to workers, which then run the tasks. The web server component of the 
Airflow provides a user-friendly, graphical user interface. From this UI, you can inspect, trigger, and debug any 
of your DAGs and their individual tasks. The DAG directory contains all of your DAG files, ready to be accessed by 
the scheduler, the executor, and each of its employed workers. Finally, Airflow hosts a metadata database, which is 
used by the scheduler, executor, and the web server to store the state of each DAG and its tasks.

lifecycle of an apache airflow task state? see airflowjobstatus.png

No status -- The task has not yet been cued for execution. Scheduled -- The scheduler has determined that the task's 
dependencies are met and has scheduled it to run. Removed -- For some reason, the task has vanished from the DAG 
since the run started. Upstream failed -- An upstream task has failed. Queued -- The task has been assigned to the 
executor, and is waiting for a worker to become available. Running -- The task is being run by a worker. Success -- 
The task completed successfully, and no errors were encountered. Failed -- The task could not be completed successfully 
due to an error, and Up for retry -- The task will be rescheduled as per the retrial configuration. Ideally, a task 
should flow throughout the scheduler from no status, to scheduled, to queued, to running, and finally to success.

apache airflow features? see airflowFeatures1.png airflowFeatures2.png

what is a dag? see whatisadag1.png and whatisadag2.png

tasks and operators? see tasksandoperators.png

dag definition components? see dagdefcomponents1.png and dagdefcomponents2.png

Anatomy of a DAG
A DAG consists of these logical blocks.

Imports
DAG Arguments
DAG Definition
Task Definitions
Task Pipeline

imports block example

# import the libraries
from datetime import timedelta
# The DAG object; we'll need this to instantiate a DAG
from airflow.models import DAG
# Operators; you need this to write tasks!
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator


DAG Arguments block example

#defining DAG arguments
# You can override them on a per-task basis during operator initialization
default_args = {
    'owner': 'Your name',
    'start_date': days_ago(0),
    'email': ['youemail@somemail.com'],
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

DAG arguments are like the initial settings for the DAG.

The above settings mention:

The owner name
When this DAG should run from: days_ago(0) means today
The email address where the alerts are sent to
The number of retries in case of failure
The time delay between retries

The other options that you can include are:

'queue': The name of the queue the task should be a part of
'pool': The pool that this task should use
'email_on_failure': Whether an email should be sent to the owner on failure
'email_on_retry': Whether an email should be sent to the owner on retry
'priority_weight': Priority weight of this task against other tasks.
'end_date': End date for the task
'wait_for_downstream': Boolean value indicating whether it should wait for downtime
'sla': Time by which the task should have succeeded. This can be a timedelta object
'execution_timeout': Time limit for running the task. This can be a timedelta object
'on_failure_callback': Some function, or list of functions to call on failure
'on_success_callback': Some function, or list of functions to call on success
'on_retry_callback': Another function, or list of functions to call on retry
'sla_miss_callback': Yet another function, or list of functions when 'sla' is missed
'on_skipped_callback': Some function to call when the task is skipped
'trigger_rule': Defines the rule by which the generated task gets triggered

DAG definition block example

# define the DAG
dag = DAG(
    dag_id='unique_id_for_DAG',
    default_args=default_args,
    description='A simple description of what the DAG does',
    schedule_interval=timedelta(days=1),
)

Here you are creating a variable named dag by instantiating the DAG class with the following parameters:

unique_id_for_DAG is the ID of the DAG. This is what you see on the web console. This is what you can use to trigger 
the DAG using a TriggerDagRunOperator.

You are passing the dictionary default_args, in which all the defaults are defined.

description helps us in understanding what this DAG does.

schedule_interval tells us how frequently this DAG runs. In this case every day. (days=1).

task definitions block example

The tasks can be defined using any of the operators that have been imported.

# define the tasks
# define a task with BashOperator
task1 = BashOperator(
    task_id='unique_task_id',
    bash_command='<some bashcommand>',
    dag=dag,
)
# define a task with PythonOperator
task2 = PythonOperator(
    task_id='bash_task',
    python_callable=<the python function to be called>,
    dag=dag,
)
# define a task with EmailOperator
task3 = EmailOperator(
    task_id='mail_task',
    to='recipient@example.com',
    subject='Airflow Email Operator example',
    html_content='<p>This is a test email sent from Airflow.</p>',
    dag=dag,
)

A task is defined using:

A task_id which is a string that helps in identifying the task
The dag this task belongs to
The actual task to be performed
The bash command it represents in case of BashOperator
The Python callable function in case of a PythonOperator
Details of the sender, subject of the mail and the mail text as HTML in case of EmailOperator

task pipeline block example

# task pipeline
task1 >> task2 >> task3

You can also use upstream and downstream to define the pipeline. For example:

task1.set_downstream(task2)
task3.set_upstream(task2)

Task pipeline helps us to organize the order of tasks. In the example, the task task1 must run first, followed by 
task2, followed by the task task3.

how the airflow scheduler works see scheduler.png

what are the advantages of workflows as code? see advantages.png

Submit a DAG

Submitting a DAG is as simple as copying the DAG Python file into the dags folder in the AIRFLOW_HOME directory.
Airflow searches for Python source files within the specified DAGS_FOLDER.

export AIRFLOW_HOME=/home/project/airflow
echo $AIRFLOW_HOME

cp [dag_file_name].py $AIRFLOW_HOME/dags

airflow dags list | grep "[dag_file_name].py"

The logging capability is required for developers to monitor the status of tasks in DAG runs and to diagnose and 
debug issues. By default, Airflow logs are saved to local file systems as log files.

see logStorage.png 

how to review airflow log files see reviewlogs.png

airflow monitoring metrics see loggingandmonitoring1.png and loggingandmonitoring2.png

ESP's and Kafka

what is an event? in the context of event streaming, an event is a type of data which describes an entity's 
observable state at a time e.g. the gps coordinates of a car, temperature of a room, blood pressure of a 
patient etc. The format of the event can be primitive e.g. string, a key value pair, a key value with a timestamp. 
Event streaming occurs from one event source to a destination. This is called event streaming. In a real world 
scenario, you have many different event sources streaming to different destinations. see event1.png

common event formats? see event_formats.png

what is event streaming? see event_streaming1.png event_streaming2 event_streaming3

To overcome such a challenge of handling different event sources and destinations, we will need to employ the event 
streaming platform. 

An ESP's (event streaming platforms) act as a middle layer between between event sources and destinations. They 
usually have different architectures and components. a diagram of common ESP components: see ESP1.png

Different ESP's may have different architectures and components see ESP2.png Here we show you some common components
included in most ESP systems. The first and foremost component is the event broker, which is designed to receive and
consume events. The second common component of an ESP is event storage, which is used for storing events being 
received from event sources. Accordingly, event destinations do not need to synchronize with event sources, and 
stored events can be retrieved at will. The third common component is the analytic and query engine, which is used 
for querying and analyzing the stored events.

Broker components see brokercomponents.png: It normally contains three subcomponents, ingester, processor, and 
consumption. The ingester is designed to efficiently receive events from various event sources. The processor 
performs operations on data such as serializing and deserializing, compressing and decompressing, encryption and 
decryption, and so on. The consumption component retrieves events from event storage and efficiently distributes 
them to subscribed event destinations.

see popularESPs.png: Each has its unique features and application scenarios. Among these ESPs, Apache Kafka is 
probably the most popular one. 

Implementing an ESP and its components from scratch can be extremely difficult, but there are several open source 
and commercial ESP solutions with built-in capabilities available in the market. Apache Kafka is an open source 
project which has become the most popular ESP. Kafka is a comprehensive platform and can be used in many application
scenarios. Kafka was originally used to track user activities such as keyboard strokes, mouse clicks, page views, 
searches, gestures, screen time, and so on. But now Kafka is also suitable for all kinds of metric streaming such as
sensor readings, GPS, and hardware and software monitoring. For enterprise applications and infrastructure with a 
huge number of logs, Kafka can be used to collect and integrate them into a centralized repository. For banks, 
insurance, or fintech companies, Kafka is widely used for payments and transactions. These scenarios are just the 
tip of the iceberg. Essentially, you can use Kafka when you want high throughput and reliable data transportation 
services among various event sources and destinations. All events will be ingested in Kafka and become available for 
subscriptions and consumption, including further data storage and movement to other online or offline databases and 
backups. Real time processing and analytics including dashboard, machine learning, AI algorithms, and so on, 
generating notifications such as email, text messages, and instant messages, or data governance and auditing to 
make sure sensitive data such as bank transactions are complying with regulations. see apachekafka.png
common_usecases.png


what is kafka? is a distributed, real time event streaming platform that adheres to client server architecture. It 
runs as a cluster of broker servers, acting as the event broker to receive events from producers, store the stream 
of records, and distribute events. see apachekafka.png. It also has servers that run kafka connect to import and 
export data as event streams. all the brokers before version 2.8 relied on another distributed system called 
Zookeeper for metadata management and to ensure all brokers work in an efficient and collaborative way. However, 
Kafka Raft, pronounced as KRaft, is now used to eliminate Kafka's reliance on Zookeeper for metadata management. 
see kafkaarchitecture.png

Using Kafka controllers, producers send or publish data to the topic, and the consumers subscribe to the topic to 
receive data. Kafka uses a transmission control protocol, TCP based network communication protocol, to exchange data
between clients and servers. For the client side, Kafka provides different types of clients such as Kafka command 
line interface, CLI. A collection of shell scripts to communicate with the Kafka server, several high level 
programming APIs such as Java, Scala, Python, Go, C, and C++, rest APIs, and some specific third party clients made 
by the Kafka community. You can select different clients based on your requirements.

see kafkafeatures.png: Kafka is a distribution system, which makes it highly scalable to handle high data throughput 
and concurrency. A Kafka cluster normally has multiple event brokers which can handle event streaming in parallel. 
Kafka is very fast and highly scalable. Kafka also divides event storage into multiple partitions and replications, 
which makes it fault-tolerant and highly reliable. Kafka stores the events permanently. As such, event consumption 
can be done whenever suitable for consumers without a deadline, and Kafka is open source, meaning that you can use 
it for free and even customize it based on your specific requirements.

Even though Kafka is open source and well documented, it is still challenging to configure and deploy Kafka without 
professional assistance. Deploying a Kafka cluster requires extensive efforts for tuning infrastructure and 
consistently adjusting the configurations, especially for enterprise-level deployments. Fortunately, several 
commercial service providers offer an on-demand ESP as a service to meet your streaming requirements. Many of them 
are built on top of Kafka and provide added value for customers. Some well known ESP providers include see kaas.png


A kafka cluster has one or many brokers. A broker is dedicated server to receive, store, process and distribute 
events. Brokers are synchronised and use kraft controller nodes that use the consensus protocol to manage the kafka 
metadata log that contains information about each change to the cluster metadata. Each broker contains one or more 
topics. You can think of each topic as a database to store specific types of events such as logs, transactions etc. 
see brokerandtopics.png

brokers save published events into topics and distribute the events to subscribed consumers

Kafka uses topic partitions and replications to increase fault tolerance and throughput so that event publication and
consumption can be done in parallel with multiple brokers. In addition, if some brokers are down, kafka clients are 
still able to work with target topics replicated in other working brokers. see partitioningAndReplication.png

The kafka cli provides a collection of script files for users to build an event streaming pipeline. The most common 
one you will use will be the kafka-topics script to manage topics in a kafka cluster see kafkaTopicsScript.png

kafka producers are client applications that publish events to a topic partition according to the same order that 
they are published. An event to be published can be optionally associated with a key. Events associated to the same 
key will be published to the same topic partition. Events not associated to any key will be published to topic 
partitions in rotation. see producer_features.png. Suppose you have an Event Source 1 which generates various log 
entries and an Event Source 2 which generates user activity tracking records. Then you can create a Kafka producer 
to publish log records to log topic partitions and a user producer to publish user activity events to user topic 
partitions, respectively. When you publish events in producers, you can choose to associate events with a key, for 
example, an application name or a user ID. see kafkaproducer.png

Similar to the Kafka topic CLI, Kafka provides the Kafka producer CLI for users to manage producers. The most 
important aspect is starting a producer to write or publish events to a topic. Here you start a producer and point 
it to the log_topic. Then you can type some messages in the console to start publishing events. For example, log1, 
log2, and log3. You can provide keys to events to make sure the events with the same key will go to the same 
partition. Here you are starting a producer to user_topic with the parse.key option to be true and you also specify 
the key.separator to be comma. Then you can write messages as follows, key, user1, value login website, key, user1, 
value, click the top item and key, user1, value, logout website. Accordingly, all events about user one will be 
saved in the same partition to facilitate the reading for consumers. see producerCLI.png

Once events are published and properly stored in topic partitions, you can create consumers to read them. Consumers 
are client applications that can subscribe to topics and read the stored events. Then event destinations can further
read events from Kafka consumers. Consumers read data from topic partitions in the same order as they are published.
Consumers also store an offset for each topic partition as the last read position. With the offset, consumers are
guaranteed to read events as they occur. A playback is also possible by resetting the offset to zero. This way, the 
consumer can read all events in the topic partition from the beginning. In Kafka, producers and consumers are fully 
decoupled. As such, producers don't need to synchronize with consumers, and after events are stored in topics, 
consumers can have independent schedules to consume them. 

To read published log and user events from topic partitions, you will need to create log and user consumers and make
them subscribe to corresponding topics. Then Kafka will push the events to those subscribed consumers. Then the 
consumers will further send to event destinations. see consumer.png 

To start a consumer is also easy using the Kafka consumer script. Let's read events from the log_topic. You just 
need to run the Kafka console consumer script and specify a Kafka cluster and the topic to subscribe to. Here you 
can subscribe to and read events from the topic log_topic. Then the started consumer will read only the new events 
starting from the last partition offset. After those events are consumed, the partition offset for the consumer will
also be updated and committed back to Kafka. Very often a user wants to read all events from the beginning as a 
playback of all historical events. To do so, you just need to add the from-beginning option. Now you can read all 
events starting from offset 0. see consumerCLI.png


Let's have a look at a more concrete example to help you understand how to build an event streaming pipeline end to 
end. Suppose you want to collect and analyze weather and Twitter event streams so that you can correlate how people 
talk about extreme weather on Twitter. Here you can use two event sources: IBM weather API to obtain real time and 
forecasted weather data in JSON format. Twitter API to obtain real-time tweets and mentions also in JSON format. To 
receive weather and Twitter JSON data in Kafka, you then create a weather topic and a Twitter topic in a Kafka 
cluster with some partitions and replications. To publish weather and Twitter JSON data to the two topics, you need 
to create a weather producer and a Twitter producer. The event's JSON data will be serialized into bytes and saved 
in Kafka topics. To read events from the two topics, you need to create a weather consumer and a Twitter consumer. 
The bytes stored in Kafka topics will be deserialized into event JSON data. If you now want to transport the weather
and Twitter event JSON data from the consumers to a relational database, you will use a DB writer to parse those 
JSON files and create database records, and then you can write those records into a database using SQL insert 
statements. Finally, you can query the database records from the relational database and visualize and analyze them 
in a dashboard to complete the end-to-end pipeline. see weather_pipeline_example.png

see recap.png

In event streaming, in addition to transporting data, data engineers also need to process data through. For example,
data filtering, aggregation, and enhancement. Any applications developed to process streams are called stream 
processing applications. For stream processing applications based on Kafka, a straightforward way is to implement 
an ad hoc data processor to read events from one topic, process them, and publish them to another topic.

Let's look at an example. You first request raw weather JSON data from a weather API, and you start a weather 
producer to publish the raw data into a raw_weather_topic. Then you start a consumer to read the raw weather data 
from the weather topic. Next, you create an ad hoc data processor to filter the raw weather data to only include 
extreme weather events, such as very high temperatures. Such a processor could be a simple script file or an 
application which works with Kafka clients to read and write data from Kafka. Afterwards, the processor sends the 
processed data to another producer and it gets published to a processed_weather_topic. Finally, the processed 
weather data will be consumed by a dedicated consumer and sent to a dashboard for visualization. Such ad hoc 
processors may become complicated if you have many different topics to be processed. see adhoc_processing.png

A solution that may solve these challenges is Kafka. It provides the Streams API to facilitate stream processing. 
Kafka Streams API is a simple client library aiming to facilitate data processing in event streaming pipelines. It 
processes and analyzes data stored in Kafka topics. Thus, both the input and output of the Streams API are Kafka
topics. Additionally, Kafka Streams API ensures that each record will only be processed once. Finally, it processes 
only one record at a time. see streams_api_features.png

Kafka Streams API is based on a computational graph called a stream processing topology. In this topology, each node
is a stream processor, which receives streams from its upstream processor; performs data transformations, such as 
mapping, filtering, formatting, and aggregation; and produces output streams to its downstream stream processors. 
Thus, the edges of the graph are the I/O streams. There are two special types of processors. On the left, you can 
see the source processor which has no upstream processors. A source processor acts like a consumer, which consumes 
streams from Kafka topics and forwards the process streams to its downstream processors. On the right, you can see 
the sink processor, which has no downstream processors. A sink processor acts like a producer which publishes the 
received stream to a Kafka topic. see streamProcessingTopology.png

Let's redesign the previous weather stream processing application with Kafka Streams API. Suppose you have a 
raw_weather_topic and a processed_weather_topic in Kafka. Now, instead of spending a huge amount of effort 
developing an ad hoc processor, you could just plug in the Kafka Streams API here. In the Kafka Streams topology, we
have three stream processors, the source processor that consumes raw weather streams from the raw_weather_topic and 
forwards the weather stream to the stream processor to filter the stream based on high temperature. Then the filtered
stream will be forwarded to the sink processor, which then publishes the output to the processed_weather_topic. 
Concluding, this is a much simpler design than an ad hoc data processor, especially if you have many different topics
to be processed. see streamProcessing.png 
