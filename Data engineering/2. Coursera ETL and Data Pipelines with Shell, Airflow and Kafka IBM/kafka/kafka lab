Hands-on Lab: Build a Streaming ETL Pipeline using Kafka

Project scenario

You are a data engineer at a data analytics consulting company. You have been assigned to a project that aims to de-congest the national highways by analyzing the road traffic data from different toll plazas. As a vehicle passes a toll plaza, the vehicle's data like vehicle_id,vehicle_type,toll_plaza_id, and timestamp are streamed to Kafka. Your job is to create a data pipe line that collects the streaming data and loads it into a database.

Objectives

In this assignment, you will create a streaming data pipe by performing these steps:

Start a MySQL database server
Create a table to hold the toll data
Start the Kafka server
Install the Kafka Python driver
Install the MySQL Python driver
Create a topic named toll in Kafka
Download streaming data generator program
Customize the generator program to steam to toll topic
Download and customize streaming data consumer
Customize the consumer program to write into a MySQL database table
Verify that streamed data is being collected in the database table

Exercise 1: Download and extract Kafka

1. Download Kafka by running the command below.

wget https://downloads.apache.org/kafka/3.7.0/kafka_2.12-3.7.0.tgz

2. Extract Kafka from the zip file by running the command below.

tar -xzf kafka_2.12-3.7.0.tgz

Exercise 2: Configure KRaft and start server

1. Change to the kafka_2.12-3.7.0 directory.

cd kafka_2.12-3.7.0

2. Generate a cluster UUID that will uniquely identify the Kafka cluster.

KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"

Note: The new cluster id generated will be used by the KRaft controller.

3. KRaft requires the log directories to be configured. Run the following command to configure the log directories passing the cluster id.

bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties

Now that KRaft is configured, you can start the Kafka server by running the following command.

bin/kafka-server-start.sh config/kraft/server.properties

Exercise 3: Start MySQL server and setup the database

1. Connect to the MySQL server using the command below in the terminal. Make sure you use the password given to you when the MySQL server starts. Please make a note of the password because you will need it later.

mysql --host=[hostname] --port=3306 --user=root --password=[your password]

2. Create a database named tolldata.

create database tolldata;

3. Create a table named livetolldata with the schema to store the data generated by the traffic simulator.

use tolldata;
create table livetolldata(timestamp datetime,vehicle_id int,vehicle_type char(15),toll_plaza_id smallint);

Note: This is the table where you will store all streamed data that comes from Kafka. Each row is a record of when a vehicle has passed through a certain toll plaza along with its type and anonymized id.

6. Disconnect from the MySQL server.

exit

Exercise 4: Install the Python packages

1. Install the Python module kafka-python. This Python module will help you to communicate with kafka server. It    
   can used to send and receive messages from Kafka.

   pip3 install kafka-python

2. Install the Python module mysql-connector-python using the pip command

pip3 install mysql-connector-python==8.0.31

Exercise 5: Create data pipeline for toll data

1. Create a Kafka topic named toll.

2. Download the toll_traffic_generator.py from the url given below using wget.

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/toll_traffic_generator.py

3. Open the toll_traffic_generator.py and set the topic to toll.

4. Run the toll_traffic_generator.py.

    python3 toll_traffic_generator.py

5. Download the streaming-data-reader.py from the URL below using wget.

wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/vVxmU5uatDowvAIKRZrFjg/streaming-data-reader.py

6. Open the streaming-data-reader.py and modify the following details so that the program can connect to your MySQL server.

TOPIC

DATABASE

USERNAME

PASSWORD

7. Run the streaming-data-reader.py.

python3 streaming-data-reader.py

8. If you completed all the steps correctly, the streaming toll data will get stored in the table livetolldata. As a last step in this lab, open mysql CLI and list the top 10 rows in the table livetolldata.