During this course, you will get an overview of the types of tasks involved in database management and see what a typical workday for a Database Administrator (DBA) looks like, revolving around various activities ranging from designing databases to planning and troubleshooting errors. You will gain hands-on experience learning about server objects, configurations, and database objects, including schemas, tables, triggers, and events.

The ability to respond quickly to system failures, corruption, and catastrophic events is a key part of any DBA's job. Crucial to this is the ability to recover data that has been lost, so you will learn to backup and recover databases and define backup and recovery policies and procedures through hands-on labs. 

Security of data and databases is critical for any organization. To help ensure data is secure, you'll learn about database security and user management, including creating and resetting user passwords, creating groups, and more.

Ongoing monitoring and optimization of databases are essential tasks that enable DBAs to respond to issues before they become problems. Course topics include creating and keeping baselines, performance metrics, standards, and finally, monitoring RAM and disk usage, connections, and cache stats. You will learn about database optimization, including updating statistics, addressing slow queries, and creating indexes.

In this course, you'll explore some basic troubleshooting processes that help data engineers find frequently occurring issues, such as connectivity, login, configuration, and whether the instance is running.

Being able to automate processes is a skill that enables DBAs to make database administration easier. You can automate many functions, from managing alerts to generating and sending reports. You can create these automation tasks using standard Linux and Unix shell commands or cron jobs. 

This course uses a free, cloud-based environment where you complete hands-on labs that enable you to apply the knowledge you learned during this course.

Course prerequisites include basic knowledge of relational databases and SQL fundamentals. Understanding the Linux command line interface and how to use shell commands will also benefit you during the final modules of the course.

database management lifecycle: see lifecycle-1 lifecycle-2 lifecycle-3 A database model represents the design of a database: which instance contains which databases and tables, how the tables relate to each other, how users access the data, and so on. DBAs model the databases and their objects with the help of Entity Relationship Diagrams or ERDs. In self-managed environments such as on-premise databases, DBAs also need to consider size, capacity, and growth. They determine appropriate server resources like storage space, memory, processing power, and log file size. They also need to plan for how database objects are physically stored. For example, DBAs can choose to store frequently used data on a high-speed disk array or to store indexes separately from the data for better performance. In the implementation stage, DBAs roll out the carefully planned design. Creation of database objects like instances, databases, tables, views, and indexes. Configuring database security, granting access for database users, groups, and roles, so database objects are accessible only by the specific users and applications authorized to access them. Automation of repeating database tasks such as backups, restores, and deployments to improve efficiency. In populating the database, data might be imported from other databases, or data might be exported based on a query from a different source, or migrate projects from one environment to another, such as moving a project from the Application Development environment to the Production environment. lifecycle-4 lifecycle-5 lifecycle-6 

Data Security, Ethical and Compliance Considerations

As a database administrator (DBA), one of the most important parts of your role is to safeguard the data in the system. You control the system, so you are responsible for ensuring that the data is secure and complies to all relevant standards. You must also hold yourself to the highest ethical standards. Some organizations include a specific database security administrator role that focuses on these duties, but all DBAs need to keep them in mind.

A foundation of basic ethical concepts supports good data security practices. These should help guide the policies and workflows you create and the actions you take. Some important concepts are:

Transparency: When you collect information, you should tell the owners of the information exactly what data you will collect and what you will do with it. Inform them about how you use the data, how you store it, who will have access to it, and how you will dispose of it when you have finished using it.

Consent: You should get clear consent from data owners before you collect their data. This should detail what data you will be allowed to collect and how you will be allowed to use it.

Integrity: Always be clear about your procedures and policies, and always follow them consistently. As far as you can, make sure that others in your organization also follow the correct procedures and policies.

Consider creating a code of ethics–a written statement of security-related standards and intentions. You can include priorities, best practices, who will be responsible, and whatever else is important to understand clearly. This will create shared expectations for yourself and others, which will help build trust and make it easier for everyone to follow correct procedures.

Secure System Design

The structure of your system is a powerful tool in keeping your data safe. If your system is built to maintain security, it’s much easier to prevent breaches. To make sure your system works for you, consider these factors.

Protection from malicious access: The front line of protection for your data is basic software security. Your firewall and other cybersecurity tools should actively prevent hacking and malware installation, and alert you to threats. Be sure you update this software frequently, to keep scanning lists up to date. Also, educate users about phishing and other ways that they can unwittingly enable malicious access.
Secure storage: The storage you choose for your data must be secure not only from malicious access, but also from hardware failure and even natural disasters. Select your services carefully and make sure you understand their security practices and disaster preparedness plans. Back up your data regularly and reliably to minimize data loss in case of an emergency.
Accurate access: Only those who need certain data should be able to access it. Establish a system of assigning and tracking privileges that assigns each user only the necessary privileges, and controls what they can do with the data. Ensure that your policy complies with any data usage agreements you have made.
Secure movement: Data can be particularly vulnerable to interception when you move it into or out of storage. Be sure to consider safe transfer methods as carefully as you plan safety for the rest of your system.
Secure archiving: At some point, you may want to move data from active storage to an archive. This can protect it from accidental access and make your system more efficient. Make sure your archiving system is as secure as the rest of your storage. Data agreements often specify how long you may use the data, so be sure the archived data is regularly weeded for expired rights and don’t retain any more data than you will need for compliance with organization policy. Eliminate your discarded data securely and completely.

Compliance Issues
Maintaining compliance with all relevant laws and standards is a vital concern. Failure can result in data insecurity, professional censure for your organization, and even legal action. This list includes some of the most common types of standards, but it’s not exhaustive; always find out which regulations and standards apply to your organization.

National/international regulations: Many industries must be concerned with important legal standards on the national or international level. Some examples include HIPAA regulations for health-related information in the US, the GDPR in Europe, and the Information Technology Act, 2000 in India.
Industry standards: Some data standards aren’t enforced by law but can still carry repercussions for your organization’s reputation and standing if they aren’t followed. An example might be the Payment Card Industry Data Security Standard (PCI DSS), which applies to any organization that collects, stores, or transmits cardholder data.
Organization best practices: Each organization will formulate standards for handling its internal data; as a DBA, you may work on that as part of your job. Employee confidentiality is often an important part of these policies, as is protecting intellectual property owned by the organization.
If you build your system and procedures thoughtfully and maintain them with consistency and vigilance, you can keep the data in your system safe and productive.

Database objects: Relational Database Management Systems (RDBMSes) contain many objects that database engineers and database administrators must organize. Storing tables, constraints, indexes, and other database objects in a hierarchical structure allows DBAs to manage security, maintenance, and accessibility. see database-objects-1 Some RDBMSes consider the schema a parent object of a database, and others consider it a database object. An instance is a logical boundary for a database or set of databases where you organize database objects and set configuration parameters. Every database within an instance is assigned a unique name, has its own set of system catalog tables (which keep track of the objects within the database) and has its own configuration files. 

You can create more than one instance on the same physical server, providing a unique database server environment for each instance. The databases and other objects within one instance are isolated from those in any other instance. You can use multiple instances when you want to use one instance for a development environment and another instance for a production environment, restrict access to sensitive information, or control high-level administrative access. database-objects-2

Not all RDBMSes use the concept of instances, often managing database configuration information in a special database instead. In Cloud-based RDBMSes, the term instance means a specific running copy of a service.

A schema is a specialized database object that provides a way to group other database objects logically. A schema can contain tables, indexes, constraints, and other objects. When you create a database object, you can assign it to a schema. In most RDBMSes, the default schema is the user schema for the currently logged-on user. Many RDBMSes use a specialized schema to hold configuration information and metadata about a particular database. database-objects-3

Database objects are the items that exist within the database. The process of database design includes defining database objects and their relationships with each other. You can create and manage database objects through graphical database management tools, scripting, or accessing the database through an API. If you use SQL to create or manage the object, you will use Data Definition Language statements like CREATE or ALTER. see database-objects-4

system objects and database configuration: Whether you are trying to find out if new indexes are helpful, the transaction count on a database at a specific time, or who’s connected to the database at any given time, data that allows you to know how your databases are performing is crucial. RDBMSes store information about their databases, known as metadata, in special databases, schemas, or catalogs. They store specific types of information about the database, such as the name of a database or table, the data type of a column, or access privileges, known as metadata. see system-objects-1

The RDBMS controls and updates the system objects, but you can query the metadata tables to discover information about the objects in the database.  Different RDBMSes use different names for their metadata stores system-objects-2. PostgreSQL uses the system catalog, which is a schema with tables and views that contain metadata about all the other objects inside the database, and more. With it, you can discover when various operations happen, how tables or indexes are accessed, and whether the database system is reading information from memory or needing to fetch data from disk.

During any database installation, you supply parameters like the location of the data directory, the port number that the service listens on for connections, memory allocation, and much more. You can accept the default options or supply custom values to suit the environment. The database installation process saves this information into files, known as configuration or initialization files. The database uses the information in these files to set parameters as it starts up. Again, different RDBMSes use different files, file locations, and settings. system-objects-3. Still, they all serve the same purpose: to provide the initial configuration information for the database as it starts up and operates. Configuration information can include general settings like the location of data and log files, and the port the server listens on for requests. Or configuration information can be more focused, like settings that affect performance, including memory allocation, a connection timeout, and the maximum packet size. system-objects-4

Suppose you want to modify these parameters after the database has started. In an on-premises RDBMS, you stop the database service, modify the file, and then restart the service, causing the database to read the modified settings. In a Cloud-based RDBMS, you select configuration options as you create the database. One advantage of Cloud-based systems is that you can scale many configuration options, such as storage size and compute power, through a graphical interface as the service is running. You don’t have to edit configuration files. system-objects-5

database storage: As a DBA, you'll be responsible for ensuring that your database has enough storage space for all the data it needs to store. You must determine the capacity required for the database and plan for growth. In cloud-based databases expanding your storage space is performed through an API or a Graphical Console. One of the advantages of using a cloud-based database is that it is so easy to scale up and scale down in terms of storage space. In a self-managed or on-premises environment, you can also plan storage space for improved performance, storing competing resources on different disks.

RDBMSes separate the physical storage of database files on disk from the logical design of the database, allowing more flexibility in managing the data files. You can manage the data through a logical object without being concerned about the nature of the physical storage. For example, you can issue a command to back up a database without having to specify all the physical disks that store the database. 

Tablespaces are structures that contain database objects such as tables, indexes, large objects, and long data. DBAs use tablespaces to logically organize database objects based on where their data is stored. Tablespaces define the mapping between logical database objects and the physical storage containers that house the data for these objects. A storage container can be a data file, a directory, or a raw device. Tablespaces can contain one or more database objects. In this example, Tablespace 1 contains multiple small tables, whereas Tablespace 2 only houses a single large table. Tablespace 3 contains frequently used indexes. DBAs configure one or more storage containers to store each tablespace. system-objects-6

By using the combination of tablespaces and containers, you can keep logical database storage separate from physical storage and manage the disk layout of a database and its objects. This results in several benefits: Performance: You can use tablespaces to optimize performance. For example, you can place a heavily used index on a fast SSD. Alternatively, you can store tables containing rarely accessed or archived data on a less expensive but slower magnetic hard drive. Recoverability: Tablespaces make backup and restore operations more convenient. Using a single command, you can make a backup or restore all the database objects without worrying about which storage container each object or tablespace is stored on. system-objects-7

Some RDBMSes provide Storage Groups. A storage group is a grouping of storage paths or containers based on similar performance characteristics. This allows you to perform Multi-Temperature Data Management more easily. In this context, temperature refers to the frequency of data access. Hot data is accessed very frequently, Warm data is accessed somewhat frequently, and Cold data is accessed infrequently. By using storage groups, you can organize your data and storage based on temperature. In this example, very frequently accessed hot tables can be placed in Tablespace H, which is distributed on a group of fast storage devices. Somewhat frequently accessed tables in Tablespaces W1 and W2 can be stored on a Warm Storage Group. And the least frequently accessed tables in Tablespace C can be stored on slower and  less expensive storage devices in a Cold Group. Using storage groups helps with optimizing performance for frequently accessed data and reducing costs for storing infrequently accessed data. see system-objects-8

A partitioned relational database is a relational database whose data is managed across multiple database partitions. You can partition tables that need to contain very large quantities of data into multiple logical partitions, with each partition containing a subset of the overall data. Database partitioning is used in scenarios that involve very large volumes of data, such as data warehousing and data analysis for business intelligence. see system-objects-9

see storage engines in MYSQL

see MySQL Storage Engines and System Tables

see PostgreSQL Instance Configuration and System Catalog

introduction to backup and restore: see backup-1 backup-2 backup-3 backup-4 backup-5 

A full backup is a complete copy of all of the data in the object or objects that you are backing up. Performing full backups simplifies the restore and recovery processes, because you just need to locate the latest full backup and restore that one file. However, as the size of your database increases, the time, bandwidth, and storage for the backup file also increases. And if you decide to keep previous copies of the backup for safety, you will be storing many instances of a large file. However, if you only keep one copy, you must accept the risk that if the backup file is corrupt, you cannot restore your data. Also consider that if only a subset of your data is regularly changing, you could be needlessly backing up the same data many times. Storing a complete copy of your data outside of the RDBMS means that you must ensure it is adequately secured and that it cannot be accessed by unauthorized users. When you restore a full backup, you restore the data to the state that it was in when the backup was taken. But the database may have processed many transactions since that time, which should ideally be restored too.

One way around this issue is to enable logging of each transaction on your database and then you can use the information in the log file to reapply the transactions to the restore database. The process of reapplying transactions after restoring a database backup is known as recovery and it enables you to recover the data to a state that it was in at a particular point in time, hence the name, point-in-time recovery. For example, if you know that a DML statement run at 11:05am erroneously deleted some data, you can restore the latest full backup and then reapply the transactions up to that point in time, minimizing the loss of data changes that occurred between the last full backup and the moment that the wrong data was deleted.

Different database systems use different terminology for the log containing the transaction information, for example, MySQL calls it the binary log, Postgres calls it the write-ahead log, and Db2 on Cloud calls it the transaction log. 

Because full backups can require a lot of time, bandwidth, and storage, one alternative is to use them in conjunction with differential backups. A differential backup consists of a copy of any data that has changed since the last full backup was taken. This makes the differential backup file much smaller than a full backup file, reducing the time, bandwidth, and storage needs for the backup while still enabling you to restore a recent copy of the data. 

Incremental backups are similar to differential backups, but they only contain data that has changed since the last backup of any type was taken. 

Hot backups, or online backups, are those performed on data when it is in use. The advantage of hot backups is that they have no impact on availability and users can continue with their activities throughout the backup period. However, hot backups can result in performance degradation for users while the backup is running and can impact on data integrity if data changes during the backup process.

The alternative to hot backups is to take the database offline while the backup is run. This is known as a cold, or offline, backups. This eliminates the data integrity risks associated with hot backups, but has a greater impact on user availability and cannot be used in 24/7 environments. 

Hot backups are generally stored on an available server and often receive regular updates from the production database. This enables them to be brought online should the production server fail, to ensure continuing availability to users. Cold backups tend to be stored on external drives or on servers that are shut down between back up operations. This can provide greater data safety, but does mean that the recovery process will take longer than that of a hot backup. backup-6 backup-7
backup-8

using database transaction logs for recovery: Separate from the diagnostic log, a database management system (DBMS) uses transaction logs to keep track of all transactions that change or modify the database. The information stored in these transaction logs can be used for recovery purposes when data is accidentally deleted, or a system or hardware failure such as a disk crash occurs. In case of such events, the transaction log, in conjunction with a database backup, can help restore your database back to a consistent state at a specific point in time. backup-9

You typically specify where you want transaction log files to be stored using the database configuration. backup-10 In PostgreSQL, the transaction log file is referred to as a write-ahead log (WAL), because before the database manager writes the database changes or transactions to data files, the DBMS first writes them to the WAL.

A recommended best practice, at least for production databases, is to place the transaction log files on storage volumes that are separate from where the database objects like tablespaces are stored. This not only helps with performance, so database writes are not competing with log writes, but also improves recoverability since it isolates logs from crashes or the corruption of data volumes. 

For even more enhanced recoverability in business-critical databases, some relational database management systems (RDBMSs) provide the ability to automatically mirror logs to a second set of storage devices. Or you can write scripts to copy or ship logs to remote replicas or standby systems in a process called log shipping.

In many RDBMSs, like Db2 and SQL Server, transaction logging is enabled by default, but you still may want to change the default settings for improved recoverability and performance. In other RDBMSs, like MySQL, transaction logging is disabled by default and may need to be manually configured. 

For MySQL, you can run the SHOW BINARY LOGS command to check if logging is enabled and if it is, you will see a list of the log files.

Unlike other types of logs, such as diagnostic and error logs that are mostly in text format and human readable, the database transaction logs are typically in binary formats that are sometimes encrypted and require specialized tools to format and display contents. 

For example, in MySQL, you use the mysqlbinlog tool to view the contents of MySQL transaction log files, also known as binary logs. And for Db2, you use the db2fmtlog tool for formatting and displaying log file information. backup-11

A log file contains multiple entries, and the information they contain may vary depending on the RDBMS. For example, a typical database log record is made up of: A log transaction ID number, which is a unique ID for the log record. The database record type, which describes the type of database log record. A log sequence number, which is a reference to the database transaction generating the log record. A previous log sequence number is a link to the last log record. This implies that database logs are constructed in linked list form. And information which details the actual changes that triggered the log record to be written. backup-12

Perform Point-in-Time Backup and Restore

Objectives:
After completing this reading, you will be able to:

Understand the significance of Point-in-Time Recovery (PITR) in database management.
Gain an overview of key Linux commands used for conducting Point-in-Time Backup and Restore of MySQL Database.

Significance of Point-in-Time Recovery (PITR) in Database Management:
Point-in-Time Recovery (PITR) allows you to restore a database to a specific moment, often before an error or data corruption occurred. This capability is critical for minimizing data loss and maintaining data integrity, especially in environments where continuous availability and data accuracy are paramount.

PITR involves a combination of full backups and incremental backups (or binary logs in MySQL), enabling administrators to revert the database to any desired point in time. This method provides flexibility and precision in recovery operations, ensuring that only the necessary data changes are applied during the restoration process.

Key Techniques for Point-in-Time Backup and Restore:
This section explores the essential techniques and methods used for performing PITR in MySQL database systems.

Full Backups and Incremental Backups:
PITR relies on both full backups and incremental backups to provide a comprehensive recovery solution. These two types of backups work together to ensure that you can restore your database to any specific point in time.

Full Backups
A full backup captures the entire database at a specific point in time. It includes all the data, schema, and configurations necessary to restore the database to that state. Full backups serve as the foundation for PITR and are typically performed periodically (e.g., daily or weekly) to ensure that there is a recent baseline from which to start the recovery process.

Incremental Backups
Incremental backups capture only the changes made to the database since the last backup (full or incremental). This approach reduces the amount of data that needs to be backed up, saving time and storage space. In MySQL, binary logs are crucial for incremental backups as they record all database changes.

Binary Logs
Binary logs in MySQL are essential for PITR. They record all database changes (e.g., INSERT, UPDATE, DELETE statements) in a binary format. These logs enable the replay of changes during recovery, allowing you to restore the database to any specific point in time by applying the changes recorded after the last full backup.

How Binary Logs Work
1. Enable Binary Logging: Binary logging must be enabled on the MySQL server. This can be done by adding the log_bin directive to the MySQL configuration file (my.cnf) and restarting the MySQL server.

2. Recording Changes: Once binary logging is enabled, MySQL starts recording all changes to the database in binary log files. Each log file is given a sequential number (e.g., mysql-bin.000001, mysql-bin.000002).

3. Managing Log Files: As the number of binary log files increases, it’s important to manage them to prevent disk space issues. MySQL provides options to purge old binary logs that are no longer needed.

4. Point-in-Time Recovery: To perform a point-in-time recovery, you first restore the database from the most recent full backup. Then, you apply the changes recorded in the binary logs to bring the database to the desired state.

see Practical Steps for Point-in-Time Backup and Restore

see Performing MySQL Physical Backup and Restoration

database security: security-1.png security-2 security-3 Users and client applications need to be permitted to access a server or database and the objects in it. Firstly, users need to be authenticated on the server or database to enable them to access it. And then they need to be granted permissions on individual objects, or groups of objects, in the database to interact with them. Even when a user is authenticated on a database, they still need to be authorized to access the objects and data in that database. You authorize users by giving them permissions, or privileges, to access objects and data. Because groups of users often need the same access privileges, in most RDBMSs you can grant privileges to a group of users who undertake a specific role in that database. In addition to restricting users to only access the information and objects that their role requires, you should also consider monitoring and auditing database activity. You should track which users access a server or database and the actions that they perform. As well as securing your systems and data and auditing access to it, you could consider using encryption to add another layer to your security system. Do remember though, that encrypting and decrypting data can be a time and resource intensive operation, so you will need to consider the impact this will have on your operations and whether you need to upgrade your hardware accordingly. Do remember though, that encrypting and decrypting data can be a time and resource intensive operation, so you will need to consider the impact this will have on your operations and whether you need to upgrade your hardware accordingly. 

users, groups, and roles:  A database user is a user account that is allowed to access specified database objects. Depending on the DBMS and your security policies, a user might be explicitly created and authenticated within the database system, or may be created externally and authenticated using external authentication such as the operating system or external identity management services like Kerberos, LDAP, and Cloud IAM.

When you first create a user, they will generally have few, if any, permissions to actually interact with the database objects unless they are the creator of the database. User names are stored in system tables or catalog tables which you should not try to edit directly.

Some RDBMSs support the concept of user groups. In some cases, such as Postgres, you define groups in the database and they are logical groupings of users to simplify user management. In other systems, such as SQL Server and Db2, you can map a database group to an administrative group in the underlying operating system. This is particularly useful when you are using that operating system to authenticate your users.

Database roles are similar to database groups in that they confer privileges and access rights to all users of that role. A database role defines a set of permissions needed to undertake a specific role in the database. For example, a backup operator role would have permissions to access a database and to perform backup functions. Some RDBMSs have a set of predefined roles that you can use for your users, such as a database owner or backup operator role. And most enable you to create your own roles.

Assigning privileges to groups or roles, rather than individual users, greatly simplifies your security management tasks. If you know that a set of users all need access to the same functionality to fulfill their job role, you can put those users in one group and assign the relevant permissions to the group. If that job role changes in the future, it is quicker, easier, and less prone to mistakes to just add the new permissions to the group rather than individual users. Similarly, if a new member of staff joins the team, you can simply add them to the role or group, rather than having to assign all the separate permissions to them. One user can be a member of one or more roles.

Managing access to databases and their objects: In systems where users are authenticated outside the database (such as on the operating system or through an external plugin), you may also need to grant them access to the databases they will be working with. You can use the SQL GRANT CONNECT command to grant connection access to a particular database to a particular user, group, or role. To grant connection access to an individual user rather than a group or role, you simply specify the user name in place of the group name.

If your RDBMS provides a guest or public account, you may find that by default it has connect privileges to all databases. In this scenario it is a good practice to revoke that permission so that you ensure only users given explicit permission can access your data. see revoke.png

You use the SQL GRANT command to grant privileges on tables in a database. You can use similar statements to enable users to insert, update, and delete data by granting them the relevant privileges on a table or tables. see grant.png  To provide these privileges to an individual user, just replace the salesteam group name with the name of that user.

see grant-2

On a procedure or function, you can permit a user or group to view the code. This means that they can view the definition of that function or procedure, but they will not be able to run it or change it. To run the code, a user needs the execute permission. And to change the definition of a procedure, a user needs the alter permission. see grant-3

As well as granting privileges, at times you may want to remove them. You can use the revoke statement to remove granted privileges, and most RDBMSs will also provide revoke functionality in the user interface. However, because an individual’s privileges are a combination of those granted to all the groups or roles they belong to, a member of the sales team role may still be able to access this table through their membership of another role. If you want to ensure that users do not have permission for a certain object or action, you can use the deny statement to override any previous grant of that permission. see deny.png

auditing database activity: auditing-1.png Auditing a database involves recording the access and the activity of database users on your database objects. By reviewing such records, also known as logs, you can identify suspicious activity and quickly respond to any security threats you find.

If an unauthorized user accesses your database, they have already overcome one or more of the levels of security in your system. Therefore, it is imperative that you track who accesses your database and review the information to identify any unauthorized users. You should also track failed attempts to access the database, as these can help you to identify potential attacks, such as brute force attempts, on your system.

Most database systems provide functionality that you can use to audit database access. For example, in Db2 on Cloud you can use the user validation category of the built-in audit facility to log user authentication events. In MySQL the audit log plugin tracks connect and disconnect events. Sometimes authorized users may find that they can view and edit data that they should not have permissions on. If you track all user activity, you can then use the output reports to review which users are accessing which tables and check whether those actions match your security plans. 

To audit database activity, some RDBMSs use triggers. These are special stored procedures that automatically log the activity after a DML statement event, such as an insert, occurs. Other systems enable you to attach actions to the events that occur in the database. In Db2 on Cloud you can use the change history event monitor to enable auditing on specific tables or objects. And in Postgres you can use the downloadable pgAudit tool to log individual action types or all actions in a database. 

encrypting data: encrypt-1.png It is important to encrypt data at rest to ensure that malicious users cannot directly access the data files, bypassing your database security measures. Some RDBMSs provide database-level encryption for the entire database. This is often termed transparent data encryption, or TDE. Some provide table or tablespace encryption for individual tables. And some provide column level for maximum granularity and flexibility. Do note though, that the flexibility that column level encryption provides is counteracted by performance degradation and complexity of set up.

You encrypt data by using an algorithm to change the data into an unreadable string, commonly known as ciphertext. These algorithms use a key to ensure that anyone trying to decipher the text cannot do so without the key. The simplest form of encryption is symmetric encryption, where the same key is used to encrypt and decrypt the data. Mechanisms such as data encryption standard, or DES, and advanced encryption standard, or AES, use symmetric keys. When using only one key, it must be shared between all parties wanting to access that data so that they can all decrypt it. Sharing a key increases the likelihood of it being compromised and if the key to the encryption is compromised, your data is effectively compromised. Alternatively, some databases use asymmetric encryption, also known as public key encryption. This uses two keys: a public key and a private key, known as a key pair. You encrypt the data with the public key and valid users have a unique matching private key to decrypt it. Rivest-Shamir-Adleman (RSA) and elliptic curve cryptography (ECC) are both examples of asymmetric encryption. 

Most modern databases take care of the complexity of encryption and key management for you. Many of them support transparent data encryption, or TDE. The “transparent” part of its name alludes to the fact that the encryption and decryption is not visible to your users. The database engine encrypts the data before storing it and then decrypts it when an authorized user or application requests information. Because the database engine is performing the encryption tasks, it also encrypts your database backups too. Although TDE simplifies the process of encrypting your database, you may have concerns about storing data in the cloud and relinquishing the key management process to a third party. Some cloud databases support customer managed keys, also known as the Bring Your Own Key (or BYOK) protocol. This provides a separation of responsibility, so that your cloud database system is responsible for the encryption process, but you retain control of the keys that it uses. BYOK provides an extra level of protection by ensuring that the cloud platform storing and encrypting your data does not have access to the decryption keys, so it cannot access your confidential data. It also enables your security admins to manage your corporate keys, while your database admins can continue to manage the data. And for businesses whose customers require regulatory compliance around key rotation and expiration, you have complete control over the keys and their lifecycle. 

Some RDBMSs support both symmetric and asymmetric encryption and enable you to configure which to use. Because of the time and effort that it takes the algorithm to process the data, any type of encryption and decryption will decrease the performance of your database. Because asymmetric algorithms generally use longer keys, they tend to have a greater performance impact than symmetric algorithms. Therefore, unless you specifically require asymmetric encryption, it is often best to use the symmetric algorithms such as AES and DES.

As well as encrypting data at rest, you should also consider encrypting it in transit, that is, when it is being transmitted across networks or the internet. As with data at rest encryption, many RDBMSs provide this functionality for you. Some cloud databases encrypt data in transit using transport layer security (or TLS) protocol, while others use the secure sockets layer (SSL) protocol. Some will encrypt your data by default, while others use a configuration setting that you can enable and disable. Similarly to at rest encryption, encrypting data in transit will affect the overall performance of your system.

see MySQL User Management, Access Control, and Encryption

database monitoring: For many database admins, one of the most challenging and necessary elements of database management is performance tuning, and one of the critical parts of this process is database monitoring. The term database monitoring refers to the different tasks related to the scrutinization of the day-to-day operational status of your database. Database monitoring is crucial to maintain the health and performance of your relational database management system, regardless of which vendor’s database product you are using.

When you perform regular database monitoring it helps to identify issues in a timely manner so that you can maintain the health and accessibility of your database system. If you do not perform this monitoring function, then problems and outages in your database might go undetected until it is too late. This can cause your users and customers to lose confidence in your service and potentially your organization could lose customers and income because of it. 

Most relational database management systems offer tools that enable you to observe the current state of your databases and to track their performance as circumstances vary over time. 

As a database admin, you can then utilize this useful information to perform several database monitoring tasks, including: Forecasting your future hardware requirements based on database usage patterns. Analyzing the performance of individual applications or database queries. Tracking the usage of indexes and tables. Determining the root cause of any system performance degradation. Optimizing database elements to provide the best possible performance. And assessing the impact of any optimization activities.

The following areas typically have the greatest effect on the performance of your database system: System hardware resources, network architecture, operating system, database applications, and client applications.

There are two ways to monitor operations in your database. You can view information that shows the state of various elements of your database in real time using monitoring table functions. For example, you can use a monitoring table function to examine the total amount of space used in a table. These table functions let you examine monitor elements and metrics that report on virtually all aspects of database operations. The monitoring table functions use a lightweight, high-speed monitoring infrastructure. Alternatively, you can set up event monitors to capture historical information as specific types of database events occur over a given time period. Event monitors capture information about database operations over time, as specific types of events occur. For example, you can create an event monitor to capture information about locks and deadlocks as they occur in the system. Or you might create an event monitor to record when a threshold that you specify (for example, the total processor time used by an application or workload) is exceeded. Event monitors generate output in different formats and can write this output to regular tables. Some event monitors have additional output options. 

Regardless of which monitoring tools you use, the relevant information is obtained by using several key performance indicators, or KPIs, which are more commonly referred to as ‘metrics.’ Database performance is measured by using these key database performance metrics, which enable database admins to effectively optimize their organizations’ databases for best performance. 

Apart from monitoring for performance reasons, regular monitoring is also useful for operations, availability, and security purposes. The ultimate goal of monitoring is to identify and prevent issues from adversely affecting database performance. However, some issues that occur may be caused by your hardware, or perhaps your software, or possibly your network connections, or even the queries that you execute, or some other unknown factor. 

Therefore, to efficiently and successfully monitor the usage and performance of your databases, you need to monitor at several distinct levels within your database environment. The four monitoring levels are: The infrastructure level, the platform (or instance) level, the query level, and the user (or session) level.

infrastructure level: It’s vital that all the underlying infrastructure components, such as operating systems, servers, storage hardware resources, and network components be working efficiently under the hood of the database platform and the queries. 

instance or database platform level: Irrespective of whether you’re handling relational database systems such as Db2, PostgreSQL, MySQL, or any other flavor of relational database system for that matter, or if you’re managing a combination of more than one of them together, each platform is a consideration in terms of performance. Database monitoring at the platform level is crucial because it offers holistic insight into all the elements necessary to maintain consistent database performance.

query level: Typically your line-of-business applications will be repeatedly running queries against your database instances, and then formatting and returning the relevant results of those queries to your users. The majority of bottlenecks that might occur at this level will primarily be due to inefficient query statements that may cause latency, mishandle errors and diminish query throughput and concurrency.

user or session level: This level can often be the most misleading monitoring level of them all, because if your users are complaining about something not working, then you can just obtain further information about the issue they are having, investigate it, and hopefully fix it. But what if your users aren’t currently complaining about things not working? Can you just assume that everything is running fine, put your feet up, and relax? Unfortunately, no. Just because there may not be an issue right now, doesn’t mean that there won’t be one just around the corner. Truly successful monitoring means constantly monitoring the usage, performance, and behavior of your database system in a proactive and continuous manner. The ‘nirvana’ of database monitoring is achieved when you proactively identify issues before your users are even aware of them. Monitoring at all four of these levels is crucial to maintaining service level agreements (or SLAs), such as high availability, high uptime, and low latency for your databases.

Some of the key metrics for monitoring the usage and performance of your database include the following: Database throughput is one of the most significant metrics of database performance. It indicates how much total work is being taken on by your database and is typically measured by the number of queries executed per second. Database resource usage monitors the database resource usage by measuring the CPU, memory, log space, and storage usage. This summary metric represents the database resource usage by two aspects: average/max/latest number and time series number. Database availability signals whether the database is up or down, that is, available or unavailable. It is typically a summary metric that represents the historical data on available time as a percentage. Database responsiveness shows how well the system is responding to inbound requests and is another of the more commonly used database performance metrics. It provides DBAs with information on the average response time per query for their database servers, indicating how quickly they respond with query results. Database contention indicates whether there is any contention between connections, by measuring lock-waits and concurrent database connections. Database contention is the term used to describe what happens when multiple processes are competing to access the same database resource at the same time. and units of work tracks what transactions (units of work) are consuming the most resources on the database server. 

Connections can display all kinds of network connection information to a database management console and can indicate whether a database server might fail due to long-running queries or having too many open connections. Database connections are network connections that enable communication between clients and database software. Open connections are used for sending commands and receiving responses in the form of result sets. 

Most frequent queries tracks the most frequent queries received by your database servers, including their frequency and average latency, that is, how long they take to be processed. It can help DBAs optimize these queries to gain substantial performance improvements. 

Locked objects shows detailed information about any locked processes and the process that blocked them. Locks and blocks stop several concurrent transactions from accessing an object at the same time. They put contending processes on hold until that object has been released and is accessible again. 

Stored procedures displays the aggregated execution metrics for procedures, external procedures, compiled functions, external functions, compiled triggers, and anonymous blocks invoked since database activation. 

Buffer pools tracks the usage of buffer pools and table spaces. A directory server uses buffer pools to store cached data and to improve database performance. When the buffer pool fills up, it removes older and less-used data to make room for newer data.

top consumers shows the top consumers of a system’s resources and can help DBAs with capacity planning and resource placement. Please note, this is just a small selection of the numerous metrics available in most database management systems. 

To review these key database performance metrics, you will use different out-of-the-box tools, depending on your database. For example, in Db2 you can use tools such as the Db2 Data Management Console, the workload manager, and snapshot monitors. In PostgreSQL, you can use the pgAdmin dashboard, which is a very popular open-source query-monitoring tool for PostgreSQL systems. In MySQL, you can utilize the Performance Dashboard, Performance Reports, and Query Statistics tools in the MySQL Workbench, and you can use the MySQL Query Profiler to identify slow running queries. 

There are several third-party performance and query monitoring tools available to help you to monitor queries, optimize performance, and speed up your database. Some of them may also provide performance tuning and query optimization capabilities as well. For PostgreSQL systems, there is the pganalyze tool which provides automatic insights into query plans, query analysis, database visualization and dashboards, log insights, and performance optimization and monitoring. And if you have a PostgreSQL, MySQL, SQL Server or Oracle database and need a query-monitoring tool, you could try PRTG Network Monitor. 

Many of the third-party tools are available for multiple database systems, such as SQL Server, Oracle, Sybase, Db2, MySQL, and PostgreSQL to name but a few. SolarWinds offers a subscription-based monitoring solution called Database Performance Analyzer. Foglight for Databases from Quest enables you to proactively monitor your complete database environment and view the state of all database platforms in a single comprehensive interface. And Datadog is a SaaS platform that includes multiple system monitoring tools. It has more than 450 built-in integrations to connect to multiple different database systems.

why do you need to optimize your databases? Well, over time as the volume of data stored in your databases increases, and their operational workloads increase, data can become fragmented, tables can be left empty or partially empty, and database performance can suffer. By optimizing your databases, you can identify bottlenecks, fine-tune database queries, and ultimately reduce database response times for your users.

Each relational database system has its own utilities or commands for optimizing its databases. For example, for MySQL databases you can use the OPTIMIZE TABLE command. For PostgreSQL databases you can use the VACUUM and REINDEX commands. And in Db2 you can use the RUNSTATS and REORG commands. If your MySQL database receives a significant number of insert, update, or delete operations, it can lead to fragmentation of your data files, meaning that a lot of unused space is wasted, which in turn can impact the database’s performance. Therefore, it is recommended that database admins defrag their MySQL tables on a regular basis.

In MySQL, the OPTIMIZE TABLE command reorganizes the physical storage of table data and associated index data, to reduce storage space and improve input/output efficiency when accessing a table. To use the OPTIMIZE TABLE command, you need to have SELECT and INSERT privileges for the table you are working on. You could also use a graphical tool such as phpMyAdmin to optimize tables in MySQL.

The VACUUM command can be used on your PostgreSQL databases to perform garbage collection, and optionally, analysis tasks. VACUUM reclaims lost storage space consumed by ‘dead’ tuples, which are not physically removed from their database tables after being deleted or made obsolete by an update during routine PostgreSQL operations. Regularly reclaiming this lost storage space can help improve your databases’ overall performance in PostgreSQL. Please note that if the autovacuum feature is enabled, then PostgreSQL will automate the vacuum maintenance process for you.

In PostgreSQL, you can use the REINDEX command to rebuild one or more indexes. REINDEX rebuilds an index using the data stored in the index's table and replaces the previous, older, version of the index. It can be used when software bugs or hardware failures have corrupted an index, or when an index has become bloated and contains numerous empty, or nearly empty, pages. You need to be the owner of the index, or the table, or the database in order to reindex it. When you use REINDEX, it rebuilds the index contents from scratch, and therefore it has a very similar effect as dropping and recreating an index.

In Db2, the RUNSTATS command updates statistics in the system catalog about the characteristics of a table, associated indexes, or statistical views. These characteristics include number of records, number of pages, and average record length. The optimizer uses these statistics to determine access paths to the data. For a table, call the RUNSTATS command when the table has had many updates, or after the table is reorganized. For tables, this command collects statistics for a table on the database partition from which it is invoked. If the table does not exist on that database partition, the first database partition in the database partition group is selected. For a statistical view, call the RUNSTATS command when changes to underlying tables substantially affect the rows that are returned by the view. The view must be previously enabled for use in query optimization by using the ALTER VIEW statement. For views, this command collects statistics by using data from tables on all participating database partitions. 

In Db2, the REORG TABLE command reorganizes a table by reconstructing the rows to eliminate fragmented data, and by compacting information. On a partitioned table, you can reorganize a single partition. In terms of its scope, the REORG TABLE command affects all database partitions in the database partition group. 

In Db2, the REORG INDEX command reorganizes indexes. You can reorganize all indexes that are defined on a table by rebuilding the index data into unfragmented, physically contiguous pages. On a data partitioned table, you can reorganize a specific nonpartitioned index on a partitioned table, or you can reorganize all the partitioned indexes on a specific data partition. If you specify the CLEANUP option of the index clause, cleanup is performed without rebuilding the indexes. You can clean up a specific index on a table, or you can clean up all of the indexes on the table. In terms of its scope, the REORG INDEX command affects all database partitions in the database partition group. 

If you're receiving feedback that it's taking too long to get the results of a query There's a process you can follow to evaluate and resolve the problem Begin by verifying how long the query takes Is it an intermittent issue, a permanent problem, or user perception? Once you have verified the issue You can use RDBMS or third-party tools to diagnose the problem Based on the diagnosis, you can take steps to resolve the problem

You can use several mechanisms and tools to determine the actual time a query is taking These tools include specialized logs like the MySQL slow query log You should also benchmark expected query performance yourself Using tools like timing in PostgreSQL Or using the shell time command when executing a query When determining the duration of a query You must recognize what the tool you are using is measuring Is it measuring only server-side computational effort? Or does it include network latency as the command and results travel between client and server? 

For example, in PostgreSQL The time that timing returns includes the network latency If you're connecting to a remote server 

A query execution plan or query plan is the series of steps used to access data when running SQL statements. An RDBMS will often provide several methods for returning the detail of a query execution plan. Some offer tools that create a graphical representation of their query plans; while others allow a distinct mode to be set on the connection, which causes the RDBMS to return a text-based description of their query plans. Lastly, some RDBMSs allow you to obtain a query plan By using an EXPLAIN statement to query a virtual database table after running the query. 

Most RDBMSs have a query optimization feature that uses a query optimizer tool to calculate the most efficient method For executing a query by evaluating all the available query execution plans. When a query gets submitted to the database The query optimizer evaluates the various possible query execution plans and returns what it determines to be the best choice. However, query optimizers can be fallible, so database admins will sometimes need to manually inspect and fine-tune the plans produced by the query optimizer to get optimum query performance. 

Some RDBMSs allow you to provide hints to the query optimizer. A hint is an additional component to the SQL statement That informs the database engine about how it wants it to execute a query; such as instructing the database engine to use an index when executing the query. Even though the query optimizer might have decided not to. All flavors of RDBMSs, such as DB2, MySQL, and PostgreSQL have an EXPLAIN statement that you can use to show a text-based representation Of the details of a query execution plan for a statement; including the processes that occur and in what order they occur. An EXPLAIN statement can be a good way to swiftly cure slow-running queries. 

Some RDBMSs also provide a graphical version of the EXPLAIN statement. For example, DB2's Visual Explain uses information from a number of sources to enable you to view the access plan for explained SQL or XQuery statements as a graph. You can use the information available from the access plan graph To tune your queries for better performance. For PostgreSQL systems, the pgAdmin utility provides a graphical EXPLAIN plan feature. Although this is not an entire substitute for EXPLAIN or EXPLAIN ANALYZE text plans, tt does offer a fast and simple method for viewing plans for additional analysis. For MySQL systems, the MySQL Workbench provides a Visual Explain plan that produces and presents a visual representation of the MySQL EXPLAIN statement. MySQL Workbench provides all of the EXPLAIN formats for executed queries including the standard format, the raw extended JSON format, and the Visual Query plan. 

In addition to the tools supplied in the RDBMS There are many third-party query optimization tools to choose from.SolarWinds Database Performance Analyzer, which features TopWeights for SQL, which displays network state and performance; Database status, including wait time, tuning, CPU, memory, and disk statistics; and color-coded graphs for each information category. SolarWinds Database Performance Analyzer supports Azure SQL, MySQL, Oracle, MariaDB, and IBM DB2

dbForge Studio, which features Query Builder and Query Profiler; which are query optimization tools for tuning queries and investigating query performance issues; code Explorer for inspecting or writing query code; report Designer for sending performance issue feedback to your team; and Index Manager for resolving index fragmentation. dbForge Studio has versions for SQL Server, Oracle, MySQL, and PostgreSQL. 

Finally, we have EverSQL Query Optimizer, which features Automatic query rewriting, code comparison and change notes after query rewriting, and indexing recommendations for improving query speed. EverSQL Query Optimizer supports MySQL, MariaDB, and PerconaDB. 

Query plans and explain can help identify the cause behind a performance issue, but how can you resolve it? Understanding your data and how it behaves is key for making good query design decisions including using the correct data types, when and where to use indexes, and how to size your buffer pools and caches. Some query performance tools will also help you understand how to write efficient queries. Techniques you can use include Avoiding complex queries that contain many subqueries, especially when using joins. Using variables to hold data for use in later queries, and being specific about the data you select from tables Select named columns, rather than using SELECT *.

using indexes: index-1.png index-2 In a database, the index is essentially an ordered copy of selected columns of table data and is designed to enable efficient searches without having to search every row in a database table every time it is accessed. The columns selected to make an index are typically defined by administrators and can be based on many factors, such as frequently searched terms. index-3. Regardless of how they are constructed, the overall goal of an index is to help users find the information they need quickly and easily. An index can include one or more columns in the table.

However, while indexes can improve database performance, they also require additional storage space and maintenance because they must be continually updated. For any database, you will likely have to experiment with different types of indexes and indexing options. This will help you find an optimal balance of query performance and index maintenance—such as how much disk space and activity are required to keep the index up to date.

For example, narrow indexes, or indexes with relatively few columns in the index key, take up less disk space and require less overhead to maintain, whereas wide indexes, while they may cover a greater number of queries, also require more space and maintenance. 

Each table can also have any number of additional or secondary indexes other than the primary key. These are non-clustered and can have single or multiple columns. Indexes can be defined as unique or non-unique. Ordering of columns is important because an index will be sorted in the order in which columns are specified when the index is created. Sorting can be ascending (which is the default) or descending, but it will sort by the first column first, then by the next column, and so on. index-4 

The primary key of a table uniquely identifies each row in a table. While having a primary key is optional in some RDBMSs, it is generally a good practice to create one when creating a table. It may also be possible to add a primary key later by altering the table. index-5

If the primary key consists of multiple columns, you can even specify the primary key constraint after defining all the columns. You simply add the PRIMARY KEY option then specify the column names in parentheses. see index-6

Since primary keys need to have unique values, one way to ensure that is to create automatically generated incrementing values for the primary key columns. In Db2, these are called IDENTITY columns and they can be created by specifying the keywords GENERATED BY DEFAULT AS IDENTITY after the column definition. And in MySQL, you simply add the AUTO underscore INCREMENT keyword after the column definition. see index-7

The basic syntax for creating an index is shown in the first code block. You use the CREATE INDEX statement to define an index, and you use the ON statement to specify on which database table to create it. You specify the column names in parentheses, and the order in which you enter them dictates the index’s sort order. index-8

To delete an index, you use the DROP statement. Dropping an index does not cause any other objects to be dropped but will cause packages that have a dependency on a dropped index or index specification to be invalidated. index-9

One of the major sources of database application bottlenecks can be poorly designed indexes, or a lack of sufficient, appropriate indexes. Therefore, designing efficient indexes is vital for optimizing database application performance. When designing an index, you should consider the following core principles: Understand how the database will be used. For example, will it be used for heavy online transaction processing or for the storage and retrieval of large data sets? Understand the most frequently used queries. For example, if you know that a frequently used query joins two or more tables, that may assist you in deciding the most appropriate index type to use. Understand the characteristics of the columns. For example, what kind of data will they contain? Will they contain integer values? And will they be unique?

You will also need to consider: When creating or maintaining an index, which indexing options will best improve its performance? For example, if you are creating a clustered index on an existing large table, it would be a good idea to use the ONLINE index option, as it permits concurrent activity on the underlying data while the index is being built. And where is the best place to store the index? Where you decide to store your indexes can improve query performance by increasing the performance of disk input/output operations. For example, if you store a non-clustered index on a storage group located on a different disk than where the table storage group is located, it can improve performance as both disks could be read simultaneously.

see Overview of SQLite and Key Concepts in Datasette

see Improving Performance of Slow Queries in MySQL

see Improving Performance of Slow Queries in MySQL lab

see Monitoring and Optimizing Your Databases in MySQL lab