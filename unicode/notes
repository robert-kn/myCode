https://www.cl.cam.ac.uk/~mgk25/unicode.html#utf-8

Unicode now replaces all encoding standards (including ASCII). With it, users can now handle pratically any 
script and language used on this plant. It also supports a comprehensive set of mathematical and technical 
symbols to simplify scientific information exchange.

With UTF-8 encoding, unicode can be used in a convenient and backwards compatible way in environments that 
were designed entirely around ASCII, like unix. UTF-8 is the way in which unicode is used under unix, linux, 
and similar systems.

What are UCS and ISO 10646?

The international organisation for standardisation defines the universal character set in ISO 10646. UCS is 
a superset of all character standards.

In the late 1980's there were 2 independent attempts to create a single unified character set. One was ISO 
10646 that was a project of the international organisation for standardisation, the other was the unicode 
project that was organised by a consortium of (initially mostly US) manufacturers of multi lingual software. 
They both joined efforts in 1991 and worked together to create a single code table (i.e. a table that maps 
characters to integers).

Both projects still exist and publish their standards independently, however, there exists an agreement between 
both to keep the code table of the unicode and ISO 10646 standards compatible whilst closely coordinating 
further extensions.

Both tables assign a code point to each character and an official name. New characters are still being added 
on a continuous basis, but existing characters will not be changed anymore and are stable. 

For scripts not yet covered, research on how to best encode them for computer usage is still going on and they 
will be added eventually.

ISO 10646 originally defined a 31 bit character set. The most commonly used characters, including all of those 
found in older encoding standards, have been placed in the first plane (0x0000 to 0xfffd) which is called the 
basic multilingual plane (BMP) or plane 0. The characters that were later added outside the 16 bit BMP are 
mostly for specialist applications such as historic scripts and scientific notation. Current plans are that 
there will never be characters assigned outside the 21 bit code space from 0x000000 to 0x10ffff which covers a
bit over one million potential future characters.

What are combining characters? 

They are similar to non spacing accent keys on a type writer. A combining character is not a full character by 
itself, it is an accent or a diacritical mark that is added to the previous character. This way, it is possible 
to place any accent on any character. The most important accented characters, like those used in common languages, 
have codes of their own in UCS to ensure backwards compatibility with older character sets. They are known as 
pre-composed characters.

What are UCS implementation levels?

Not all systems can be expected to support all advanced mechanisms of UCS hence ISO 10646 specifies the 
following 3 implementations levels:

level 1:
Combining characters and hangul jamo characters (required to fully support the korean sript) are not supported.

level 2:
like level 1, however in some scripts, a fixed list of combining characters is now allowed. These scripts cannot be represented adequately in UCS without support for atleast certain combining characters.

level 3:
all UCS characters are supported. 

How do ISO 10646 and unicode correspond?

Unicode 1.1 corresponded to ISO 10646-1:1993, Unicode 3.0 corresponded to ISO 10646-1:2000 etc see wikipedia 
for completing mapping.

Differences between Unicode and ISO 10646?

The Unicode Standard published by the Unicode Consortium corresponds to ISO 10646 at implementation level 3. 
All characters are at the same positions and have the same names in both standards.

The Unicode Standard defines in addition much more semantics associated with some of the characters and is in 
general a better reference for implementors of high-quality typographic publishing systems. Unicode specifies 
algorithms for rendering presentation forms of some scripts (say Arabic), handling of bi-directional texts 
that mix for instance Latin and Hebrew, algorithms for sorting and string comparison, and much more.

The ISO 10646 standard on the other hand is not much more than a simple character set table, comparable to the 
old ISO 8859 standards. 

What is UTF-8?

UCS and Unicode are first of all just code tables that assign integer numbers to characters. There exist 
several alternatives for how a sequence of such characters or their respective integer values can be 
represented as a sequence of bytes. ISO/IEC 10646 and Unicode define several encoding forms of character sets. 
UTF-8, UCS-2, UTF-16, UCS-4 and UTF-32. All of these encoding standards bar UTF-8 have an encoding unit larger 
than one octet, making them hard to use in many current applications and protocols that assume 8 or even 7 bit 
characters.

Unless otherwise specified, the most significant byte comes first in these (Bigendian convention). An ASCII or
Latin-1 file can be transformed into a UCS-2 file by simply inserting a 0x00 byte in front of every ASCII byte.
If we want to have a UCS-4 file, we have to insert three 0x00 bytes instead before every ASCII byte.

Using UCS-2 (or UCS-4) under Unix would lead to very severe problems. Strings with these encodings can contain 
as parts of many wide characters bytes like “\0” or “/” which have a special meaning in filenames and other C
library function parameters. In addition, the majority of UNIX tools expects ASCII files and cannot read 16-bit 
words as characters without major modifications. For these reasons, UCS-2 is not a suitable external encoding 
of Unicode in filenames, text files, environment variables, etc.

UTF-8 has a one-octet encoding unit.  It uses all bits of an octet, but has the quality of preserving the full 
US-ASCII [US-ASCII] range. 

UTF-8 encodes UCS characters (>U+007F) as a varying number of octets (sequence of several bytes), where the 
number of octets, and the value of each, depend on the integer value assigned to the character in ISO/IEC 
10646 (the character number, a.k.a. code position, code point or Unicode scalar value). 

The first octet (byte) of a multi-octet sequence that represents a non-ASCII character indicates the number of 
octets in the sequence and is always in the range 0xC0 to 0xFD. All further bytes in a multibyte sequence are 
in the range 0x80 to 0xBF. This allows easy resynchronization and makes the encoding stateless and robust 
against missing bytes.

All possible 2^(31) UCS codes can be encoded. UTF-8 encoded characters may theoretically be up to six bytes 
long, however 16-bit BMP characters are only up to three bytes long.

The following byte sequences are used to represent a character. The sequence to be used depends on the Unicode 
number of the character:

U-00000000 – U-0000007F:	0xxxxxxx
U-00000080 – U-000007FF:	110xxxxx 10xxxxxx
U-00000800 – U-0000FFFF:	1110xxxx 10xxxxxx 10xxxxxx
U-00010000 – U-001FFFFF:	11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
U-00200000 – U-03FFFFFF:	111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
U-04000000 – U-7FFFFFFF:	1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx

The xxx bit positions are filled with the bits of the character code number in binary representation. The 
rightmost x bit is the least-significant bit. Only the shortest possible multibyte sequence which can 
represent the code number of the character can be used. Note that in multibyte sequences, the number of 
leading 1 bits in the first byte is identical to the number of bytes in the entire sequence.

Examples: The Unicode character U+00A9 = 1010 1001 (copyright sign) is encoded in UTF-8 as

11000010 10101001 = 0xC2 0xA9

and character U+2260 = 0010 0010 0110 0000 (not equal to) is encoded as:

11100010 10001001 10100000 = 0xE2 0x89 0xA0

The official name and spelling of this encoding is UTF-8, where UTF stands for UCS Transformation Format. 
Please do not write UTF-8 in any documentation text in other ways (such as utf8 or UTF_8), unless of course 
you refer to a variable name and not the encoding itself.

What different encodings are there?

Both the UCS and Unicode standards are first of all large tables that assign to every character an integer 
number. If you use the term “UCS”, “ISO 10646”, or “Unicode”, this just refers to a mapping between characters 
and integers. This does not yet specify how to store these integers as a sequence of bytes in memory.

BY now, you already know that ISO 10646 defines UCS-2 and UCS-4 encodings which are sequences of 2 bytes and 4
bytes per character respectively. You also know that ISO 10646 was from the beginning designed as a 31 bit
character set (with possible code positions ranging from U-00000000 to U-7fffffff). UCS-4 can represent all 
UCS and Unicode characters, UCS-2 can represent only those from the BMP (U+0000 to U+FFFF).

“Unicode” originally implied that the encoding was UCS-2 and it initially didn’t make any provisions for 
characters outside the BMP (U+0000 to U+FFFF). When it became clear that more than 64k characters would be 
needed for certain special applications (historic alphabets and ideographs, mathematical and musical 
typesetting, etc.), Unicode was turned into a sort of 21-bit character set with possible code points in the 
range U-00000000 to U-0010FFFF. 

No endianess is implied by the encoding names UCS-2, UCS-4, UTF-16, and UTF-32, though ISO 10646-1 says that 
Bigendian should be preferred unless otherwise agreed. It has become customary to append the letters “BE” 
(Bigendian, high-byte first) and “LE” (Little endian, low-byte first) to the encoding names in order to 
explicitly specify a byte order.

In order to allow the automatic detection of the byte order, it has become customary on some platforms 
(notably Win32) to start every Unicode file with the character U+FEFF (ZERO WIDTH NO-BREAK SPACE), also known 
as the Byte-Order Mark (BOM). Its byte-swapped equivalent U+FFFE is not a valid Unicode character, therefore 
it helps to unambiguously distinguish the Bigendian and Littleendian variants of UTF-16 and UTF-32.

****************************************************************************************************************


Computers were originally built to process numbers. Over the last few decades, they have become increasingly
better at handling text as well, but the transition to from human scribbling and typography to bits and bytes
has been complicated. Going from a paper document to a computerised representation of that document means 
learning about how the computer handles text, and requires learning about characters, character codes, fonts
and encodings.

Computers use two basic data types in most of their processing: characters and numbers. These basic types are
combined in various ways to create strings, arrays, records and other data structures. Inside the computer 
characters are numbers but the way these numbers are handled is very different from numbers meant for calculation.

Computers internally work on numbers. This means that characters need to be coded as numbers. A typical 
arrangement is to use numbers from 0 to 255 (one octet) because that range fits into a basic unit of data storage.

When you define how those numbers correspond to characters, you define a character code. There are quite a 
number of character codes defined and used in the world. Most of them have the same assignments for numbers 
0 to 127, used for characters that appear in english as well as many other languages. the letters a–z plus 
their uppercase equivalents, the digits 0–9, and a few punctuation marks.

For French texts, for example, you need additional characters such as accented letters These can be provided 
by using code numbers in the range 128–255 in addition to the ASCII range, and this gives room for letters 
used in most other Western European languages as well. Thus, you can use a single character code, called 
Latin 1, even for a text containing a mixture of English, French, Spanish, and German, because these languages 
all use the Latin characters with relatively few additions.

However, you quickly run out of numbers if you try to cover too many languages within 256 characters. For this
reason, different character codes were developed. For example, Latin 1 is for Western European languages, 
Latin 2 for several languages spoken in Central and Eastern Europe, and additional character codes exist for 
Greek, Cyrillic, Arabic, etc.

Character codes that use only the code numbers from 0 to 255 are called 8-bit codes, since such code numbers 
can be represented using 8 bits.

Things change when you need to combine languages in one document and the lan- guages are fundamentally 
different in their use of characters. In an English-German or French-Spanish glossary, for example, you can 
use Latin 1. In English-Greek data, you can use one of the character codes developed for Greek, since these 
codes contain the ASCII characters. But what about French-Greek? That’s not possible the same way, since the 
character codes discussed above do not support such a combination. A code either has Latin accented letters 
in the “upper half” (the range of 128–255), or it has Greek letters (α, β, γ, etc.) there. It would be 
impractical, and often impossible, to define 256-character codes for all the possible language combinations.


As you probably know, the number of characters needed for Chinese and Japanese is very large. They just would 
not fit into a set with only 256 characters. Therefore, different strategies are used. For example, 2 bytes 
(octets) instead of one might be used for one character. This would give 65,536 possible numbers for a 
character. On the other hand, the character codes developed for the needs of East Asian languages do not 
contain all the characters used in the world.

The solution to such problems, and many other problems in the world of growing information exchange, is the 
introduction of a character code that gives every character of every language a unique number.

This number does not depend on the language used in the text, the font used to display the character, the 
software, the operating system, or the device. It is universal and kept unchanged. The range of possible 
numbers is set sufficiently high to cover all the current and future needs of all languages.

The solution is called Unicode