{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 Helvetica;
\f3\fmodern\fcharset0 Courier-Bold;\f4\fswiss\fcharset0 Arial-BoldMT;\f5\fswiss\fcharset0 ArialMT;
\f6\fmodern\fcharset0 Courier;\f7\froman\fcharset0 Times-Italic;}
{\colortbl;\red255\green255\blue255;\red24\green25\blue27;\red255\green255\blue255;\red0\green0\blue0;
\red254\green246\blue217;\red24\green24\blue24;\red251\green226\blue224;\red227\green236\blue254;\red0\green0\blue0;
\red11\green84\blue83;\red115\green0\blue2;\red82\green0\blue83;\red83\green85\blue2;}
{\*\expandedcolortbl;;\cssrgb\c12549\c12941\c14118;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;
\cssrgb\c99608\c96863\c87843;\cssrgb\c12157\c12157\c12157;\cssrgb\c98824\c90980\c90196;\cssrgb\c90980\c94118\c99608;\cssrgb\c0\c0\c0\c5098;
\cssrgb\c0\c40000\c40000;\cssrgb\c53333\c0\c0;\cssrgb\c40000\c0\c40000;\cssrgb\c40000\c40000\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat7\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat8\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid20}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}}
\paperw11900\paperh16840\margl1440\margr1440\vieww33100\viewh16740\viewkind0
\deftab720
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Overview\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this lab, you:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Deal with late data.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Deal with malformed data by:\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Writing a composite transform for more modular code.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Writing a transform that emits multiple outputs of different types.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Collecting malformed data and writing it to a location where it can be examined.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 The end of the previous lab introduces one sort of challenge that real-time pipelines must contend with: the gap between when events transpire and when they are processed, also known as lag. This lab introduces Apache Beam concepts that allow pipeline creators to specify how their pipelines should deal with lag in a formal way.\
But lag isn\'92t the only sort of problem that pipelines are likely to encounter in a streaming context: whenever input comes from outside the system, there is always the possibility that it will be malformed in some way. This lab also introduces techniques that can be used to deal with such inputs.\
The final pipeline in this lab resembles the picture below. Note that it contains a branch.\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \cb1 \strokec4 {{\NeXTGraphic fbr1memtxi3rosTr3yGxhisB23mr4XlSayXhqCZUfK4=.png \width13160 \height17440 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Jupyter notebook-based development environment setup\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 For this lab, you will be running all commands in a terminal from your notebook.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud Console, on the\'a0
\f0\b Navigation Menu
\f1\b0 , click\'a0
\f0\b Vertex AI > Workbench
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Enable\'a0
\f0\b Notebooks API
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the Workbench page, click\'a0
\f0\b CREATE NEW
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the\'a0
\f0\b New instance
\f1\b0 \'a0dialog box that appears, set the region to\'a0
\f3\b \cb5 region
\f1\b0 \cb3 \'a0and zone to\'a0
\f3\b \cb5 zone
\f1\b0 \cb3 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For Environment, select\'a0
\f0\b Python 3 (with Intel\'ae MKL)
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b CREATE
\f1\b0 \'a0at the bottom of the dialog vox.\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb7 \strokec6 Note:\'a0
\f1\b0 \AppleTypeServices The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.\
\pard\pardeftab720\partightenfactor0

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \
\pard\pardeftab720\partightenfactor0

\f4\b \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 Note:\'a0
\f1\b0 \AppleTypeServices Click
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f4\b \AppleTypeServices\AppleTypeServicesF65539 Enable Notebook API
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f1 \AppleTypeServices to enable the notebook api.\
\pard\pardeftab720\partightenfactor0

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once the environment is ready, click the\'a0
\f0\b OPEN JUPYTERLAB
\f1\b0 \'a0link next to your Notebook name. This will open up your environment in a new tab in your browser.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, click\'a0
\f0\b Terminal
\f1\b0 . This will open up a terminal where you can run all the commands in this lab.\cb1 \
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 Download Code Repository\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Next you will download a code repository for use in this lab.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal you just opened, enter the following:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 git clone https://github.com/GoogleCloudPlatform/training-data-analyst\
cd /home/jupyter/training-data-analyst/quests/dataflow_python/\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls6\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the left panel of your notebook environment, in the file browser, you will notice the\'a0
\f0\b training-data-analyst
\f1\b0 \'a0repo added.\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate into the cloned repo\'a0
\f6\fs30 \cb9 /training-data-analyst/quests/dataflow_python/
\f1\fs32 \cb3 . You will see a folder for each lab, which is further divided into a\'a0
\f6\fs30 \cb9 lab
\f1\fs32 \cb3 \'a0sub-folder with code to be completed by you, and a\'a0
\f6\fs30 \cb9 solution
\f1\fs32 \cb3 \'a0sub-folder with a fully workable example to reference if you get stuck.\cb1 \
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Lab part 1: Dealing with late data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In the previous labs, you wrote code that divided elements by event time into windows of fixed width, using code that looked like the following:\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 parsed_msgs\
        | "WindowByMinute" >> beam.WindowInto(beam.window.FixedWindows(window_duration))\
        | "CountPerMinute" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\cf0 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf6 \cb1 \strokec6 Copied!\
\pard\pardeftab720\qc\partightenfactor0

\fs48 \cf6 \
\pard\pardeftab720\partightenfactor0
\cf6 content_copy
\fs32 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 However, as you saw at the end of the last non-SQL lab, streams of data often have lag. Lag is problematic when windowing using event time (as opposed to processing time) because it introduces uncertainty: have all of the events for a particular point in event time actually arrived, or haven\'92t they?\
Clearly, in order to output results, the pipeline you wrote needed to make a decision in this respect. It did so using a concept called a watermark. A watermark is the system\'92s heuristic-based notion of when all data up to a certain point in event time can be expected to have arrived in the pipeline. Once the watermark progresses past the end of a window, any further element that arrives with a timestamp in that window is considered late data and is simply dropped. So, the default windowing behavior is to emit a single, hopefully complete result when the system is confident that it has all of the data.\
Apache Beam uses a number of heuristics to make an educated guess about what the watermark is. However, these are still heuristics. More to the point, those heuristics are general-purpose and are not suitable for all use cases. Instead of using general-purpose heuristics, pipeline designers need to thoughtfully consider the following questions in order to determine what tradeoffs are appropriate:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Completeness: How important is it to have all of your data before you compute your result?\cb1 \
\ls7\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Latency: How long do you want to wait for data? For example, do you wait until you think you have all data, or do you process data as it arrives?\cb1 \
\ls7\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Cost: How much compute power and money are you willing to spend to lower the latency?\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Armed with those answers, it\'92s possible to use Apache Beam\'92s formalisms to write code that makes the right tradeoff.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Allowed lateness\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Allowed lateness controls how long a window should retain its state; once the watermark reaches the end of the allowed lateness period, all state is dropped. While it would be great to be able to keep all of our persistent state around until the end of time, in reality, when dealing with an unbounded data source, it\'92s often not practical to keep a given window's state indefinitely; we\'92ll eventually run out of disk space.\
As a result, any real-world, out-of-order processing system needs to provide some way to bound the lifetimes of the windows it\'92s processing. A clean and concise way of doing this is by defining a horizon on the allowed lateness within the system, i.e. placing a bound on how late any given record may be (relative to the watermark) for the system to bother processing it; any data that arrives after this horizon is simply dropped. Once you\'92ve bounded how late individual data may be, you\'92ve also established precisely how long the state for windows must be kept around: until the watermark exceeds the lateness horizon for the end of the window.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 1. Prepare the environment\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 As in the prior labs, the first step is to generate data for the pipeline to process. You will open the lab environment and generate the data as before:\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Open the appropriate lab\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal in your IDE, run the following commands to change to the directory you will use for this lab:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 # Change directory into the lab\
cd 7_Advanced_Streaming_Analytics/lab\
export BASE_DIR=$(pwd)\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Setting up virtual environment and dependencies\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Before you can begin editing the actual pipeline code, you need to ensure you have installed the necessary dependencies.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Execute the following to create a virtual environment for your work in this lab:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 sudo apt-get update && sudo apt-get install -y python3-venv\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \

\f6\fs28 \cb3 \strokec4 ## Create and activate virtual environment\
 python3 -m venv df-env\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \

\f6\fs28 \cb3 \strokec4  source df-env/bin/activate\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, install the packages you will need to execute your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 python3 -m pip install -q --upgrade pip setuptools wheel\
python3 -m pip install apache-beam[gcp]\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Ensure that the Dataflow API is enabled:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 gcloud services enable dataflow.googleapis.com\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Set up the data environment\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 # Create GCS buckets and BQ dataset\
cd $BASE_DIR/../../\
source create_streaming_sinks.sh\
\
# Change to the directory containing the practice version of the code\
cd $BASE_DIR\cf0 \
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 2. Set allowed lateness\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the file explorer, navigate to\'a0
\f6\fs30 \cb9 training-data-analyst/quest/dataflow_python/7_Advanced_Streaming_Analytics/lab
\f1\fs32 \cb3 \'a0and open the\'a0
\f6\fs30 \cb9 streaming_minute_traffic_pipeline.py
\f1\fs32 \cb3 \'a0file.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 In Apache Beam, allowed lateness is set using the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 allowed_lateness
\f1\fs32 \cf2 \cb3 \strokec2 \'a0keyword argument with the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 AfterWatermark()
\f1\fs32 \cf2 \cb3 \strokec2 \'a0trigger within the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 WindowInto
\f1\fs32 \cf2 \cb3 \strokec2 \'a0PTransform, as in the example below:
\f6\fs28 \cf6 \cb3 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 items = p | ...\
\
Windowed_items = items | beam.WindowInto(beam.window.FixedWindows(\cf10 \cb3 \strokec10 60\cf6 \cb3 \strokec4 ), \cf11 \cb3 \strokec11 # 1 minute\cf6 \cb3 \strokec4 \
                                         trigger=AfterWatermark(),\
                                         allowed_lateness=\cf10 \cb3 \strokec10 60\cf6 \cb3 \strokec4 *\cf10 \cb3 \strokec10 60\cf6 \cb3 \strokec4 *\cf10 \cb3 \strokec10 24\cf6 \cb3 \strokec4 ) \cf11 \cb3 \strokec11 # 1 day\cf6 \cb3 \strokec4 \
\

\f1\fs32 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, examine the windowing transform and allowed lateness defined by the command-line argument\'a0
\f6\fs30 \cb9 allowed_lateness
\f1\fs32 \cb3 . Use your judgment as to what a reasonable value should be and update the command line to reflect the right units.\cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Triggers\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Pipeline designers also have discretion over when to emit preliminary results. In the previous step we used the AfterWatermark() trigger with a specified allowed lateness. For example, say that the watermark for the end of a window has not yet been reached, but 75% of the expected data has already arrived. In many cases, such a sample can be assumed to be representative, making it worth showing to end users.\
\pard\pardeftab720\sa480\partightenfactor0

\f6\fs30 \cf2 \cb9 \strokec2 Trigger
\f1\fs32 \cf2 \cb3 \strokec2 s determine at what point during processing time results will be materialized. Each specific output for a window is referred to as a pane of the window. Triggers fire panes when the trigger\'92s conditions are met. In Apache Beam, those conditions include watermark progress, processing time progress (which will progress uniformly, regardless of how much data has actually arrived), element counts (such as when a certain amount of new data arrives), and data-dependent triggers, like when the end of a file is reached.\
A trigger\'92s conditions may lead it to fire a pane many times. Consequently, it\'92s also necessary to specify how to accumulate these results. Apache Beam currently supports two accumulation modes, one which accumulates results together and the other which returns only the portions of the result that are new since the last pane fired.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 3. Set a trigger\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 When you set a windowing function for a\'a0
\f6\fs30 \cf2 \cb9 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0by using the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 Window
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform, you can also specify a trigger.\
You set the trigger(s) for a PCollection by setting the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 trigger
\f1\fs32 \cf2 \cb3 \strokec2 \'a0keyword argument of your\'a0
\f6\fs30 \cf2 \cb9 \strokec2 WindowInto
\f1\fs32 \cf2 \cb3 \strokec2 \'a0PTransform. Apache Beam comes with a number of provided triggers:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls14\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.trigger.html#apache_beam.transforms.trigger.AfterWatermark"}}{\fldrslt \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 AfterWatermark}}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \'a0for firing when the watermark passes a timestamp determined from either the end of the window or the arrival of the first element in a pane.\cb1 \
\ls14\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.transforms.trigger.html#apache_beam.transforms.trigger.AfterCount"}}{\fldrslt \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 AfterProcessingTime}}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \'a0for firing after some amount of processing time has elapsed (typically since the first element in a pane).\cb1 \
\ls14\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.trigger.html#apache_beam.transforms.trigger.AfterCount"}}{\fldrslt \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 AfterCount}}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \'a0for firing when the number of elements in the window reaches a certain count.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 This code sample sets a time-based trigger for a PCollection that emits results one minute after the first element in that window has been processed. In the last line of the code sample, we set the window\'92s accumulation mode by defining the keyword argument\'a0
\f6\fs30 \cf2 \cb9 \strokec2 accumulation_mode
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to\'a0
\f6\fs30 \cf2 \cb9 \strokec2 AccumulationMode.DISCARDING
\f1\fs32 \cf2 \cb3 \strokec2 :\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 items = p | ...\
windowed_items = items | beam.WindowInto(FixedWindows(\cf10 \cb3 \strokec10 60\cf6 \cb3 \strokec4 ), \cf11 \cb3 \strokec11 # 1 minute\cf6 \cb3 \strokec4 \
                                         trigger=AfterProcessingTime(\cf10 \cb3 \strokec10 60\cf6 \cb3 \strokec4 ),\
                                         accumulation_mode=AccumulationMode.DISCARDING)\

\f1\fs32 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls15\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, add the keyword argument\'a0
\f6\fs30 \cb9 trigger
\f1\fs32 \cb3 \'a0to\'a0
\f6\fs30 \cb9 WindowInto
\f1\fs32 \cb3 \'a0passing in the\'a0
\f6\fs30 \cb9 AfterWatermark
\f1\fs32 \cb3 \'a0trigger. When designing your trigger, keep in mind this use case, in which data is windowed into one-minute windows and data can arrive late. Also, as an argument to the AfterWatermark trigger, add a late trigger for every late element (within the allowed lateness). If you are stuck, take a look at the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform//training-data-analyst/tree/master/quests/dataflow_python/7_Advanced_Streaming_Analytics/solution"}}{\fldrslt solution}}.\cb1 \
\ls15\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fill out the following\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 \'a0near line 113 to set trigger and accumulation mode:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 trigger=AfterProcessingTime(120),\
accumulation_mode=AccumulationMode.DISCARDING)\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fill out the following\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 \'a0near line 119 to set allowed lateness, trigger, and accumulation mode:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 trigger=AfterWatermark(late=AfterCount(1)),\
allowed_lateness=int(allowed_lateness),\
accumulation_mode=AccumulationMode.ACCUMULATING)\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Lab part 2: Dealing with malformed data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 Depending on how you set up your\'a0
\f6\fs30 \cf2 \cb9 \strokec2 Trigger
\f1\fs32 \cf2 \cb3 \strokec2 , if you were to run the pipeline right now and compare it to the pipeline from the previous lab, you might notice that the new pipeline presents results earlier. It\'92s also possible that its results might be more accurate, if the heuristics did a poor job of predicting streaming behavior and the allowed lateness is better.\
However, while the current pipeline is more robust to lateness, it is still vulnerable to malformed data. If you were to run the pipeline and publish a message containing anything but a well-formed JSON string that could be parsed into a\'a0
\f6\fs30 \cf2 \cb9 \strokec2 CommonLog
\f1\fs32 \cf2 \cb3 \strokec2 , the pipeline would generate an error. Although tools like Cloud Logging make it straightforward to read those errors, a better-designed pipeline will store these in a pre-defined location for later inspection.\
In this section, you add components to the pipeline that make it both more modular as well as more robust.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 1. Collect malformed data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In order to be more robust to malformed data, the pipeline needs a way of filtering out this data and branching to process it differently. You have already seen one way to introduce a branch into a pipeline: by making one\'a0
\f6\fs30 \cf2 \cb9 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0the input the multiple transforms.\
This form of branching is powerful. However, there are some use cases where this strategy is inefficient. For example, say you want to create two different subsets of the same\'a0
\f6\fs30 \cf2 \cb9 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 . Using the multiple transform method, you would create one filter transform for each subset and apply them both to the original\'a0
\f6\fs30 \cf2 \cb9 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 . However, this would process each element twice.\
An alternative method for producing a branching pipeline is to have a single transform produce multiple outputs while processing the input\'a0
\f6\fs30 \cf2 \cb9 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0one time. In this task, you write a transform that produces multiple outputs, the first of which are the results obtained from well-formed data and second of which are the malformed elements from the original input stream.\
In order to emit multiple results while still creating only a single\'a0
\f6\fs30 \cf2 \cb9 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 , Apache Beam uses a class called\'a0
\f6\fs30 \cf2 \cb9 \strokec2 TaggedOutput
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to key the outputs of the DoFn with multiple (possibly heterogeneous) outputs.\
Here is an example of\'a0
\f6\fs30 \cf2 \cb9 \strokec2 TaggedOutput
\f1\fs32 \cf2 \cb3 \strokec2 \'a0being used to tag different outputs from a DoFn. Those PCollections are then recovered using the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 with_outputs()
\f1\fs32 \cf2 \cb3 \strokec2 \'a0method and referenced with the tag name specified in the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 TaggedOutput
\f1\fs32 \cf2 \cb3 \strokec2 .\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 class ConvertToCommonLogFn(beam.DoFn):\
  def process(self, element):\
    try:\
        row = json.loads(element.decode('utf-8'))\
        yield beam.pvalue.TaggedOutput('parsed_row', CommonLog(**row))\
    except:\
        yield beam.pvalue.TaggedOutput('unparsed_row', element.decode('utf-8'))\
\
\'85\
\
rows = (p | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(input_topic)\
              | 'ParseJson' >> beam.ParDo(ConvertToCommonLogFn()).with_outputs('parsed_row', 'unparsed_row')\
                                                                 .with_output_types(CommonLog))\
\
(rows.unparsed_row | \'85\
\
(rows.parsed_row | \'85\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 To complete this task, declare two TaggedOutput returns in the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 ConvertToCommonLogFn
\f1\fs32 \cf2 \cb3 \strokec2 \'a0class as above. In the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 try
\f1\fs32 \cf2 \cb3 \strokec2 \'a0statement, return the parsed row as an instance of the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 CommonLog
\f1\fs32 \cf2 \cb3 \strokec2 \'a0class, and in the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 catch
\f1\fs32 \cf2 \cb3 \strokec2 \'a0statement return the unparsed (decoded) row.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fill out the first\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 \'a0in the\'a0
\f6\fs30 \cb9 ConvertToCommonLogFn
\f1\fs32 \cb3 \'a0class:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 beam.pvalue.TaggedOutput('parsed_row', CommonLog(**row))\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls18\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fill out the second\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 \'a0in the\'a0
\f6\fs30 \cb9 ConvertToCommonLogFn
\f1\fs32 \cb3 \'a0class:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 beam.pvalue.TaggedOutput('unparsed_row', element.decode('utf-8'))\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 2. Write malformed data for later analysis\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In order to fix the upstream problem that is producing malformed data, it is important to be able to analyze the malformed data. Doing so requires materializing it somewhere. In this task, you write malformed data to Google Cloud Storage. This pattern is called using dead-letter storage.\
In previous labs, you wrote directly from a bounded source (batch) to Cloud Storage using\'a0
\f6\fs30 \cf2 \cb9 \strokec2 beam.io.WriteToText()
\f1\fs32 \cf2 \cb3 \strokec2 . However, when writing from an unbounded source (streaming), this approach needs to be modified slightly.\
Firstly, upstream of the write transform, you need to use a\'a0
\f6\fs30 \cf2 \cb9 \strokec2 Trigger
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to specify when, in processing time, to write. Otherwise, if the defaults are left, the write will never take place. By default, every event belongs to the Global Window. When operating in batch, this is fine because the full data set is known at run time. However, with unbounded sources, the full dataset size is unknown and so Global Window panes never fire, as they are never complete.\
Because you are using a\'a0
\f6\fs30 \cf2 \cb9 \strokec2 Trigger
\f1\fs32 \cf2 \cb3 \strokec2 , you also need to use a\'a0
\f6\fs30 \cf2 \cb9 \strokec2 Window
\f1\fs32 \cf2 \cb3 \strokec2 . However, you don\'92t necessarily need to change the window. In previous labs and tasks, you have used windowing transforms to replace the global window with a window of fixed duration in event time. In this case, which elements are grouped together is not as important as that the results be materialized in a useful manner and at a useful rate.\
In the example below, the window fires the Global Window pane after every 10 seconds of processing time but only writes new events:\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 pcollection | \'93FireEvery10s\'94 >> WindowInto(FixedWindows(\cf10 \cb3 \strokec10 10\cf6 \cb3 \strokec4 )\
                                           trigger=AfterProcessingTime(\cf10 \cb3 \strokec10 10\cf6 \cb3 \strokec4 ))\
                                           accumulation_mode=AccumulationMode.DISCARDING\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Once you\'92ve set a Trigger, we will need to change the sink from\'a0
\f6\fs30 \cf2 \cb9 \strokec2 beam.io.WriteToText()
\f1\fs32 \cf2 \cb3 \strokec2 \'a0(which does not support streaming) to\'a0
\f6\fs30 \cf2 \cb9 \strokec2 beam.io.fileio.WriteToFiles()
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to perform the writes. When writing downstream of a windowing transform, we specify a number of shards so that writing can be done in parallel:
\f6\fs28 \cf6 \cb3 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 windowed_items = p | 'WriteWindowedPCollection' >> fileio.WriteToFiles("gs://path/to/somewhere",\
shards=int(num_shards),\
max_writers_per_bundle=0)\

\f1\fs32 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls19\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, create a new transform using\'a0
\f6\fs30 \cb9 rows.unparsed_row
\f1\fs32 \cb3 \'a0as the input to retrieve the malformed data. Use a processing time trigger of 120 seconds for your fixed window of length 120 seconds with accumulation mode set to\'a0
\f6\fs30 \cb9 AccumulationMode.DISCARDING
\f1\fs32 \cb3 .\cb1 \
\ls19\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fill out the\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 \'a0to use\'a0
\f6\fs30 \cb9 beam.fileio.WriteToFiles
\f1\fs32 \cb3 \'a0to write out to GCS:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 fileio.WriteToFiles(output_path,shards=1,max_writers_per_bundle=0)\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 3. Run your pipeline\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 To run your pipeline, construct a command resembling the example below. Note that it will need to be modified to reflect the names of any command-line options that you have included:\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  PROJECT_ID=$(gcloud\cf12 \cb3 \strokec12  config \cf6 \cb3 \strokec4 get-value project)\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  REGION=Region\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  BUCKET=gs://$\{PROJECT_ID\}\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  PIPELINE_FOLDER=\cf13 \cb3 \strokec13 $\{BUCKET\}\cf6 \cb3 \strokec4 \
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  RUNNER=DataflowRunner\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  PUBSUB_TOPIC=projects/$\{PROJECT_ID\}/topics/my_topic\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  WINDOW_DURATION=60\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  ALLOWED_LATENESS=1\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  OUTPUT_TABLE_NAME=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf6 \cb3 \strokec4 :logs.minute_traffic\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  DEADLETTER_BUCKET=\cf13 \cb3 \strokec13 $\{BUCKET\}\cf6 \cb3 \strokec4 \
\
cd \cf13 \cb3 \strokec13 $BASE_DIR\cf6 \cb3 \strokec4 \
\
\
python3 streaming_minute_traffic_pipeline.py \\\
--project=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf6 \cb3 \strokec4  \\\
--region=\cf13 \cb3 \strokec13 $\{REGION\}\cf6 \cb3 \strokec4  \\\
--staging_location=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf6 \cb3 \strokec4 /staging \\\
--temp_location=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf6 \cb3 \strokec4 /temp \\\
--runner=\cf13 \cb3 \strokec13 $\{RUNNER\}\cf6 \cb3 \strokec4  \\\
--input_topic=\cf13 \cb3 \strokec13 $\{PUBSUB_TOPIC\}\cf6 \cb3 \strokec4  \\\
--window_duration=\cf13 \cb3 \strokec13 $\{WINDOW_DURATION\}\cf6 \cb3 \strokec4  \\\
--allowed_lateness=\cf13 \cb3 \strokec13 $\{ALLOWED_LATENESS\}\cf6 \cb3 \strokec4  \\\
--table_name=\cf13 \cb3 \strokec13 $\{OUTPUT_TABLE_NAME\}\cf6 \cb3 \strokec4  \\\
--dead_letter_bucket=\cf13 \cb3 \strokec13 $\{DEADLETTER_BUCKET\}\cf6 \cb3 \strokec4  \\\
--allow_unsafe_triggers\
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 The code for this quest includes a script for publishing JSON events using Pub/Sub. To complete this task and start publishing messages, open a\'a0
\f0\b \cf2 \cb3 \strokec2 new terminal
\f1\b0 \cf2 \cb3 \strokec2 \'a0side-by-side with your current one and run the following script. It will keep publishing messages until you kill the script. Make sure you are in the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 training-data-analyst/quests/dataflow_python
\f1\fs32 \cf2 \cb3 \strokec2 \'a0folder.\
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 Note:\'a0
\f1\b0 \AppleTypeServices The
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf6 \cb9 \strokec6 true
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f1 \AppleTypeServices flag adds late events to the stream.
\f5 \AppleTypeServices\AppleTypeServicesF65539 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \AppleTypeServices \cf6 \cb3 \strokec4 cd /home/jupyter/training-data-analyst/quests/dataflow_python/\
bash generate_streaming_events.sh true\cf0 \
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 4. Test your pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls20\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate to\'a0
\f0\b Pub/Sub
\f1\b0 \'a0>\'a0
\f0\b Topics
\f1\b0 \'a0and click on the topic\'a0
\f6\fs30 \cb9 my_topic
\f1\fs32 \cb3 .\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Then click on\'a0
\f0\b Message
\f1\b0 \'a0and\'a0
\f7\i Select a Cloud Pub/Sub Subscription to pull messages from
\f1\i0 \'a0and select the my topic subscription from the dropdown.\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click the\'a0
\f0\b Publish Message
\f1\b0 \'a0button.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls20\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the following page, enter a message to be published.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 So long as it doesn\'92t conform perfectly to the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 CommonLog
\f1\fs32 \cf2 \cb3 \strokec2 \'a0JSON spec, it should arrive in the dead-letter Cloud Storage bucket shortly. You can trace its path through the pipeline by returning to the pipeline monitoring window and clicking on a node in the branch responsible for handling unparsed messages. Once you see an element added to this branch, you can then navigate to Cloud Storage and verify that the message has been written to disk:\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  PROJECT_ID=$(gcloud\cf12 \cb3 \strokec12  config \cf6 \cb3 \strokec4 get-value project)\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  REGION=Region\
\cf12 \cb3 \strokec12 export\cf6 \cb3 \strokec4  BUCKET=gs://$\{PROJECT_ID\}/deadletter\
\
gsutil ls \cf13 \cb3 \strokec13 $BUCKET\cf6 \cb3 \strokec4 \
gsutil cat \cf13 \cb3 \strokec13 $BUCKET\cf6 \cb3 \strokec4 /*\cf0 \
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 \
}