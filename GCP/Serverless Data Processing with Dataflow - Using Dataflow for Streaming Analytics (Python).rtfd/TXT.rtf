{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 Helvetica;
\f3\fmodern\fcharset0 Courier-Bold;\f4\fswiss\fcharset0 Arial-BoldMT;\f5\fswiss\fcharset0 ArialMT;
\f6\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red24\green25\blue27;\red255\green255\blue255;\red0\green0\blue0;
\red254\green246\blue217;\red24\green24\blue24;\red251\green226\blue224;\red227\green236\blue254;\red0\green0\blue0;
\red0\green0\blue117;\red82\green0\blue83;\red115\green0\blue2;}
{\*\expandedcolortbl;;\cssrgb\c12549\c12941\c14118;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;
\cssrgb\c99608\c96863\c87843;\cssrgb\c12157\c12157\c12157;\cssrgb\c98824\c90980\c90196;\cssrgb\c90980\c94118\c99608;\cssrgb\c0\c0\c0\c5098;
\cssrgb\c0\c0\c53333;\cssrgb\c40000\c0\c40000;\cssrgb\c53333\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat7\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat8\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat4\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat4\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid25}
{\list\listtemplateid26\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid26}
{\list\listtemplateid27\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat5\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid27}
{\list\listtemplateid28\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat6\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid28}
{\list\listtemplateid29\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat7\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid29}
{\list\listtemplateid30\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid30}
{\list\listtemplateid31\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid3001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid31}
{\list\listtemplateid32\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid32}
{\list\listtemplateid33\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid33}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}{\listoverride\listid26\listoverridecount0\ls26}{\listoverride\listid27\listoverridecount0\ls27}{\listoverride\listid28\listoverridecount0\ls28}{\listoverride\listid29\listoverridecount0\ls29}{\listoverride\listid30\listoverridecount0\ls30}{\listoverride\listid31\listoverridecount0\ls31}{\listoverride\listid32\listoverridecount0\ls32}{\listoverride\listid33\listoverridecount0\ls33}}
\paperw11900\paperh16840\margl1440\margr1440\vieww33100\viewh16740\viewkind0
\deftab720
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Overview\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this lab, you take many of the concepts introduced in a batch context and apply them in a streaming context to create a pipeline similar to batch_minute_traffic_pipeline, but which operates in real time. The finished pipeline will first read JSON messages from Pub/Sub and parse those messages before branching. One branch writes some raw data to BigQuery and takes note of event and processing time. The other branch windows and aggregates the data and then writes the results to BigQuery.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 Objectives\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\fs32 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Read data from a streaming source.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Write data to a streaming sink.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Window data in a streaming context.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Experimentally verify the effects of lag.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 You will build the following pipeline:\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \cb1 \strokec4 {{\NeXTGraphic FAbWaMRRLeEg3NbMhByJiMJJui0xfBHj4G9dIC4tFTQ=.png \width13060 \height14960 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Jupyter notebook-based development environment setup\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 For this lab, you will be running all commands in a terminal from your notebook.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud Console, on the\'a0
\f0\b Navigation Menu
\f1\b0 , click\'a0
\f0\b Vertex AI > Workbench
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Enable\'a0
\f0\b Notebooks API
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the Workbench page, click\'a0
\f0\b CREATE NEW
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the\'a0
\f0\b New instance
\f1\b0 \'a0dialog box that appears, set the region to\'a0
\f3\b \cb5 region
\f1\b0 \cb3 \'a0and zone to\'a0
\f3\b \cb5 zone
\f1\b0 \cb3 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For Environment, select\'a0
\f0\b Apache Beam
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b CREATE
\f1\b0 \'a0at the bottom of the dialog vox.\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb7 \strokec6 Note:\'a0
\f1\b0 \AppleTypeServices The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.\
\
\pard\pardeftab720\partightenfactor0

\f4\b \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 Note:\'a0
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 Click\'a0
\f4\b \AppleTypeServices\AppleTypeServicesF65539 Enable Notebook API
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 \'a0to enable the notebook api.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once the environment is ready, click the\'a0
\f0\b OPEN JUPYTERLAB
\f1\b0 \'a0link next to your Notebook name. This will open up your environment in a new tab in your browser.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \strokec4 {{\NeXTGraphic Qw=.png \width18920 \height3380 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, click\'a0
\f0\b Terminal
\f1\b0 . This will open up a terminal where you can run all the commands in this lab.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Download Code Repository\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 Next you will download a code repository for use in this lab.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal you just opened, enter the following:\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 git clone https://github.com/GoogleCloudPlatform/training-data-analyst\
cd /home/jupyter/training-data-analyst/quests/dataflow_python/\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls6\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the left panel of your notebook environment, in the file browser, you will notice the\'a0
\f0\b training-data-analyst
\f1\b0 \'a0repo added.\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate into the cloned repo\'a0
\f6\fs30 \cb9 /training-data-analyst/quests/dataflow_python/
\f1\fs32 \cb3 . You will see a folder for each lab, which is further divided into a\'a0
\f6\fs30 \cb9 lab
\f1\fs32 \cb3 \'a0sub-folder with code to be completed by you, and a\'a0
\f6\fs30 \cb9 solution
\f1\fs32 \cb3 \'a0sub-folder with a fully workable example to reference if you get stuck.\cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Open the appropriate lab\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In your terminal, run the following commands to change to the directory you will use for this lab:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 # Change directory into the lab\
cd 5_Streaming_Analytics/lab\
export BASE_DIR=$(pwd)\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Setting up dependencies\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Execute the following to install the packages you will need to execute your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 python3 -m pip install -q --upgrade pip setuptools wheel\
python3 -m pip install apache-beam[gcp]\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Ensure that the Dataflow API is enabled:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 gcloud services enable dataflow.googleapis.com\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Finally, grant the\'a0
\f6\fs30 \cb9 dataflow.worker
\f1\fs32 \cb3 \'a0role to the Compute Engine default service account:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 PROJECT_ID=$(gcloud config get-value project)\
export PROJECT_NUMBER=$(gcloud projects list --filter="$PROJECT_ID" --format="value(PROJECT_NUMBER)")\
export serviceAccount=""$PROJECT_NUMBER"-compute@developer.gserviceaccount.com"\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls11\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Cloud Console, navigate to\'a0
\f0\b IAM & ADMIN > IAM
\f1\b0 , click on\'a0
\f0\b Edit principal
\f1\b0 \'a0icon for\'a0
\f6\fs30 \cb9 Compute Engine default service account
\f1\fs32 \cb3 .\cb1 \
\ls11\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add\'a0
\f0\b Dataflow Worker
\f1\b0 \'a0as another role and click\'a0
\f0\b Save
\f1\b0 .\cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Set up the data environment\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 # Create GCS buckets and BQ dataset\
cd $BASE_DIR/../..\
source create_streaming_sinks.sh\
\
# Change to the directory containing the practice version of the code\
cd $BASE_DIR\cf0 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf2 \cb1 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 1. Reading from a streaming source\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In the previous labs, you used\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.io.textio.html#apache_beam.io.textio.ReadFromText"}}{\fldrslt 
\f6\fs30 \cf2 \cb9 \strokec2 beam.io.ReadFromText}}\'a0to read from Google Cloud Storage. In this lab, instead of Google Cloud Storage, you use Pub/Sub. Pub/Sub is a fully managed real-time messaging service that allows publishers to send messages to a "topic," to which subscribers can subscribe via a "subscription."\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \cb1 \strokec4 {{\NeXTGraphic D9ZGISTBs2NprZLGjjIwqU=.png \width14720 \height8580 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\partightenfactor0
\cf0 \strokec4 \
\
\pard\pardeftab720\sa480\partightenfactor0

\fs32 \cf2 \cb3 \strokec2 The pipeline you create subscribes to a topic called\'a0
\f6\fs30 \cf2 \cb9 \strokec2 my_topic
\f1\fs32 \cf2 \cb3 \strokec2 \'a0that you just created via\'a0
\f6\fs30 \cf2 \cb9 \strokec2 create_streaming_sinks.sh
\f1\fs32 \cf2 \cb3 \strokec2 \'a0script. In a production situation, this topic will often be created by the publishing team. You can view it in the\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/cloudpubsub/topic/list"}}{\fldrslt \cf2 \cb3 \strokec2 Pub/Sub portion of the console}}.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the file explorer, navigate to\'a0
\f6\fs30 \cb9 training-data-analyst/quest/dataflow_python/5_Streaming_Analytics/lab/
\f1\fs32 \cb3 \'a0and open the\'a0
\f6\fs30 \cb9 streaming_minute_traffic_pipeline.py
\f1\fs32 \cb3 \'a0file.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To read from Pub/Sub using Apache Beam\'92s IO connectors, add a transform to the pipeline which uses the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.io.gcp.pubsub.html?highlight=pubsub#apache_beam.io.gcp.pubsub.ReadFromPubSub"}}{\fldrslt 
\f6\fs30 \cb9 beam.io.ReadFromPubSub()}}\'a0class. This class has attributes for specifying the source topic as well as the\'a0
\f6\fs30 \cb9 timestamp_attribute
\f1\fs32 \cb3 . By default, this attribute is set to the message publishing time.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 Note:\'a0
\f1\b0 \AppleTypeServices \cf6 \cb8 \strokec6 Publication time is the time when the Pub/Sub service first receives the message. In systems where there may be a delay between the actual event time and publish time (i.e., late data) and you would like to take this into account, the client code publishing the message needs to set a 'timestamp' metadata attribute on the message and provide the actual event timestamp, since Pub/Sub will not natively know how to extract the event timestamp embedded in the payload. You can see the
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/efc7ed26b88d54bc1d8c0c0376ed01558d1f3b59/quests/dataflow/streaming_event_generator.py#L112"}}{\fldrslt \cf6 \cb8 \strokec6 client code generating the messages you'll use here}}
\f1 \AppleTypeServices \cf6 \cb8 \strokec6 .\

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \AppleTypeServices \cf2 \cb3 \strokec2 To complete this task:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls14\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add a transform that reads from the Pub/Sub topic specified by the\'a0
\f6\fs30 \cb9 input_topic
\f1\fs32 \cb3 \'a0command-line parameter.\cb1 \
\ls14\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Then, use the provided function,\'a0
\f6\fs30 \cb9 parse_json
\f1\fs32 \cb3 \'a0with\'a0
\f6\fs30 \cb9 beam.Map
\f1\fs32 \cb3 \'a0to convert each JSON string into a\'a0
\f6\fs30 \cb9 CommonLog
\f1\fs32 \cb3 \'a0instance.\cb1 \
\ls14\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Collect the results from this transform into a\'a0
\f6\fs30 \cb9 PCollection
\f1\fs32 \cb3 \'a0of\'a0
\f6\fs30 \cb9 CommonLog
\f1\fs32 \cb3 \'a0instances using\'a0
\f6\fs30 \cb9 with_output_types()
\f1\fs32 \cb3 .\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls15\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the first\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 , add the following code:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 beam.io.ReadFromPubSub(input_topic)\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 2. Window the data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_minute_traffic_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 previous non-SQL lab}}, you implemented fixed-time windowing in order to group events by event time into mutually-exclusive windows of fixed size. Do the same thing here with the streaming inputs. Feel free to reference the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_minute_traffic_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 previous lab's code}}\'a0or the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/5_Streaming_Analytics/solution/streaming_minute_traffic_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 solution}}\'a0if you get stuck.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 Window into one-minute windows\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 To complete this task:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls16\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add a transform to your pipeline that accepts the\'a0
\f6\fs30 \cb9 PCollection
\f1\fs32 \cb3 \'a0of\'a0
\f6\fs30 \cb9 CommonLog
\f1\fs32 \cb3 \'a0data and windows elements into windows of\'a0
\f6\fs30 \cb9 window_duration
\f1\fs32 \cb3 \'a0seconds long, with\'a0
\f6\fs30 \cb9 window_duration
\f1\fs32 \cb3 \'a0as another command-line parameter.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Use the following code to add a transform to your pipeline that windows elements into one-minute windows:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 "WindowByMinute" >> beam.WindowInto(beam.window.FixedWindows(60))\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \

\f6\fs28 \cf0 \cb3 \strokec4 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 3. Aggregate the data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In the previous lab, you used the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Count"}}{\fldrslt 
\f6\fs30 \cf2 \cb9 \strokec2 CountCombineFn()}}\'a0combiner to count the number of events per window. Do the same here.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 Count events per window\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 To complete this task:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls17\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Pass the windowed\'a0
\f6\fs30 \cb9 PCollection
\f1\fs32 \cb3 \'a0as input to a transform that counts the number of events per window.\cb1 \
\ls17\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 After this, use the provided\'a0
\f6\fs30 \cb9 DoFn
\f1\fs32 \cb3 ,\'a0
\f6\fs30 \cb9 GetTimestampFn
\f1\fs32 \cb3 , with\'a0
\f6\fs30 \cb9 beam.ParDo
\f1\fs32 \cb3 \'a0to include the window start timestamp.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Use the following code to add a transform to your pipeline that counts the number of events per window:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 "CountPerMinute" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 4. Write to BigQuery\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 This pipeline writes to BigQuery in two separate branches. The first branch writes the aggregated data to BigQuery. The second branch, which has already been authored for you, writes out some metadata regarding each raw event, including the event timestamp and the actual processing timestamp. Both write directly to BigQuery via streaming inserts.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 Write aggregated data to BigQuery\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 Writing to BigQuery has been covered extensively in previous labs, so the basic mechanics will not be covered in depth here.\
To complete this task:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls18\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Create a new command-line parameter called\'a0
\f6\fs30 \cb9 agg_table_name
\f1\fs32 \cb3 \'a0for the table intended to house aggregated data.\cb1 \
\ls18\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add a transfrom as before that writes to BigQuery.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 Note:\'a0
\f1\b0 \AppleTypeServices \cf6 \cb8 \strokec6 When in a streaming context,
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f6\fs30 \AppleTypeServices \cf6 \cb9 \strokec6 beam.io.WriteToBigQuery()
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f1 \AppleTypeServices \cf6 \cb8 \strokec6 does not support
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f6\fs30 \AppleTypeServices \cf6 \cb9 \strokec6 write_disposition
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f1 \AppleTypeServices \cf6 \cb8 \strokec6 of
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f6\fs30 \AppleTypeServices \cf6 \cb9 \strokec6 WRITE_TRUNCATE
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f1 \AppleTypeServices \cf6 \cb8 \strokec6 in which the table is dropped and recreated. In this example, use
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \'a0
\f6\fs30 \AppleTypeServices \cf6 \cb9 \strokec6 WRITE_APPEND
\f1\fs24 \cf6 \cb8 \strokec6 .\

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb8 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \AppleTypeServices \cf2 \cb3 \strokec2 BigQuery insertion method\
\pard\pardeftab720\sa480\partightenfactor0

\f6\fs30 \cf2 \cb9 \strokec2 beam.io.WriteToBigQuery
\f1\fs32 \cf2 \cb3 \strokec2 \'a0will default to either\'a0{\field{\*\fldinst{HYPERLINK "https://cloud.google.com/bigquery/streaming-data-into-bigquery"}}{\fldrslt \cf2 \cb3 \strokec2 streaming inserts}}\'a0for unbounded PCollections or\'a0{\field{\*\fldinst{HYPERLINK "https://cloud.google.com/bigquery/docs/loading-data-cloud-storage"}}{\fldrslt \cf2 \cb3 \strokec2 batch file load jobs}}\'a0for bounded PCollections. Streaming inserts can be particularly useful when you want data to show up in aggregations immediately, but does incur\'a0{\field{\*\fldinst{HYPERLINK "https://cloud.google.com/bigquery/pricing#streaming_pricing"}}{\fldrslt \cf2 \cb3 \strokec2 extra charges}}. In streaming use cases where you are OK with periodic batch uploads on the order of every couple minutes, you can specify this behavior via the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 method
\f1\fs32 \cf2 \cb3 \strokec2 \'a0keyword argument, and also set the frequency with the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 triggering_frequency
\f1\fs32 \cf2 \cb3 \strokec2 \'a0keyword argument. Learn more from the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.io.gcp.bigquery.html#apache_beam.io.gcp.bigquery.WriteToBigQuery"}}{\fldrslt \cf2 \cb3 \strokec2 Write data to BigQuery section of the apache_beam.io.gcp.bigquery module documentation}}.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls19\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Use the following code to add a transform to your pipeline that writes aggregated data to the BigQuery table.\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\
  agg_table_name,\
  schema=agg_table_schema,\
  create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\
  write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\
  )\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\partightenfactor0

\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb7 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \AppleTypeServices \cf2 \cb3 \strokec2 Task 5. Run your pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls20\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the terminal and execute the following code to run your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 export PROJECT_ID=$(gcloud config get-value project)\
export REGION='us-central1'\
export BUCKET=gs://$\{PROJECT_ID\}\
export PIPELINE_FOLDER=$\{BUCKET\}\
export RUNNER=DataflowRunner\
export PUBSUB_TOPIC=projects/$\{PROJECT_ID\}/topics/my_topic\
export WINDOW_DURATION=60\
export AGGREGATE_TABLE_NAME=$\{PROJECT_ID\}:logs.windowed_traffic\
export RAW_TABLE_NAME=$\{PROJECT_ID\}:logs.raw\
\
\
python3 streaming_minute_traffic_pipeline.py \\\
--project=$\{PROJECT_ID\} \\\
--region=$\{REGION\} \\\
--staging_location=$\{PIPELINE_FOLDER\}/staging \\\
--temp_location=$\{PIPELINE_FOLDER\}/temp \\\
--runner=$\{RUNNER\} \\\
--input_topic=$\{PUBSUB_TOPIC\} \\\
--window_duration=$\{WINDOW_DURATION\} \\\
--agg_table_name=$\{AGGREGATE_TABLE_NAME\} \\\
--raw_table_name=$\{RAW_TABLE_NAME\}\cf0 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf6 \cb1 \strokec6 Copied!\
\pard\pardeftab720\qc\partightenfactor0

\fs48 \cf6 \
\pard\pardeftab720\partightenfactor0
\cf6 content_copy
\fs32 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb7 \strokec6 Note:\'a0
\f1\b0 \AppleTypeServices If you get a Dataflow pipeline failed error saying that it is unable to open the
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb7 \strokec6 \'a0
\f6\fs30 \AppleTypeServices \cb9 pipeline.py
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cb7 \'a0
\f1 \AppleTypeServices \cf6 \cb7 \strokec6 file, run the pipeline again and it should run with no issues.
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf6 \cb7 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \AppleTypeServices \cf2 \cb3 \strokec2 Ensure in the\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/dataflow/jobs"}}{\fldrslt \cf2 \cb3 \strokec2 Dataflow UI}}\'a0that it executes successfully without errors. Note that there is no data yet being created and ingested by the pipeline, so it will be running but not processing anything. You will introduce data in the next step.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 6. Generate lag-less streaming input\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 Because this is a streaming pipeline, it subscribes to the streaming source and will await input; there is none currently. In this section, you generate data with no lag. Actual data will almost invariably contain lag. However, it is instructive to understand lag-less streaming inputs.\
The code for this quest includes a script for publishing JSON events using Pub/Sub.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls21\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task and start publishing messages, open a\'a0
\f0\b new terminal
\f1\b0 \'a0side-by-side with your current one and run the following script. It will keep publishing messages until you kill the script. Make sure you are in the\'a0
\f6\fs30 \cb9 training-data-analyst/quests/dataflow_python
\f1\fs32 \cb3 \'a0folder.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 bash generate_streaming_events.sh\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Examine the results\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls22\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Wait a couple minutes for the data to start to populate. Then navigate to\'a0{\field{\*\fldinst{HYPERLINK "http://console.cloud.google.com/bigquery"}}{\fldrslt BigQuery}}\'a0and query the\'a0
\f6\fs30 \cb9 logs.minute_traffic
\f1\fs32 \cb3 \'a0table with the following query:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf10 \cb3 \strokec10 SELECT\cf6 \cb3 \strokec4  \cf11 \cb3 \strokec11 timestamp\cf6 \cb3 \strokec4 , page_views\
\cf10 \cb3 \strokec10 FROM\cf6 \cb3 \strokec4  `logs.windowed_traffic`\
\cf10 \cb3 \strokec10 ORDER\cf6 \cb3 \strokec4  \cf10 \cb3 \strokec10 BY\cf6 \cb3 \strokec4  \cf11 \cb3 \strokec11 timestamp\cf6 \cb3 \strokec4  \cf10 \cb3 \strokec10 ASC\cf0 \cb3 \strokec4 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 You should see that the number of pageviews hovered around 100 views a minute.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls23\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Alternatively, you can use the BigQuery command-line tool as a quick way to confirm results are being written:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 bq head logs.raw\
bq head logs.windowed_traffic\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls24\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Now, enter the following query:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf10 \cb3 \strokec10 SELECT\cf6 \cb3 \strokec4 \
  UNIX_MILLIS(\cf11 \cb3 \strokec11 TIMESTAMP\cf6 \cb3 \strokec4 (event_timestamp)) - min_millis.min_event_millis \cf10 \cb3 \strokec10 AS\cf6 \cb3 \strokec4  event_millis,\
  UNIX_MILLIS(\cf11 \cb3 \strokec11 TIMESTAMP\cf6 \cb3 \strokec4 (processing_timestamp)) - min_millis.min_event_millis \cf10 \cb3 \strokec10 AS\cf6 \cb3 \strokec4  processing_millis,\
  user_id,\
\
  \cf12 \cb3 \strokec12 -- added as unique label so we see all the points\cf6 \cb3 \strokec4 \
  \cf11 \cb3 \strokec11 CAST\cf6 \cb3 \strokec4 (UNIX_MILLIS(\cf11 \cb3 \strokec11 TIMESTAMP\cf6 \cb3 \strokec4 (event_timestamp)) - min_millis.min_event_millis \cf10 \cb3 \strokec10 AS\cf6 \cb3 \strokec4  STRING) \cf10 \cb3 \strokec10 AS\cf6 \cb3 \strokec4  label\
\cf10 \cb3 \strokec10 FROM\cf6 \cb3 \strokec4 \
  `logs.raw`\
\cf10 \cb3 \strokec10 CROSS\cf6 \cb3 \strokec4  \cf10 \cb3 \strokec10 JOIN\cf6 \cb3 \strokec4  (\
  \cf10 \cb3 \strokec10 SELECT\cf6 \cb3 \strokec4 \
    \cf11 \cb3 \strokec11 MIN\cf6 \cb3 \strokec4 (UNIX_MILLIS(\cf11 \cb3 \strokec11 TIMESTAMP\cf6 \cb3 \strokec4 (event_timestamp))) \cf10 \cb3 \strokec10 AS\cf6 \cb3 \strokec4  min_event_millis\
  \cf10 \cb3 \strokec10 FROM\cf6 \cb3 \strokec4 \
    `logs.raw`) min_millis\
\cf10 \cb3 \strokec10 WHERE\cf6 \cb3 \strokec4 \
  event_timestamp \cf10 \cb3 \strokec10 IS\cf6 \cb3 \strokec4  \cf10 \cb3 \strokec10 NOT\cf6 \cb3 \strokec4  \cf10 \cb3 \strokec10 NULL\cf6 \cb3 \strokec4 \
\cf10 \cb3 \strokec10 ORDER\cf6 \cb3 \strokec4  \cf10 \cb3 \strokec10 BY\cf6 \cb3 \strokec4 \
  event_millis \cf10 \cb3 \strokec10 ASC\cf0 \cb3 \strokec4 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 This query illustrates the gap between event time and processing time. However, it can be hard to see the big picture by looking at just the raw tabular data. We will use Looker Studio, a lightweight data visualization and BI engine.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls25\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To enable Looker Studio:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls26\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Visit\'a0{\field{\*\fldinst{HYPERLINK "https://lookerstudio.google.com/"}}{\fldrslt Looker Studio}}.\cb1 \
\ls26\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b Create
\f1\b0 \'a0in the upper left.\cb1 \
\ls26\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b Report
\f1\b0 .\cb1 \
\ls26\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Select\'a0
\f0\b Country
\f1\b0 \'a0name and enter a\'a0
\f0\b Company
\f1\b0 \'a0name. Check the checkbox to acknowledge you have read and agree to the Looker Studio Additional Terms, then click\'a0
\f0\b Continue
\f1\b0 .\cb1 \
\ls26\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Select "No" to all options, then click\'a0
\f0\b Continue
\f1\b0 .\cb1 \
\ls26\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the BigQuery UI.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls27\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the BigQuery UI, click on the\'a0
\f0\b Explore data
\f1\b0 \'a0button and choose\'a0
\f0\b Explore With Looker Studio
\f1\b0 .\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 This will open a new window.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls28\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b Get Started
\f1\b0 .\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Notice that default visualizations are created for the data.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls29\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To remove the default visualizations, right-click on each, and select\'a0
\f0\b Delete
\f1\b0 .\cb1 \
\ls29\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b Add a chart
\f1\b0 \'a0on the top menu bar.\cb1 \
\ls29\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	9	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Select the\'a0
\f0\b Scatter chart
\f1\b0 \'a0type.\cb1 \
\ls29\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	10	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the\'a0
\f0\b Data
\f1\b0 \'a0column of the panel on the right hand side, set the following values:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls30\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Dimension: label\cb1 \
\ls30\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Metric X: event_millis\cb1 \
\ls30\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Metric Y: processing_millis\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 The chart will transform to be a scatterplot, where all points are on the diagonal. This is because in the streaming data currently being generated, events are processed immediately after they were generated \'97 there was no lag. If you started the data generation script quickly, i.e. before the Dataflow job was fully up and running, you may see a hockey stick, as there were messages queuing in Pub/Sub that were all processed more or less at once.\
But in the real world, lag is something that pipelines need to cope with.\
\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \cb1 \strokec4 {{\NeXTGraphic 0=.png \width31200 \height21160 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\partightenfactor0
\cf0 \strokec4 \
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 7. Introduce lag to streaming input\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 The streaming event script is capable of generating events with simulated lag.\
This represents scenarios where there is a time delay between when the events are generated and published to Pub/Sub, for example when a mobile client goes into offline mode if a user has no service, but events are collected on the device and all published at once when the device is back online.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 Generate streaming input with lag\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls31\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 First, close the Looker Studio window.\cb1 \
\ls31\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Then, to turn on lag, return to the terminal and stop the running script using\'a0
\f6\fs30 \cb9 CTRL+C
\f1\fs32 \cb3 .\cb1 \
\ls31\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Then, run the following:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf6 \cb3 \strokec4 bash generate_streaming_events.sh true\cf0 \

\f1\fs32 \cf6 \cb1 \strokec6 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Examine the results\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls32\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the BigQuery UI, rerun the query, and then recreate the Looker Studio view as before. The new data that arrive, which should appear on the right side of the chart, should no longer be perfect; instead, some will appear above the diagonal, indicating that they were processed after the events transpired.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Chart Type: Scatter\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls33\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Dimension: label\cb1 \
\ls33\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Metric X: event_millis\cb1 \
\ls33\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Metric Y: processing_millis\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf0 \cb3 \strokec4 \
\pard\pardeftab720\partightenfactor0

\f1\fs24 \cf0 \cb1 \strokec4 {{\NeXTGraphic p0zJbBsA=.png \width26420 \height15880 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}}