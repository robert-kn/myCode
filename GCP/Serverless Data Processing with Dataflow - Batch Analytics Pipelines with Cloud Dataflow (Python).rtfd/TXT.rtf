{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 Helvetica;
\f3\fmodern\fcharset0 Courier;\f4\fswiss\fcharset0 Arial-BoldMT;\f5\fswiss\fcharset0 ArialMT;
\f6\fmodern\fcharset0 Courier-Bold;}
{\colortbl;\red255\green255\blue255;\red24\green25\blue27;\red255\green255\blue255;\red0\green0\blue0;
\red24\green24\blue24;\red227\green236\blue254;\red254\green246\blue217;\red251\green226\blue224;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c12549\c12941\c14118;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0\c5098;
\cssrgb\c12157\c12157\c12157;\cssrgb\c90980\c94118\c99608;\cssrgb\c99608\c96863\c87843;\cssrgb\c98824\c90980\c90196;\cssrgb\c0\c0\c0;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat6\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat7\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat8\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid25}
{\list\listtemplateid26\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid26}
{\list\listtemplateid27\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid27}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}{\listoverride\listid26\listoverridecount0\ls26}{\listoverride\listid27\listoverridecount0\ls27}}
\paperw11900\paperh16840\margl1440\margr1440\vieww33100\viewh16740\viewkind0
\deftab720
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Overview\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this lab, you:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Write a pipeline that aggregates site traffic by user.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Write a pipeline that aggregates site traffic by minute.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Implement windowing on time series data.\cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 Prerequisites\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\f1\fs32 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Basic familiarity with Python.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 Check project permissions\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 Before you begin your work on Google Cloud, you need to ensure that your project has the correct permissions within Identity and Access Management (IAM).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls3\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud console, on the\'a0
\f0\b Navigation menu
\f1\b0 \'a0(\cb1 {{\NeXTGraphic tkgw1TDgj4Q+YKQUW4jUFd0O5OEKlUMBRYbhlCrF0WY=.png \width300 \height260 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cb3 ), select\'a0
\f0\b IAM & Admin
\f1\b0 \'a0>\'a0
\f0\b IAM
\f1\b0 .\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Confirm that the default compute Service Account\'a0
\f3\fs30 \cb4 \{project-number\}-compute@developer.gserviceaccount.com
\f1\fs32 \cb3 \'a0is present and has the\'a0
\f3\fs30 \cb4 editor
\f1\fs32 \cb3 \'a0role assigned. The account prefix is the project number, which you can find on\'a0
\f0\b Navigation menu > Cloud Overview > Dashboard
\f1\b0 .\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 Note:\'a0
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 If the account is not present in IAM or does not have the\'a0
\f3\fs30 \AppleTypeServices \cb4 editor
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cb6 \'a0role, follow the steps below to assign the required role.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls4\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud console, on the\'a0
\f0\b Navigation menu
\f1\b0 , click\'a0
\f0\b Cloud Overview > Dashboard
\f1\b0 .\cb1 \
\ls4\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Copy the project number (e.g.\'a0
\f3\fs30 \cb4 729328892908
\f1\fs32 \cb3 ).\cb1 \
\ls4\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the\'a0
\f0\b Navigation menu
\f1\b0 , select\'a0
\f0\b IAM & Admin
\f1\b0 \'a0>\'a0
\f0\b IAM
\f1\b0 .\cb1 \
\ls4\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 At the top of the roles table, below\'a0
\f0\b View by Principals
\f1\b0 , click\'a0
\f0\b Grant Access
\f1\b0 .\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For\'a0
\f0\b New principals
\f1\b0 , type:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
  \{project-number\}-compute@developer.gserviceaccount.com\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Replace\'a0
\f3\fs30 \cb4 \{project-number\}
\f1\fs32 \cb3 \'a0with your project number.\cb1 \
\ls5\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For\'a0
\f0\b Role
\f1\b0 , select\'a0
\f0\b Project
\f1\b0 \'a0(or Basic) >\'a0
\f0\b Editor
\f1\b0 .\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b Save
\f1\b0 .\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Jupyter notebook-based development environment setup\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 For this lab, you will be running all commands in a terminal from your notebook.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls6\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud Console, on the\'a0
\f0\b Navigation Menu
\f1\b0 , click\'a0
\f0\b Vertex AI > Workbench
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Enable\'a0
\f0\b Notebooks API
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the Workbench page, click\'a0
\f0\b CREATE NEW
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the\'a0
\f0\b New instance
\f1\b0 \'a0dialog box that appears, set the region to\'a0
\f6\b \cb7 region
\f1\b0 \cb3 \'a0and zone to\'a0
\f6\b \cb7 zone
\f1\b0 \cb3 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For Environment, select\'a0
\f0\b Python 3 (with Intel\'ae MKL)
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b CREATE
\f1\b0 \'a0at the bottom of the dialog vox.\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb8 \strokec5 Note:\'a0
\f1\b0 \AppleTypeServices The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.\
\pard\pardeftab720\partightenfactor0

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \
\pard\pardeftab720\partightenfactor0

\f4\b \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 Note:\'a0
\f1\b0 \AppleTypeServices Click
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f4\b \AppleTypeServices\AppleTypeServicesF65539 Enable Notebook API
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f1 \AppleTypeServices to enable the notebook api.\
\pard\pardeftab720\partightenfactor0

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once the environment is ready, click the\'a0
\f0\b OPEN JUPYTERLAB
\f1\b0 \'a0link next to your Notebook name. This will open up your environment in a new tab in your browser.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, click\'a0
\f0\b Terminal
\f1\b0 . This will open up a terminal where you can run all the commands in this lab.\cb1 \
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Download Code Repository\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 Next you will download a code repository for use in this lab.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal you just opened, enter the following:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \strokec2 git clone https://github.com/GoogleCloudPlatform/training-data-analyst\
cd /home/jupyter/training-data-analyst/quests/dataflow_python/\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls10\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the left panel of your notebook environment, in the file browser, you will notice the\'a0
\f0\b training-data-analyst
\f1\b0 \'a0repo added.\cb1 \
\ls10\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate into the cloned repo\'a0
\f3\fs30 \cb4 /training-data-analyst/quests/dataflow_python/
\f1\fs32 \cb3 . You will see a folder for each lab, which is further divided into a\'a0
\f3\fs30 \cb4 lab
\f1\fs32 \cb3 \'a0sub-folder with code to be completed by you, and a\'a0
\f3\fs30 \cb4 solution
\f1\fs32 \cb3 \'a0sub-folder with a fully workable example to reference if you get stuck.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\b \cf5 \cb6 \strokec5 Note:\'a0
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 To open a file for editing purposes, simply navigate to the file and click on it. This will open the file, where you can add or modify code.\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \AppleTypeServices \cf2 \cb3 \strokec2 Part 1. Aggregating site traffic by user\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this part of the lab, you write a pipeline that:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls11\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Reads the day\'92s traffic from a file in Cloud Storage.\cb1 \
\ls11\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Converts each event into a\'a0
\f3\fs30 \cb4 CommonLog
\f1\fs32 \cb3 \'a0object.\cb1 \
\ls11\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Sums the number of hits for each unique user by grouping each object by user ID and combining the values to get the total number of hits for that particular user.\cb1 \
\ls11\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Performs additional aggregations on each user.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Writes the resulting data to BigQuery.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 1. Generate synthetic data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 As in the prior labs, the first step is to generate data for the pipeline to process. You will open the lab environment and generate the data as before:\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 Open the appropriate lab\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal in your IDE environment, run the following commands:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
# Change directory into the lab\
cd 3_Batch_Analytics/lab\
export BASE_DIR=$(pwd)\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Setting up virtual environment and dependencies\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Execute the following to create a virtual environment for your work in this lab:\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf5 \cb3 \strokec9 sudo apt-get update && sudo apt-get install -y python3-venv
\f1\fs32 \cf5 \cb1 \strokec5 \
\

\f3\fs28 \cf5 \cb3 \strokec9 # Create and activate virtual environment\
python3 -m venv df-env
\f1\fs32 \cf5 \cb1 \strokec5 \
\

\f3\fs28 \cf5 \cb3 \strokec9 source df-env/bin/activate
\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\qc\partightenfactor0

\fs48 \cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0

\fs32 \cf5 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls14\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, install the packages you will need to execute your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf5 \cb3 \strokec9 python3 -m pip install -q --upgrade pip setuptools wheel\
python3 -m pip install apache-beam[gcp]\cf0 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf5 \cb1 \strokec5 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls15\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Ensure that the Dataflow API is enabled:\cb1 \
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf5 \cb3 \strokec9 gcloud services enable dataflow.googleapis.com\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Set up the data environment\
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf2 \cb3 \strokec9 # Create GCS buckets and BQ dataset\
cd $BASE_DIR/../..\
source create_batch_sinks.sh\
\
# Generate event dataflow\
source generate_batch_events.sh\
\
# Change to the directory containing the practice version of the code\
cd $BASE_DIR\
\cf0 \
\
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 The script creates a file called\'a0
\f3\fs30 \cb4 events.json
\f1\fs32 \cb3 \'a0containing lines resembling the following:\
\
\{"user_id": "-6434255326544341291", "ip": "192.175.49.116", "timestamp": "2019-06-19T16:06:45.118306Z", "http_request": "\\"GET eucharya.html HTTP/1.0\\"", "lat": 37.751, "lng": -97.822, "http_response": 200, "user_agent": "Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)", "num_bytes": 182\}\
\
\
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 It then automatically copies this file to your Google Cloud Storage bucket at\'a0
\f6\b \cf2 \cb7 \strokec2 Cloud Storage path
\f1\b0 \cf2 \cb3 \strokec2 .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate to\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/storage"}}{\fldrslt Google Cloud Storage}}\'a0and confirm that your storage bucket contains a file called events.json.\cb1 \
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf0 \cb3 \strokec9 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 2. Sum page views per user\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In the file explorer, navigate to\'a0
\f3\fs30 \cf2 \cb4 \strokec2 training-data-analyst/quest/dataflow_python/3_Batch_Analytics/lab
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and open the\'a0
\f3\fs30 \cf2 \cb4 \strokec2 batch_user_traffic_pipeline.py
\f1\fs32 \cf2 \cb3 \strokec2 \'a0file. This pipeline already contains the necessary code to accept command-line options for the input path and the output table name, as well as code to read in events from Google Cloud Storage, parse those events, and write results to BigQuery. However, some important parts are missing.\
The next step in the pipeline is to aggregate the events by each unique\'a0
\f3\fs30 \cf2 \cb4 \strokec2 user_id
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and count page views for each. An easy way to do this on objects of type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 beam.Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0or objects with a Beam schema is to use the\'a0
\f3\fs30 \cf2 \cb4 \strokec2 GroupBy
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform and then perform some aggregations on the resulting group. For example:\
\pard\pardeftab720\partightenfactor0
\cf5 \cb1 \strokec5 \
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf5 \cb3 \strokec9 purchases | GroupBy('user_id', 'address')\cf0 \
\
\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 will return a PCollection of rows with two fields.\
The first is a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0with schema representing every unique combination of\'a0
\f3\fs30 \cf2 \cb4 \strokec2 'user_id'
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and\'a0
\f3\fs30 \cf2 \cb4 \strokec2 address
\f1\fs32 \cf2 \cb3 \strokec2 \'a0(both strings), "key", and "values". The second field is an iterable of type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0containing all of the objects in the unique group from the first field.\
This is most useful when you can perform aggregate calculations on this grouping and name the resulting fields, like so:\
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf2 \cb3 \strokec9 (purchases | GroupBy('user_id')\
             .aggregate_field("item_id", CountCombineFn(), "num_purchases")\
             .aggregate_field("cost_cents", sum, "total_spend_cents")\
             .aggregate_field("cost_cents", max, "largest_purchases"))\
             .with_output_types(UserPurchases)\cf0 \
\
\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 This returns a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0with fields corresponding to the "key(s)" we grouped by and the corresponding aggregations computed here.\
The\'a0
\f3\fs30 \cf2 \cb4 \strokec2 aggregate_field
\f1\fs32 \cf2 \cb3 \strokec2 \'a0method takes three arguments. The first argument is a string, referring to the name of the field we wish to aggregate in the input PCollection's schema. The second is the combiner we wish to apply, implemented as a subclass of\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.CombineFn"}}{\fldrslt \cf2 \cb3 \strokec2 CombineFn}}. The third argument is a string that we use to identify the aggregation in the schema of the output PCollection.\
Certain aggregation functions, such as\'a0
\f3\fs30 \cf2 \cb4 \strokec2 sum
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and\'a0
\f3\fs30 \cf2 \cb4 \strokec2 max
\f1\fs32 \cf2 \cb3 \strokec2 , are implemented directly as combiners in Beam Python\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.CombinePerKey"}}{\fldrslt \cf2 \cb3 \strokec2 (Link)}}. Count is implemented via\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Count"}}{\fldrslt \cf2 \cb3 \strokec2 CountCombineFn}}.\
The output PCollection by default is a PCollection of type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Row
\f1\fs32 \cf2 \cb3 \strokec2 , but we can also apply our own custom types with schema using\'a0
\f3\fs30 \cf2 \cb4 \strokec2 with_output_types
\f1\fs32 \cf2 \cb3 \strokec2 . We see that above with\'a0
\f3\fs30 \cf2 \cb4 \strokec2 UserPurchases
\f1\fs32 \cf2 \cb3 \strokec2 . However, this means that we need to define a schema for type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 UserPurchases
\f1\fs32 \cf2 \cb3 \strokec2 . We can do so easily by creating a subclass of\'a0
\f3\fs30 \cf2 \cb4 \strokec2 typing.NamedTuple
\f1\fs32 \cf2 \cb3 \strokec2 \'a0or via creating the schema ad hoc using\'a0
\f3\fs30 \cf2 \cb4 \strokec2 beam.Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0or\'a0
\f3\fs30 \cf2 \cb4 \strokec2 beam.Select
\f1\fs32 \cf2 \cb3 \strokec2 . We will cover the first case here. For the second please refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/programming-guide/#schema-definition"}}{\fldrslt \cf2 \cb3 \strokec2 Beam programming guide}}.\
The output of our aggregation above has four fields:\'a0
\f3\fs30 \cf2 \cb4 \strokec2 user_id
\f1\fs32 \cf2 \cb3 \strokec2 \'a0(type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 str
\f1\fs32 \cf2 \cb3 \strokec2 ),\'a0
\f3\fs30 \cf2 \cb4 \strokec2 num_purchases
\f1\fs32 \cf2 \cb3 \strokec2 ,\'a0
\f3\fs30 \cf2 \cb4 \strokec2 total_spend_cents
\f1\fs32 \cf2 \cb3 \strokec2 , and\'a0
\f3\fs30 \cf2 \cb4 \strokec2 largest_purchases
\f1\fs32 \cf2 \cb3 \strokec2 \'a0(all type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 int
\f1\fs32 \cf2 \cb3 \strokec2 ).\
We create a subclass of\'a0
\f3\fs30 \cf2 \cb4 \strokec2 NamedTuple
\f1\fs32 \cf2 \cb3 \strokec2 \'a0with these field names and types then register the coder for the schema:\
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf2 \cb3 \strokec9 class UserPurchases(typing.NamedTuple):\
  user_id : str\
  num_purchases : int\
  total_spend_cents : int\
  largest_purchases : int\
\
beam.coders.registry.register_coder(UserPurchases, beam.coders.RowCoder)\cf0 \
\
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 Note:\'a0
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 In this example you could aggregate on any of the fields for `CountCombineFn()`, or even on the wildcard field `*`, as this transform is simply counting how many elements are in the entire group.\
\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \AppleTypeServices \cf2 \cb3 \strokec2 The next step in the pipeline is to aggregate events by user_id, sum the pageviews, and also calculate some additional aggregations on num_bytes, for example total user bytes, maximum user bytes, and minimum user bytes.\
To complete this task, add another transform to the pipeline that groups the events by\'a0
\f3\fs30 \cf2 \cb4 \strokec2 user_id
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and then performs the relevant aggregations. Keep in mind the input, the CombineFns to use, and how you name the output fields. After this, create a new output type with schema (call it\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PerUserAggregation
\f1\fs32 \cf2 \cb3 \strokec2 ) and ensure that the output\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0is converted into this type. The tasks in the code are marked with\'a0
\f3\fs30 \cf2 \cb4 \strokec2 # TODO
\f1\fs32 \cf2 \cb3 \strokec2 .\
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf0 \cb3 \strokec9 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 3. Run your pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to Cloud Shell and execute the following command to run your pipeline using the Cloud Dataflow service. You can run it with DirectRunner if you're having trouble, or refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_user_traffic_pipeline.py"}}{\fldrslt solution}}.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
export PROJECT_ID=$(gcloud config get-value project)\
export REGION=Region\
export BUCKET=gs://$\{PROJECT_ID\}\
export PIPELINE_FOLDER=$\{BUCKET\}\
export RUNNER=DataflowRunner\
export INPUT_PATH=$\{PIPELINE_FOLDER\}/events.json\
export TABLE_NAME=$\{PROJECT_ID\}:logs.user_traffic\
\
cd $BASE_DIR\
python3 batch_user_traffic_pipeline.py \\\
--project=$\{PROJECT_ID\} \\\
--region=$\{REGION\} \\\
--staging_location=$\{PIPELINE_FOLDER\}/staging \\\
--temp_location=$\{PIPELINE_FOLDER\}/temp \\\
--runner=$\{RUNNER\} \\\
--input_path=$\{INPUT_PATH\} \\\
--table_name=$\{TABLE_NAME\}\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 4. Verify results in BigQuery\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls18\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, wait a few minutes for the pipeline to complete, then navigate to\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/bigquery"}}{\fldrslt BigQuery}}\'a0and query the\'a0
\f3\fs30 \cb4 user_traffic
\f1\fs32 \cb3 \'a0table.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Part 2. Aggregating site traffic by minute\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this part of the lab, you create a new pipeline called\'a0
\f3\fs30 \cf2 \cb4 \strokec2 batch_minute_traffic
\f1\fs32 \cf2 \cb3 \strokec2 .\'a0
\f3\fs30 \cf2 \cb4 \strokec2 batch_minute_traffic
\f1\fs32 \cf2 \cb3 \strokec2 \'a0expands on the basic batch analysis principles used in\'a0
\f3\fs30 \cf2 \cb4 \strokec2 batch_user_traffic
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and, instead of aggregating by users across the entire batch, aggregates by when events occurred.\
In the IDE, open the file\'a0
\f3\fs30 \cf2 \cb4 \strokec2 batch_minute_traffic_pipeline
\f1\fs32 \cf2 \cb3 \strokec2 \'a0inside\'a0
\f3\fs30 \cf2 \cb4 \strokec2 3_Batch_Analytics/lab
\f1\fs32 \cf2 \cb3 \strokec2 .\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 5. Add timestamps to each element\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 An unbounded source provides a timestamp for each element. Depending on your unbounded source, you may need to configure how the timestamp is extracted from the raw data stream.\
However, bounded sources (such as a file from TextIO, as is used in this pipeline) do not provide timestamps.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls19\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 You can parse the timestamp field from each record and use the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.window.html#apache_beam.transforms.window.TimestampedValue"}}{\fldrslt beam.window.TimestampedValue}}\'a0transform to attach the timestamps to each element in your PCollection.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
def add_timestamp(element):\
  ts = # Do Something\
  return beam.window.TimestampedValue(element, ts)\
\
unstamped = ...\
stamped = unstamped | beam.Map(add_timestamp)\
\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls20\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, add a transform to the pipeline that adds timestamps to each element of the pipeline. To do this, leverage the\'a0{\field{\*\fldinst{HYPERLINK "https://docs.python.org/3/library/datetime.html"}}{\fldrslt 
\f3\fs30 \cb4 datetime}}\'a0package to convert the timestamp field of the element into a\'a0
\f3\fs30 \cb4 datetime
\f1\fs32 \cb3 \'a0object. You may need to explore the\'a0{\field{\*\fldinst{HYPERLINK "https://docs.python.org/3/library/datetime.html"}}{\fldrslt 
\f3\fs30 \cb4 datetime.strptime}}\'a0function to do so.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 6. Window into one-minute windows\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 Windowing subdivides a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0according to the timestamps of its individual elements. Transforms that aggregate multiple elements, such as\'a0
\f3\fs30 \cf2 \cb4 \strokec2 GroupByKey
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and Combine, work implicitly on a per-window basis \'97 they process each\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0as a succession of multiple, finite windows, though the entire collection itself may be of unbounded size.\
You can define different kinds of windows to divide the elements of your\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 . Beam provides several windowing functions, including:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls21\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fixed-time windows\cb1 \
\ls21\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Sliding-time windows\cb1 \
\ls21\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Per-session windows\cb1 \
\ls21\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Single global window\cb1 \
\ls21\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Calendar-based windows (not supported by the Beam SDK for Python, as of when this lab was written)\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 In this lab, you use fixed-time windows. A fixed-time window represents a non-overlapping time interval of consistent duration in the data stream. Consider windows with a five-minute duration: all of the elements in your unbounded\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0with timestamp values from 0:00:00 up to (but not including) 0:05:00 belong to the first window, elements with timestamp values from 0:05:00 up to (but not including) 0:10:00 belong to the second window, and so on.\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \cb1 \strokec9 {{\NeXTGraphic a6yHjc4nHwlFgQxOM=.png \width13320 \height8680 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls22\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Implement a fixed-time window with a five-minute duration as follows:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 p = ...\
p_windowed = p | beam.WindowInto(beam.window.FixedWindows(5*60))\
\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls23\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, add a transform to your pipeline that windows elements into fixed windows one minute long.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 To learn more about other types of windowing, read the Apache Beam documentation\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/programming-guide/#provided-windowing-functions"}}{\fldrslt \cf2 \cb3 \strokec2 Section 8.2. Provided windowing functions}}.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 7. Count events per window\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 Next, the pipeline needs to compute the number of events that occurred within each window. In the batch_user_traffic pipeline, a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 sum
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform was used to sum per key. However, unlike in that pipeline, in this case the elements have been windowed and the desired computation needs to respect window boundaries.\
Despite this new constraint, the Combine transform is still appropriate. That\'92s because Combine transforms automatically respect window boundaries.\
Refer to the documentation for\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Count"}}{\fldrslt \cf2 \cb3 \strokec2 Count}}\'a0for how to add a new transform that counts the number of elements per window.\
As of Beam 2.28, the best option to count elements of rows while windowing is to use\'a0
\f3\fs30 \cf2 \cb4 \strokec2 beam.CombineGlobally(CountCombineFn()).without_defaults()
\f1\fs32 \cf2 \cb3 \strokec2 \'a0(that is, without using full-on SQL, which we will cover more in the next lab). This transform will output a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0of type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 int
\f1\fs32 \cf2 \cb3 \strokec2 \'a0which, you'll notice, is no longer using Beam schemas.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls24\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, add a transform that counts all the elements in each window. Remember to refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_minute_traffic_pipeline.py"}}{\fldrslt solution}}\'a0if you get stuck.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 8. Convert back to a row and add timestamp\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In order to write to BigQuery, each element needs to be converted to a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 dict
\f1\fs32 \cf2 \cb3 \strokec2 \'a0object with "page_views" as a field and additional field called "timestamp". The idea is to use the boundary of each window as one field and the combined number of pageviews as the other.\
One other issue, at this point, is that the Count transform is only providing elements of type\'a0
\f3\fs30 \cf2 \cb4 \strokec2 int
\f1\fs32 \cf2 \cb3 \strokec2 \'a0that no longer bear any sort of timestamp information.\
In fact, however, they do, though not in so obvious a way. Apache Beam runners know by default how to supply the value for a number of additional parameters, including event timestamps, windows, and pipeline options; for a full list refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/programming-guide/#other-dofn-parameters"}}{\fldrslt \cf2 \cb3 \strokec2 Apache's DoFn parameters documentation}}.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls25\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, write a ParDo function that accepts elements of type int, passes in the additional parameter to access window information,\'a0
\f3\fs30 \cb4 beam.DoFn.WindowParam
\f1\fs32 \cb3 , and emits dictionaries with the fields mentioned above. Note that the timestamp field in the BigQuery table schema is a STRING, so you will have to convert the timestamp to a string. The\'a0
\f3\fs30 \cb4 datetime.strftime
\f1\fs32 \cb3 \'a0function will be helpful here.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 class GetTimestampFn(beam.DoFn):\
    def process(self, element, window=beam.DoFn.WindowParam):\
        window_start = #Do something!\
        output = \{'page_views': element, 'timestamp': window_start\}\
        yield output\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 9. Run the pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls26\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once you\'92ve finished coding, run the pipeline using the command below. Keep in mind that, while testing your code, it will be much faster to change the RUNNER environment variable to DirectRunner, which will run the pipeline locally.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
export PROJECT_ID=$(gcloud config get-value project)\
export REGION=Region\
export BUCKET=gs://$\{PROJECT_ID\}\
export PIPELINE_FOLDER=$\{BUCKET\}\
export RUNNER=DataflowRunner\
export INPUT_PATH=$\{PIPELINE_FOLDER\}/events.json\
export TABLE_NAME=$\{PROJECT_ID\}:logs.minute_traffic\
\
cd $BASE_DIR\
python3 batch_minute_traffic_pipeline.py \\\
--project=$\{PROJECT_ID\} \\\
--region=$\{REGION\} \\\
--staging_location=$\{PIPELINE_FOLDER\}/staging \\\
--temp_location=$\{PIPELINE_FOLDER\}/temp \\\
--runner=$\{RUNNER\} \\\
--input_path=$\{INPUT_PATH\} \\\
--table_name=$\{TABLE_NAME\}\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 10. Verify the results\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls27\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, wait a few minutes for the pipeline to execute, then navigate to\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/bigquery"}}{\fldrslt BigQuery}}\'a0and query the\'a0
\f3\fs30 \cb4 minute_traffic
\f1\fs32 \cb3 \'a0table.\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
}