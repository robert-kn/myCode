{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 Helvetica;
\f3\fmodern\fcharset0 Courier-Bold;\f4\fswiss\fcharset0 Arial-BoldMT;\f5\fswiss\fcharset0 ArialMT;
\f6\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red24\green25\blue27;\red255\green255\blue255;\red254\green246\blue217;
\red24\green24\blue24;\red251\green226\blue224;\red227\green236\blue254;\red0\green0\blue0;\red0\green0\blue0;
\red0\green0\blue117;\red16\green121\blue2;\red82\green0\blue83;\red83\green85\blue2;}
{\*\expandedcolortbl;;\cssrgb\c12549\c12941\c14118;\cssrgb\c100000\c100000\c100000;\cssrgb\c99608\c96863\c87843;
\cssrgb\c12157\c12157\c12157;\cssrgb\c98824\c90980\c90196;\cssrgb\c90980\c94118\c99608;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c0\c5098;
\cssrgb\c0\c0\c53333;\cssrgb\c0\c53333\c0;\cssrgb\c40000\c0\c40000;\cssrgb\c40000\c40000\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat7\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat8\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat4\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat5\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat6\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid25}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}}
\paperw11900\paperh16840\margl1440\margr1440\vieww33100\viewh16740\viewkind0
\deftab720
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Overview\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this lab, you:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Write a pipeline that uses SQL to aggregate site traffic by user.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Write a pipeline that uses SQL to aggregate site traffic by minute.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Jupyter notebook-based development environment setup\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 For this lab, you will be running all commands in a terminal from your notebook.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud Console, on the\'a0
\f0\b Navigation Menu
\f1\b0 , click\'a0
\f0\b Vertex AI > Workbench
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Enable\'a0
\f0\b Notebooks API
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the Workbench page, click\'a0
\f0\b CREATE NEW
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the\'a0
\f0\b New instance
\f1\b0 \'a0dialog box that appears, set the region to\'a0
\f3\b \cb4 region
\f1\b0 \cb3 \'a0and zone to\'a0
\f3\b \cb4 zone
\f1\b0 \cb3 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For Environment, select\'a0
\f0\b Apache Beam
\f1\b0 .\cb1 \
\ls2\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b CREATE
\f1\b0 \'a0at the bottom of the dialog vox.\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 Note:\'a0
\f1\b0 \AppleTypeServices The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.\

\f5 \AppleTypeServices\AppleTypeServicesF65539 \

\f4\b \AppleTypeServices\AppleTypeServicesF65539 \cb7 Note:\'a0
\f1\b0 \AppleTypeServices Click
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f4\b \AppleTypeServices\AppleTypeServicesF65539 Enable Notebook API
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f1 \AppleTypeServices to enable the notebook api.\

\f5 \AppleTypeServices\AppleTypeServicesF65539 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once the environment is ready, click the\'a0
\f0\b OPEN JUPYTERLAB
\f1\b0 \'a0link next to your Notebook name. This will open up your environment in a new tab in your browser.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, click\'a0
\f0\b Terminal
\f1\b0 . This will open up a terminal where you can run all the commands in this lab.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Download Code Repository\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Next you will download a code repository for use in this lab.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal you just opened, enter the following:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 git clone https://github.com/GoogleCloudPlatform/training-data-analyst\
cd /home/jupyter/training-data-analyst/quests/dataflow_python/\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls6\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the left panel of your notebook environment, in the file browser, you will notice the\'a0
\f0\b training-data-analyst
\f1\b0 \'a0repo added.\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate into the cloned repo\'a0
\f6\fs30 \cb9 /training-data-analyst/quests/dataflow_python/
\f1\fs32 \cb3 . You will see a folder for each lab, which is further divided into a\'a0
\f6\fs30 \cb9 lab
\f1\fs32 \cb3 \'a0sub-folder with code to be completed by you, and a\'a0
\f6\fs30 \cb9 solution
\f1\fs32 \cb3 \'a0sub-folder with a fully workable example to reference if you get stuck.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 Part 1: Aggregating site traffic by user with SQL\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this part of the lab, you rewrite your previous BatchUserTraffic pipeline so that it performs the following:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Reads the day\'92s traffic from a file in Cloud Storage.\cb1 \
\ls7\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Converts each event into a\'a0
\f6\fs30 \cb9 CommonLog
\f1\fs32 \cb3 \'a0object.\cb1 \
\ls7\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Uses SQL instead of Java transforms to sum the number of hits for each unique user ID and perform additional aggregations.\cb1 \
\ls7\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Writes the resulting data to BigQuery.\cb1 \
\ls7\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Has an additional branch that writes the raw data to BigQuery for later analysis.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 1. Generate synthetic data\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 As in the prior labs, the first step is to generate data for the pipeline to process. You will open the lab environment and generate the data as before:\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Open the appropriate lab\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0
\f1\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal in your IDE, change to the directory you will use for this lab:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 # Change directory into the lab\
cd 4_SQL_Batch_Analytics/lab\
export BASE_DIR=$(pwd)\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Setting up dependencies\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In your terminal, execute the following to install the packages you will need to execute your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 python3 -m pip install -q --upgrade pip setuptools wheel\
python3 -m pip install apache-beam[gcp]\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Ensure that the Dataflow and Data Catalog APIs are enabled:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 gcloud services enable dataflow.googleapis.com\
gcloud services enable datacatalog.googleapis.com\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Set up the data environment\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 # Create GCS buckets and BQ dataset\
cd $BASE_DIR/../..\
source create_batch_sinks.sh\
\
# Generate event dataflow\
source generate_batch_events.sh\
\
# Change to the directory containing the practice version of the code\
cd $BASE_DIR\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 The script creates a file called\'a0
\f6\fs30 \cf2 \cb9 \strokec2 events.json
\f1\fs32 \cf2 \cb3 \strokec2 \'a0containing lines resembling the following:\
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 \{"user_id": "-6434255326544341291", "ip": "192.175.49.116", "timestamp": "2019-06-19T16:06:45.118306Z", "http_request": "\\"GET eucharya.html HTTP/1.0\\"", "lat": 37.751, "lng": -97.822, "http_response": 200, "user_agent": "Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)", "num_bytes": 182\}\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 It then automatically copies this file to your Google Cloud Storage bucket at\'a0
\f3\b \cf2 \cb4 \strokec2 Cloud Storage path
\f1\b0 \cf2 \cb3 \strokec2 .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate to\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/storage"}}{\fldrslt Google Cloud Storage}}\'a0and confirm that your storage bucket contains a file called\'a0
\f6\fs30 \cb9 events.json
\f1\fs32 \cb3 .\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 2. Add SQL dependencies\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In your File Explorer, navigate to\'a0
\f6\fs30 \cb9 training-data-analyst/quests/dataflow_python/4_SQL_Batch_Analytics/lab/
\f1\fs32 \cb3 \'a0and open the\'a0
\f6\fs30 \cb9 batch_user_traffic_SQL_pipeline.py
\f1\fs32 \cb3 \'a0file.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 This pipeline already contains the necessary code to accept command-line options for the input path and one output table name, as well as code to read in events from Google Cloud Storage, parse those events, and write results to BigQuery. However, some important parts are missing.\
As in the previous lab, the next step in the pipeline is to aggregate the events by each unique\'a0
\f6\fs30 \cf2 \cb9 \strokec2 user_id
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and count pageviews for each. This time, however, you will perform the aggregation using SQL, using\'a0
\f6\fs30 \cf2 \cb9 \strokec2 SqlTransform
\f1\fs32 \cf2 \cb3 \strokec2 \'a0instead of Python-based transforms.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In\'a0
\f6\fs30 \cb9 batch_user_traffic_SQL_pipeline.py
\f1\fs32 \cb3 , add the following import statement:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf10 \cb3 \strokec10 from\cf5 \cb3 \strokec8  apache_beam.transforms.sql \cf10 \cb3 \strokec10 import\cf5 \cb3 \strokec8  SqlTransform\cf0 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls14\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, you will add the following SQL query to the file in the\'a0
\f6\fs30 \cb9 query
\f1\fs32 \cb3 \'a0variable definition:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 SELECT user_id,\
COUNT(*) AS page_views, SUM(num_bytes) as total_bytes,\
MAX(num_bytes) AS max_bytes, MIN(num_bytes) as min_bytes\
FROM PCOLLECTION\
GROUP BY user_id\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls15\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, fill out the following\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 . You will use this code to write a Transform to write raw data to BigQuery:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 logs | \cf11 \cb3 \strokec11 'WriteRawToBQ'\cf5 \cb3 \strokec8  >> beam.io.WriteToBigQuery(\
  raw_table_name,\
  schema=raw_table_schema,\
  create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\
  write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\
  )\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Beam SQL can be implemented in both the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/dsls/sql/calcite/overview/"}}{\fldrslt \cf2 \cb3 \strokec2 Apache Calcite dialects (default)}}\'a0and\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/dsls/sql/zetasql/overview/"}}{\fldrslt \cf2 \cb3 \strokec2 ZetaSQL dialects}}. While both will be executed by Dataflow, we will be implementing ZetaSQL in this example, as it is similar to the language used in BigQuery and is also the dialect implemented in 'Dataflow SQL' \'97 SQL queries authored directly in the Dataflow UI.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Fill out the last\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 . Apply a SQLTransform using ZetaSQL Dialect using the following code:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 SqlTransform(query, dialect='zetasql')\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Save the changes you made to the file.\cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 3. Run your pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls18\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the terminal and execute the following code to run your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  PROJECT_ID=$(gcloud\cf12 \cb3 \strokec12  config \cf5 \cb3 \strokec8 get-value project)\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  REGION=Region\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  BUCKET=gs://$\{PROJECT_ID\}\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  PIPELINE_FOLDER=\cf13 \cb3 \strokec13 $\{BUCKET\}\cf5 \cb3 \strokec8 \
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  RUNNER=DataflowRunner\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  INPUT_PATH=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf5 \cb3 \strokec8 /events.json\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  TABLE_NAME=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf5 \cb3 \strokec8 :logs.user_traffic\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  AGGREGATE_TABLE_NAME=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf5 \cb3 \strokec8 :logs.user_traffic\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  RAW_TABLE_NAME=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf5 \cb3 \strokec8 :logs.raw\
\
python3 batch_user_traffic_SQL_pipeline.py \\\
--project=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf5 \cb3 \strokec8  \\\
--region=\cf13 \cb3 \strokec13 $\{REGION\}\cf5 \cb3 \strokec8  \\\
--staging_location=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf5 \cb3 \strokec8 /staging \\\
--temp_location=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf5 \cb3 \strokec8 /temp \\\
--runner=\cf13 \cb3 \strokec13 $\{RUNNER\}\cf5 \cb3 \strokec8  \\\
--experiments=use_runner_v2 \\\
--input_path=\cf13 \cb3 \strokec13 $\{INPUT_PATH\}\cf5 \cb3 \strokec8  \\\
--agg_table_name=\cf13 \cb3 \strokec13 $\{AGGREGATE_TABLE_NAME\}\cf5 \cb3 \strokec8  \\\
--raw_table_name=\cf13 \cb3 \strokec13 $\{RAW_TABLE_NAME\}\cf5 \cb3 \strokec8 \
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls19\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate to\'a0
\f0\b Navigation Menu
\f1\b0 \'a0>\'a0
\f0\b Dataflow
\f1\b0 \'a0to see the status of your pipeline.\cb1 \
\ls19\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once your pipeline has finished, go to the BigQuery UI to query the two resulting tables.\cb1 \
\ls19\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Make sure that\'a0
\f6\fs30 \cb9 logs.raw
\f1\fs32 \cb3 \'a0exists and has data populated, as you will need that later in the lab.\cb1 \
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Part 2: Aggregating site traffic by minute with SQL\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this part of the lab, you rewrite your previous BatchMinuteTraffic pipeline so that it performs the following:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls20\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Reads the day\'92s traffic from a file in Cloud Storage.\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Converts each event into a\'a0
\f6\fs30 \cb9 CommonLog
\f1\fs32 \cb3 \'a0object and then adds a Joda Timestamp attribute to the object.\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Uses SQL instead of Java transforms to again Window sum the number of total hits per minute.\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Writes the resulting data to BigQuery.\cb1 \
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 1. Add timestamp field to CommonLog row\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this task, you will add a Joda timestamp field to your\'a0
\f6\fs30 \cf2 \cb9 \strokec2 CommonLog
\f1\fs32 \cf2 \cb3 \strokec2 \'a0object, implicitly converting it to a generic\'a0
\f6\fs30 \cf2 \cb9 \strokec2 Row
\f1\fs32 \cf2 \cb3 \strokec2 \'a0object.\
The appropriate imports and options have already been set to use ZetaSQL. A pipeline has been built with steps to ingest and write the data, but not to transform or aggregate it. In the file explorer of the IDE, navigate to\'a0
\f6\fs30 \cf2 \cb9 \strokec2 training-data-analyst/quests/dataflow_python/4_SQL_Batch_Analytics/lab/
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and open the file\'a0
\f6\fs30 \cf2 \cb9 \strokec2 batch_minute_user_SQL_pipeline.py
\f1\fs32 \cf2 \cb3 \strokec2 .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls21\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For the first\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 , add the formatted timestamp as a string:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 ts = datetime.strptime(element.ts[:-8], "%Y-%m-%dT%H:%M:%S")\
ts = datetime.strftime(ts, "%Y-%m-%d %H:%M:%S")\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Note that when using the Python SDK, we currently cannot directly pass in objects of type\'a0
\f6\fs30 \cf2 \cb9 \strokec2 datetime
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to a\'a0
\f6\fs30 \cf2 \cb9 \strokec2 SqlTransform
\f1\fs32 \cf2 \cb3 \strokec2 . Instead, we will convert the object into a string using\'a0
\f6\fs30 \cf2 \cb9 \strokec2 strftime
\f1\fs32 \cf2 \cb3 \strokec2 \'a0and then in SQL use the\'a0
\f6\fs30 \cf2 \cb9 \strokec2 TIMESTAMP
\f1\fs32 \cf2 \cb3 \strokec2 \'a0function.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls22\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, add the following SQL query:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 SELECT\
    COUNT(*) AS page_views,\
    STRING(window_start) AS start_time\
FROM\
    TUMBLE(\
        (SELECT TIMESTAMP(ts) AS ts FROM PCOLLECTION),\
        DESCRIPTOR(ts),\
        'INTERVAL 1 MINUTE')\
GROUP BY window_start\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 Note:\'a0
\f1\b0 \AppleTypeServices In this SQL query, we convert our field
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf5 \cb9 \strokec5 ts
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 \'a0
\f1 \AppleTypeServices to type
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf5 \cb9 \strokec5 TIMESTAMP
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 \'a0
\f1 \AppleTypeServices and use it as the event timestamp for our fixed one-minute windows. The
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf5 \cb9 \strokec5 window_start
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 \'a0
\f1 \AppleTypeServices field is generated by
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf5 \cb9 \strokec5 TUMBLE
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 \'a0
\f1 \AppleTypeServices and is of type
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf5 \cb9 \strokec5 TIMESTAMP
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 \'a0
\f1 \AppleTypeServices as well. Due to the issue we discussed earlier with the Python SDK, we must convert this field to a
\f5 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f6\fs30 \AppleTypeServices \cf5 \cb9 \strokec5 STRING
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb7 \strokec5 \'a0
\f1 \AppleTypeServices before sending the resulting PCollection back to Python transforms.\

\f5 \AppleTypeServices\AppleTypeServicesF65539 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls23\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For the next\'a0
\f6\fs30 \cb9 #TODO
\f1\fs32 \cb3 , apply a SQLTransform using ZetaSQL Dialect using the following code:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 SqlTransform(query, dialect='zetasql')\cf0 \

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 2. Run your pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls24\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Back in your terminal, execute the following code to run your pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  PROJECT_ID=$(gcloud\cf12 \cb3 \strokec12  config \cf5 \cb3 \strokec8 get-value project)\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  REGION=Region\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  BUCKET=gs://$\{PROJECT_ID\}\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  PIPELINE_FOLDER=\cf13 \cb3 \strokec13 $\{BUCKET\}\cf5 \cb3 \strokec8 \
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  RUNNER=DataflowRunner\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  INPUT_PATH=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf5 \cb3 \strokec8 /events.json\
\cf12 \cb3 \strokec12 export\cf5 \cb3 \strokec8  TABLE_NAME=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf5 \cb3 \strokec8 :logs.minute_traffic\
\
\
\
python3 batch_minute_traffic_SQL_pipeline.py \\\
--project=\cf13 \cb3 \strokec13 $\{PROJECT_ID\}\cf5 \cb3 \strokec8  \\\
--region=Region \\\
--stagingLocation=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf5 \cb3 \strokec8 /staging \\\
--tempLocation=\cf13 \cb3 \strokec13 $\{PIPELINE_FOLDER\}\cf5 \cb3 \strokec8 /temp \\\
--runner=\cf13 \cb3 \strokec13 $\{RUNNER\}\cf5 \cb3 \strokec8  \\\
--inputPath=\cf13 \cb3 \strokec13 $\{INPUT_PATH\}\cf5 \cb3 \strokec8  \\\
--tableName=\cf13 \cb3 \strokec13 $\{TABLE_NAME\}\cf5 \cb3 \strokec8  \\\
--experiments=use_runner_v2\
\pard\pardeftab720\partightenfactor0

\f1\fs32 \cf5 \cb1 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls25\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Cloud Console, navigate to\'a0
\f0\b Navigation Menu
\f1\b0 \'a0>\'a0
\f0\b Dataflow
\f1\b0 \'a0to see the status of your pipeline.\cb1 \
\ls25\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once your pipeline has finished, go to the BigQuery UI to query the resulting\'a0
\f6\fs30 \cb9 logs.minute_traffic
\f1\fs32 \cb3 \'a0tables. Alternatively, you can query from the terminal:\cb1 \
\pard\pardeftab720\partightenfactor0

\f6\fs28 \cf5 \cb3 \strokec8 bq head -n 10 $PROJECT_ID:logs.minute_traffic\cf0 \
\pard\tx720\pardeftab720\partightenfactor0

\f1\fs32 \cf2 \cb1 \strokec2 \
}