{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 Helvetica;
\f3\fmodern\fcharset0 Courier;\f4\fswiss\fcharset0 Arial-BoldMT;\f5\fswiss\fcharset0 ArialMT;
\f6\fmodern\fcharset0 Courier-Bold;\f7\froman\fcharset0 Times-Italic;}
{\colortbl;\red255\green255\blue255;\red24\green25\blue27;\red255\green255\blue255;\red0\green0\blue0;
\red24\green24\blue24;\red227\green236\blue254;\red254\green246\blue217;\red251\green226\blue224;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c12549\c12941\c14118;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0\c5098;
\cssrgb\c12157\c12157\c12157;\cssrgb\c90980\c94118\c99608;\cssrgb\c99608\c96863\c87843;\cssrgb\c98824\c90980\c90196;\cssrgb\c0\c0\c0;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat6\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat7\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat8\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat3\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid25}
{\list\listtemplateid26\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat4\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid26}
{\list\listtemplateid27\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid27}
{\list\listtemplateid28\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid28}
{\list\listtemplateid29\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid29}
{\list\listtemplateid30\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid2901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid30}
{\list\listtemplateid31\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid3001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid31}
{\list\listtemplateid32\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat2\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid3101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid32}
{\list\listtemplateid33\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid3201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid33}
{\list\listtemplateid34\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat6\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid3301\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid34}
{\list\listtemplateid35\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid3401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid35}
{\list\listtemplateid36\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{circle\}}{\leveltext\leveltemplateid3502\'01\uc0\u9702 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid36}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}{\listoverride\listid26\listoverridecount0\ls26}{\listoverride\listid27\listoverridecount0\ls27}{\listoverride\listid28\listoverridecount0\ls28}{\listoverride\listid29\listoverridecount0\ls29}{\listoverride\listid30\listoverridecount0\ls30}{\listoverride\listid31\listoverridecount0\ls31}{\listoverride\listid32\listoverridecount0\ls32}{\listoverride\listid33\listoverridecount0\ls33}{\listoverride\listid34\listoverridecount0\ls34}{\listoverride\listid35\listoverridecount0\ls35}{\listoverride\listid36\listoverridecount0\ls36}}
\paperw11900\paperh16840\margl1440\margr1440\vieww33100\viewh16740\viewkind0
\deftab720
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Overview\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 In this lab, you will learn how to:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Build a batch Extract-Transform-Load pipeline in Apache Beam, which takes raw data from Google Cloud Storage and writes it to Google BigQuery.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Run the Apache Beam pipeline on Cloud Dataflow.\cb1 \
\ls1\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parameterise the execution of the pipeline.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\sa480\partightenfactor0

\f0\b \cf2 \cb3 Prerequisites:
\f1\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Basic familiarity with Python.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Check project permissions\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Before you begin your work on Google Cloud, you need to ensure that your project has the correct permissions within Identity and Access Management (IAM).\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls3\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud console, on the\'a0
\f0\b Navigation menu
\f1\b0 \'a0(\cb1 {{\NeXTGraphic tkgw1TDgj4Q+YKQUW4jUFd0O5OEKlUMBRYbhlCrF0WY=.png \width300 \height260 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\cb3 ), select\'a0
\f0\b IAM & Admin
\f1\b0 \'a0>\'a0
\f0\b IAM
\f1\b0 .\cb1 \
\ls3\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Confirm that the default compute Service Account\'a0
\f3\fs30 \cb4 \{project-number\}-compute@developer.gserviceaccount.com
\f1\fs32 \cb3 \'a0is present and has the\'a0
\f3\fs30 \cb4 editor
\f1\fs32 \cb3 \'a0role assigned. The account prefix is the project number, which you can find on\'a0
\f0\b Navigation menu > Cloud Overview > Dashboard
\f1\b0 .\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 Note:\'a0
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 If the account is not present in IAM or does not have the\'a0
\f3\fs30 \AppleTypeServices \cb4 editor
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cb6 \'a0role, follow the steps below to assign the required role.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls4\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud console, on the\'a0
\f0\b Navigation menu
\f1\b0 , click\'a0
\f0\b Cloud Overview > Dashboard
\f1\b0 .\cb1 \
\ls4\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Copy the project number (e.g.\'a0
\f3\fs30 \cb4 729328892908
\f1\fs32 \cb3 ).\cb1 \
\ls4\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the\'a0
\f0\b Navigation menu
\f1\b0 , select\'a0
\f0\b IAM & Admin
\f1\b0 \'a0>\'a0
\f0\b IAM
\f1\b0 .\cb1 \
\ls4\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 At the top of the roles table, below\'a0
\f0\b View by Principals
\f1\b0 , click\'a0
\f0\b Grant Access
\f1\b0 .\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For\'a0
\f0\b New principals
\f1\b0 , type:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
  \{project-number\}-compute@developer.gserviceaccount.com\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Replace\'a0
\f3\fs30 \cb4 \{project-number\}
\f1\fs32 \cb3 \'a0with your project number.\cb1 \
\ls5\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For\'a0
\f0\b Role
\f1\b0 , select\'a0
\f0\b Project
\f1\b0 \'a0(or Basic) >\'a0
\f0\b Editor
\f1\b0 .\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b Save
\f1\b0 .\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Jupyter notebook-based development environment setup\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 For this lab, you will be running all commands in a terminal from your notebook.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls6\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Google Cloud Console, on the\'a0
\f0\b Navigation Menu
\f1\b0 , click\'a0
\f0\b Vertex AI > Workbench
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Enable\'a0
\f0\b Notebooks API
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the Workbench page, click\'a0
\f0\b CREATE NEW
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the\'a0
\f0\b New instance
\f1\b0 \'a0dialog box that appears, set the region to\'a0
\f6\b \cb7 region
\f1\b0 \cb3 \'a0and zone to\'a0
\f6\b \cb7 zone
\f1\b0 \cb3 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For Environment, select\'a0
\f0\b Apache Beam
\f1\b0 .\cb1 \
\ls6\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b CREATE
\f1\b0 \'a0at the bottom of the dialog vox.\cb1 \
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb8 \strokec5 Note:\'a0
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 The environment may take 3 - 5 minutes to be fully provisioned. Please wait until the step is complete.\

\f4\b \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \
Note:\'a0
\f1\b0 \AppleTypeServices \cf5 \cb6 \strokec5 Click
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \'a0
\f4\b \AppleTypeServices\AppleTypeServicesF65539 Enable Notebook API
\f5\b0 \AppleTypeServices\AppleTypeServicesF65539 \'a0
\f1 \AppleTypeServices \cf5 \cb6 \strokec5 to enable the notebook api.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once the environment is ready, click the\'a0
\f0\b OPEN JUPYTERLAB
\f1\b0 \'a0link next to your Notebook name. This will open up your environment in a new tab in your browser.\cb1 \
\pard\pardeftab720\partightenfactor0

\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \
\pard\pardeftab720\partightenfactor0

\f1 \AppleTypeServices \cf0 \cb1 \strokec9 {{\NeXTGraphic Qw=.png \width18920 \height3380 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, click\'a0
\f0\b Terminal
\f1\b0 . This will open up a terminal where you can run all the commands in this lab.\cb1 \
\pard\pardeftab720\partightenfactor0

\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \AppleTypeServices \cf2 \cb3 \strokec2 Download Code Repository\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 Next you will download a code repository for use in this lab.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal you just opened, enter the following:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0

\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 git clone https://github.com/GoogleCloudPlatform/training-data-analyst\
cd /home/jupyter/training-data-analyst/quests/dataflow_python/\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0

\f1\fs32 \AppleTypeServices \cf2 \cb1 \strokec2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls10\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 On the left panel of your notebook environment, in the file browser, you will notice the\'a0
\f0\b training-data-analyst
\f1\b0 \'a0repo added.\cb1 \
\ls10\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Navigate into the cloned repo\'a0
\f3\fs30 \cb4 /training-data-analyst/quests/dataflow_python/
\f1\fs32 \cb3 . You will see a folder for each lab, which is further divided into a\'a0
\f3\fs30 \cb4 lab
\f1\fs32 \cb3 \'a0sub-folder with code to be completed by you, and a\'a0
\f3\fs30 \cb4 solution
\f1\fs32 \cb3 \'a0sub-folder with a fully workable example to reference if you get stuck.\cb1 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \strokec9 {{\NeXTGraphic CILaUn6YJSJjfGKlB6ju13WPLsGaZj6tEqxpw0=.png \width7440 \height6540 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\pardeftab720\partightenfactor0
\cf0 \strokec9 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Apache Beam and Cloud Dataflow\
\pard\pardeftab720\sa480\partightenfactor0

\f7\i\fs32 \cf2 \cb3 \strokec2 About 5 minutes
\f1\i0 \cf2 \cb3 \strokec2 \
Cloud Dataflow is a fully-managed Google Cloud Platform service for running batch and streaming Apache Beam data processing pipelines.\
Apache Beam is an open source, advanced, unified, and portable data processing programming model that allows end users to define both batch and streaming data-parallel processing pipelines using Java, Python, or Go. Apache Beam pipelines can be executed on your local development machine on small datasets, and at scale on Cloud Dataflow. However, because Apache Beam is open source, other runners exist \'97 you can run Beam pipelines on Apache Flink and Apache Spark, among others.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {{\NeXTGraphic Pasted Graphic.png \width8100 \height9680 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}
\f1\fs32 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Lab part 1. Writing an ETL pipeline from scratch\
\pard\pardeftab720\sa640\partightenfactor0

\f2\b0\fs48 \cf2 \cb3 \strokec2 Introduction\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 In this section, you write an Apache Beam Extract-Transform-Load (ETL) pipeline from scratch.\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Dataset and use case review\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \cf2 \cb3 \strokec2 For each lab in this quest, the input data is intended to resemble web server logs in\'a0{\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Common_Log_Format"}}{\fldrslt \cf2 \cb3 \strokec2 Common Log format}}\'a0along with other data that a web server might contain. For this first lab, the data is treated as a batch source; in later labs, the data will be treated as a streaming source. Your task is to read the data, parse it, and then write it to BigQuery, a serverless data warehouse, for later data analysis.\
\pard\pardeftab720\sa360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 Open the appropriate lab\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0
\f1\b0 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the terminal in your IDE, and copy and paste the following command:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf5 \strokec9 cd 1_Basic_ETL/lab\
export BASE_DIR=$(pwd)\
\cf0 \
\pard\pardeftab720\sa360\partightenfactor0

\f0\b\fs32 \cf2 \cb3 \strokec2 Set up the virtual environment and dependencies\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0 \cf2 \cb3 \strokec2 Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the terminal, create a virtual environment for your work in this lab:\cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \strokec9 \
sudo apt-get update && sudo apt-get install -y python3-venv\
\
python3 -m venv df-env\
\
source df-env/bin/activate\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Next, install the packages you will need to execute your pipeline:\cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \strokec9 \
python3 -m pip install -q --upgrade pip setuptools wheel\
python3 -m pip install apache-beam[gcp]\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls14\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Finally, ensure that the Dataflow API is enabled:\cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \strokec9 \
gcloud services enable dataflow.googleapis.com\
\
\
\pard\pardeftab720\sa640\partightenfactor0

\f2\fs48 \cf2 \cb3 \strokec2 Write your first pipeline\
\pard\pardeftab720\sa480\partightenfactor0

\f7\i\fs32 \cf2 \cb3 \strokec2 1 hour
\f1\i0 \cf2 \cb3 \strokec2 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 1. Generate synthetic data\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls15\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Run the following command in the terminal to clone a repository containing scripts for generating synthetic web server logs:\cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \strokec9 \
cd $BASE_DIR/../..\
\
source create_batch_sinks.sh\
\
bash generate_batch_events.sh\
\
head events.json\
\
\pard\pardeftab720\partightenfactor0

\fs32 \cf2 \cb3 \strokec2 The script creates a file called\'a0
\f3\fs30 \cb4 events.json
\f1\fs32 \cb3 \'a0containing lines resembling the following:
\fs24 \cf0 \cb1 \strokec9 \
\
\{"user_id": "-6434255326544341291", "ip": "192.175.49.116", "timestamp": "2019-06-19T16:06:45.118306Z", "http_request": "\\"GET eucharya.html HTTP/1.0\\"", "lat": 37.751, "lng": -97.822, "http_response": 200, "user_agent": "Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)", "num_bytes": 182\}\
\
\pard\pardeftab720\sa480\partightenfactor0

\fs32 \cf2 \cb3 \strokec2 It then automatically copies this file to your Google Cloud Storage bucket at\'a0
\f6\b \cf2 \cb7 \strokec2 Cloud Storage input path
\f1\b0 \cf2 \cb3 \strokec2 .\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In another browser tab, navigate to\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/storage"}}{\fldrslt Google Cloud Storage}}\'a0and confirm that your storage bucket contains a file called\'a0
\f3\fs30 \cb4 events.json
\f1\fs32 \cb3 .\cb1 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \strokec9 \
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 2. Read data from your source\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 If you get stuck in this or later sections, you can refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/1_Basic_ETL/solution/my_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 solution}}.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In your file explorer, navigate to the lab folder\'a0
\f3\fs30 \cb4 1_Basic_ETL/lab
\f1\fs32 \cb3 \'a0and click\'a0
\f0\b my_pipeline.py
\f1\b0 . This will open the file in an editor panel. Make sure the following packages are imported:\cb1 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \strokec9 \
import argparse\
import time\
import logging\
import json\
import apache_beam as beam\
from apache_beam.options.pipeline_options import GoogleCloudOptions\
from apache_beam.options.pipeline_options import PipelineOptions\
from apache_beam.options.pipeline_options import StandardOptions\
from apache_beam.runners import DataflowRunner, DirectRunner\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls18\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Scroll down to the\'a0
\f3\fs30 \cb4 run()
\f1\fs32 \cb3 \'a0method. This method currently contains a pipeline that doesn\'92t do anything; note how a\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.pipeline.html"}}{\fldrslt Pipeline}}\'a0object is created using a\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.options.pipeline_options.html"}}{\fldrslt PipelineOptions}}\'a0object and the final line of the method runs the pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \strokec9 \
options = PipelineOptions()\
# Set options\
p = beam.Pipeline(options=options)\
# Do stuff\
p.run()\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls19\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 All data in Apache Beam pipelines reside in\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.pvalue.html"}}{\fldrslt PCollections}}. To create your pipeline\'92s initial\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 , you will need to apply a root transform to your pipeline object. A root transform creates a\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0from either an external data source or some local data you specify.\cb1 \
\ls19\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 There are two kinds of root transforms in the Beam SDKs:\'a0
\f0\b Read
\f1\b0 \'a0and\'a0
\f0\b Create
\f1\b0 .\'a0
\f0\b Read
\f1\b0 \'a0transforms read data from an external source, such as a text file or a database table.\'a0
\f0\b Create
\f1\b0 \'a0transforms create a\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0from an in-memory\'a0
\f3\fs30 \cb4 list
\f1\fs32 \cb3 \'a0and are especially useful for testing.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 The following example code shows how to apply a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 ReadFromText
\f1\fs32 \cf2 \cb3 \strokec2 \'a0root transform to read data from a text file. The transform is applied to a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Pipeline
\f1\fs32 \cf2 \cb3 \strokec2 \'a0object,\'a0
\f3\fs30 \cf2 \cb4 \strokec2 p
\f1\fs32 \cf2 \cb3 \strokec2 , and returns a pipeline dataset in the form of a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection[str]
\f1\fs32 \cf2 \cb3 \strokec2 \'a0(using notation coming from\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/sdks/python-type-safety/#parameterized-type-hints"}}{\fldrslt \cf2 \cb3 \strokec2 parameterized type hints}}).\'a0
\f7\i \cf2 \cb3 \strokec2 "ReadLines"
\f1\i0 \cf2 \cb3 \strokec2 \'a0is your name for the transform, which will be helpful later when working with larger pipelines.\
\pard\pardeftab720\partightenfactor0
\cf5 \cb1 \strokec5 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf5 \strokec9 lines = p | "ReadLines" >> beam.io.ReadFromText("gs://path/to/input.txt")\cf0 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls20\ilvl0
\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Inside the\'a0
\f3\fs30 \cb4 run()
\f1\fs32 \cb3 \'a0method, create a string constant called\'a0
\f7\i \'93input\'94
\f1\i0 \'a0and set its value to\'a0
\f3\fs30 \cb4 gs://<YOUR-PROJECT-ID>/events.json
\f1\fs32 \cb3 . In a future lab, you will use command-line parameters to pass this information.\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Create a\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0of strings of all the events in\'a0
\f3\fs30 \cb4 events.json
\f1\fs32 \cb3 \'a0by calling the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.io.textio.html#apache_beam.io.textio.ReadFromText"}}{\fldrslt 
\f3\fs30 \cb4 textio.ReadFromText}}\'a0transform.\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add any appropriate import statements to the top of\'a0
\f3\fs30 \cb4 my_pipeline.py
\f1\fs32 \cb3 .\cb1 \
\ls20\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To save your work, click on\'a0
\f0\b File
\f1\b0 \'a0and select\'a0
\f0\b Save
\f1\b0 \'a0in the top navigation menu.\cb1 \
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 3. Run your pipeline to verify that it works\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls21\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the terminal, and return to the\'a0
\f3\fs30 \cb4 $BASE_DIR
\f1\fs32 \cb3 \'a0folder and execute the following commands. Be sure to set the\'a0
\f3\fs30 \cb4 PROJECT_ID
\f1\fs32 \cb3 \'a0environment variable before running the pipeline:\cb1 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \strokec9 \
\
cd $BASE_DIR\
\
# Set up environment variables\
export PROJECT_ID=$(gcloud config get-value project)\
\
# Run the pipeline\
python3 my_pipeline.py \\\
  --project=$\{PROJECT_ID\} \\\
  --region=Region \\\
  --stagingLocation=gs://$PROJECT_ID/staging/ \\\
  --tempLocation=gs://$PROJECT_ID/temp/ \\\
  --runner=DirectRunner\
\
\pard\pardeftab720\partightenfactor0

\fs32 \cf2 \strokec2 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 At the moment, your pipeline doesn\'92t actually do anything; it simply reads in data.\
However, running it demonstrates a useful workflow, in which you verify the pipeline locally and cheaply using\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/runners/direct/"}}{\fldrslt \cf2 \cb3 \strokec2 DirectRunner}}\'a0running on your local machine before doing more expensive computations. To run the pipeline using Google Cloud Dataflow, you may change\'a0
\f3\fs30 \cf2 \cb4 \strokec2 runner
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/documentation/runners/dataflow/"}}{\fldrslt \cf2 \cb3 \strokec2 DataflowRunner}}.\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 4. Add in a transformation\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 If you get stuck, refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/1_Basic_ETL/solution/my_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 solution}}.\
Transforms are what change your data. In Apache Beam, transforms are done by the\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.ptransform.html#apache_beam.transforms.ptransform.PTransform"}}{\fldrslt \cf2 \cb3 \strokec2 PTransform}}\'a0class. At runtime, these operations will be performed on a number of independent workers.\
The input and output to every\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PTransform
\f1\fs32 \cf2 \cb3 \strokec2 \'a0is a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 . In fact, though you may not have realized it, you have already used a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PTransform
\f1\fs32 \cf2 \cb3 \strokec2 \'a0when you read in data from Google Cloud Storage. Whether or not you assigned it to a variable, this created a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0of strings.\
Because Beam uses a generic apply method for\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 s, represented by the pipe operator\'a0
\f3\fs30 \cf2 \cb4 \strokec2 |
\f1\fs32 \cf2 \cb3 \strokec2 \'a0in Python, you can chain transforms sequentially. For example, you can chain transforms to create a sequential pipeline, like this one:\
[Output_PCollection] = ([Input_PCollection] | [First Transform]\
                                            | [Second Transform]\
                                            | [Third Transform])\
\
For this task, you will use a new sort of transform,\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.ParDo"}}{\fldrslt \cf2 \cb3 \strokec2 a ParDo}}.\'a0
\f3\fs30 \cf2 \cb4 \strokec2 ParDo
\f1\fs32 \cf2 \cb3 \strokec2 \'a0is a Beam transform for generic parallel processing.\
The\'a0
\f3\fs30 \cf2 \cb4 \strokec2 ParDo
\f1\fs32 \cf2 \cb3 \strokec2 \'a0processing paradigm is similar to the \'93Map\'94 phase of a Map/Shuffle/Reduce-style algorithm: a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 ParDo
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform considers each element in the input\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 , performs some processing function (your user code) on that element, and emits zero, one, or multiple elements to an output\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 .\
\pard\pardeftab720\sa480\partightenfactor0

\f3\fs30 \cf2 \cb4 \strokec2 ParDo
\f1\fs32 \cf2 \cb3 \strokec2 \'a0is useful for a variety of common data processing operations, however there are special\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PTransform
\f1\fs32 \cf2 \cb3 \strokec2 s in Python to make the process simpler, including:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls22\ilvl0
\f0\b \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Filtering a dataset.
\f1\b0 \'a0You can use\'a0
\f3\fs30 \cb4 Filter
\f1\fs32 \cb3 \'a0to consider each element in a\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0and either output that element to a new\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 , or discard it depending on the output of a Python callable which returns a boolean value.\cb1 \
\ls22\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Formatting or type-converting each element in a dataset.
\f1\b0 \'a0If your input\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0contains elements that are of a different type or format than you want, you can use\'a0
\f3\fs30 \cb4 Map
\f1\fs32 \cb3 \'a0to perform a conversion on each element and output the result to a new\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 .\cb1 \
\ls22\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Extracting parts of each element in a dataset.
\f1\b0 \'a0If you have a\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0of records with multiple fields, for example, you can also use\'a0
\f3\fs30 \cb4 Map
\f1\fs32 \cb3 \'a0or\'a0
\f3\fs30 \cb4 FlatMap
\f1\fs32 \cb3 \'a0to parse out just the fields you want to consider into a new\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 .\cb1 \
\ls22\ilvl0
\f0\b \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Performing computations on each element in a dataset.
\f1\b0 \'a0You can use\'a0
\f3\fs30 \cb4 ParDo
\f1\fs32 \cb3 ,\'a0
\f3\fs30 \cb4 Map
\f1\fs32 \cb3 , or\'a0
\f3\fs30 \cb4 FlatMap
\f1\fs32 \cb3 \'a0to perform simple or complex computations on every element, or certain elements, of a\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 \'a0and output the results as a new\'a0
\f3\fs30 \cb4 PCollection
\f1\fs32 \cb3 .\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 To complete this task, you need to write a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Map
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform that reads in a JSON string representing a single event, parses it using the Python\'a0{\field{\*\fldinst{HYPERLINK "https://docs.python.org/3/library/json.html"}}{\fldrslt 
\f3\fs30 \cf2 \cb4 \strokec2 json
\f1\fs32 \cb3 \'a0package}}, and outputs the dictionary returned by\'a0
\f3\fs30 \cf2 \cb4 \strokec2 json.loads
\f1\fs32 \cf2 \cb3 \strokec2 .\
\pard\pardeftab720\sa480\partightenfactor0

\f3\fs30 \cf2 \cb4 \strokec2 Map
\f1\fs32 \cf2 \cb3 \strokec2 \'a0functions can be implemented in two ways, either\'a0
\f7\i \cf2 \cb3 \strokec2 inline
\f1\i0 \cf2 \cb3 \strokec2 \'a0or via a\'a0
\f7\i \cf2 \cb3 \strokec2 predefined callable
\f1\i0 \cf2 \cb3 \strokec2 . You write inline\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Map
\f1\fs32 \cf2 \cb3 \strokec2 \'a0functions like this:\
p | beam.Map(lambda x : something(x))\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 Alternatively,\'a0
\f3\fs30 \cb4 beam.Map
\f1\fs32 \cb3 \'a0can be used with a Python callable defined earlier in the script:\
\
def something(x):\
  y = # Do something!\
  return y\
\
p | beam.Map(something)\
\
\
If you need more flexibility, than\'a0
\f3\fs30 \cb4 beam.Map
\f1\fs32 \cb3 \'a0(and other lightweight\'a0
\f3\fs30 \cb4 DoFn
\f1\fs32 \cb3 s) offers, then you can implement\'a0
\f3\fs30 \cb4 ParDo
\f1\fs32 \cb3 \'a0with custom\'a0
\f3\fs30 \cb4 DoFn
\f1\fs32 \cb3 s that subclass\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.DoFn"}}{\fldrslt \cf0 \cb3 \strokec9 DoFn}}. This allows them to be more easily integrated with testing frameworks.\
\
class MyDoFn(beam.DoFn):\
  def process(self, element):\
    output = #Do Something!\
    yield output\
\
p | beam.ParDo(MyDoFn())\
\
Remember, if you get stuck, refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/1_Basic_ETL/solution/my_pipeline.py"}}{\fldrslt \cf0 \cb3 \strokec9 solution}}.\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 \cb3 \strokec2 Task 5. Write to a sink\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 At this point, your pipeline reads a file from Google Cloud Storage, parses each line, and emits a Python dictionary for each element. The next step is to write these objects into a BigQuery table.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls23\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 While you can instruct your pipeline to create a BigQuery table if needed, you will need to create the dataset ahead of time. This has already been done by the\'a0
\f3\fs30 \cb4 generate_batch_events.sh
\f1\fs32 \cb3 \'a0script. You can examine the dataset using the following code:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
# Examine dataset\
bq ls\
\
# No tables yet\
bq ls logs\
\
\
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 To output your pipeline\'92s final\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 s, you apply a\'a0
\f7\i \cf2 \cb3 \strokec2 Write
\f1\i0 \cf2 \cb3 \strokec2 \'a0transform to that\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 .\'a0
\f7\i \cf2 \cb3 \strokec2 Write
\f1\i0 \cf2 \cb3 \strokec2 \'a0transforms can output the elements of a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0to an external data sink, such as a database table. You can use\'a0
\f7\i \cf2 \cb3 \strokec2 Write
\f1\i0 \cf2 \cb3 \strokec2 \'a0to output a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0at any time in your pipeline, although you\'92ll typically write out data at the end of your pipeline.\
The following example code shows how to apply a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 WriteToText
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform to write a\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PCollection
\f1\fs32 \cf2 \cb3 \strokec2 \'a0of string to a text file:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 p | "WriteMyFile" >> beam.io.WriteToText("gs://path/to/output")\cb1 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls24\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In this case, instead of using\'a0
\f3\fs30 \cb4 WriteToText
\f1\fs32 \cb3 , use\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/pydoc/2.9.0/apache_beam.io.gcp.bigquery.html"}}{\fldrslt 
\f3\fs30 \cb4 WriteToBigQuery}}.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 This function requires a number of things to be specified, including the specific table to write to and the schema of this table. You can optionally specify whether to append to an existing table, recreate existing tables (helpful in early pipeline iteration), or create the table if it doesn't exist. By default, this transform\'a0
\f7\i \cf2 \cb3 \strokec2 will
\f1\i0 \cf2 \cb3 \strokec2 \'a0create tables that don't exist and\'a0
\f7\i \cf2 \cb3 \strokec2 won't
\f1\i0 \cf2 \cb3 \strokec2 \'a0write to a non-empty table.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls25\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 However, we do need to specify our schema. There are two ways to do this. We can specify the schema as a single string or in JSON format. For example, suppose our dictionary has three fields: name (of type\'a0
\f3\fs30 \cb4 str
\f1\fs32 \cb3 ), ID (of type\'a0
\f3\fs30 \cb4 int
\f1\fs32 \cb3 ) and balance (of type\'a0
\f3\fs30 \cb4 float
\f1\fs32 \cb3 ). Then we can specify the schema in a single line:\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\cf2 \strokec2 \
table_schema = 'name:STRING,id:INTEGER,balance:FLOAT'\
\
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \strokec2 Or specify it as JSON:\
\
table_schema = \{\
        "fields": [\
            \{\
                "name": "name",\
                "type": "STRING"\
            \},\
            \{\
                "name": "id",\
                "type": "INTEGER",\
                "mode": "REQUIRED"\
            \},\
            \{\
                "name": "balance",\
                "type": "FLOAT",\
                "mode": "REQUIRED"\
            \}\
        ]\
    \}\
\
\
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 In the first case (the single string), all fields are assumed to be\'a0
\f3\fs30 \cf2 \cb4 \strokec2 NULLABLE
\f1\fs32 \cf2 \cb3 \strokec2 . We can specify the mode if we use the JSON approach instead.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls26\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once we have defined the table schema, then we can add the sink to our DAG:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec2 \
\
p | 'WriteToBQ' >> beam.io.WriteToBigQuery(\
            'project:dataset.table',\
            schema=table_schema,\
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\
            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\
            )\
\
\
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb8 \strokec5 Note:\'a0
\f3\b0\fs30 \AppleTypeServices \cf5 \cb4 \strokec5 WRITE_TRUNCATE
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb8 \strokec5 \'a0will delete and recreate your table each and every time. This is helpful in early pipeline iteration, especially as you are iterating on your schema, but can easily cause unintended issues in production.\'a0
\f3\fs30 \AppleTypeServices \cf5 \cb4 \strokec5 WRITE_APPEND
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb8 \strokec5 \'a0or\'a0
\f3\fs30 \AppleTypeServices \cf5 \cb4 \strokec5 WRITE_EMPTY
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb8 \strokec5 \'a0are safer.\
\
\
\pard\pardeftab720\sa480\partightenfactor0

\f1\fs32 \AppleTypeServices \cf2 \cb3 \strokec2 Remember to define the table schema\'a0
\f0\b and
\f1\b0 \'a0add the BigQuery sink to your pipeline. Remember, if you get stuck, refer to the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/1_Basic_ETL/solution/my_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 solution}}.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 6. Run your pipeline\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls27\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the terminal, and run your pipeline using almost the same command as earlier. However, now use the\'a0
\f3\fs30 \cb4 DataflowRunner
\f1\fs32 \cb3 \'a0to run the pipeline on Cloud Dataflow.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 \
\
# Set up environment variables\
cd $BASE_DIR\
export PROJECT_ID=$(gcloud config get-value project)\
\
# Run the pipelines\
python3 my_pipeline.py \\\
  --project=$\{PROJECT_ID\} \\\
  --region=Region \\\
  --stagingLocation=gs://$PROJECT_ID/staging/ \\\
  --tempLocation=gs://$PROJECT_ID/temp/ \\\
  --runner=DataflowRunner\
The overall shape should be a single path, starting with the\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Read
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform and ending with the\'a0
\f3\fs30 \cf2 \cb4 \strokec2 Write
\f1\fs32 \cf2 \cb3 \strokec2 \'a0transform. As your pipeline runs, workers will be added automatically, as the service determines the needs of your pipeline, and then disappear when they are no longer needed. You can observe this by navigating to\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/compute"}}{\fldrslt \cf2 \cb3 \strokec2 Compute Engine}}, where you should see virtual machines created by the Dataflow service.\
\pard\pardeftab720\partightenfactor0

\f4\b\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 Note:\'a0
\f1\b0 \AppleTypeServices \cf5 \cb6 \strokec5 If your pipeline is building successfully, but you're seeing a lot of errors due to code or misconfiguration in the Dataflow service, you can set
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \'a0
\f3\fs30 \AppleTypeServices \cb4 runner
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cb6 \'a0
\f1 \AppleTypeServices \cf5 \cb6 \strokec5 back to
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \'a0
\f3\fs30 \AppleTypeServices \cb4 DirectRunner
\f5\fs24 \AppleTypeServices\AppleTypeServicesF65539 \cb6 \'a0
\f1 \AppleTypeServices \cf5 \cb6 \strokec5 to run it locally and receive faster feedback. This approach works in this case because the dataset is small and you are not using any features that aren't supported by
\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \'a0
\f3\fs30 \AppleTypeServices \cb4 DirectRunner
\f1\fs24 \cf5 \cb6 \strokec5 .\

\f5 \AppleTypeServices\AppleTypeServicesF65539 \cf5 \cb6 \strokec5 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls28\ilvl0
\f1\fs32 \AppleTypeServices \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once your pipeline has finished, return to the BigQuery browser window and query your table.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 If your code isn\'92t performing as expected and you don\'92t know what to do, check out the\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/1_Basic_ETL/solution/my_pipeline.py"}}{\fldrslt \cf2 \cb3 \strokec2 solution}}.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Lab part 2. Parameterizing basic ETL\
\pard\pardeftab720\sa480\partightenfactor0

\f7\i\b0\fs32 \cf2 \cb3 \strokec2 Approximately 20 minutes
\f1\i0 \cf2 \cb3 \strokec2 \
Much of the work of data engineers is either predictable, like recurring jobs, or it\'92s similar to other work. However, the process for running pipelines requires engineering expertise. Think back to the steps that you just completed:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls29\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 You created a development environment and developed a pipeline. The environment included the Apache Beam SDK and other dependencies.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls29\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 You executed the pipeline from the development environment. The Apache Beam SDK staged files in Cloud Storage, created a job request file, and submitted the file to the Cloud Dataflow service.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 It would be much better if there were a way to initiate a job through an API call or without having to set up a development environment (which non-technical users would be unable to do). This would also allow you to run pipelines.\
Dataflow Templates seek to solve this problem by changing the representation that is created when a pipeline is compiled so that it is parameterizable. Unfortunately, it is not as simple as exposing command-line parameters, although that is something you do in a later lab. With Dataflow Templates, the workflow above becomes:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls30\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Developers create a development environment and develop their pipeline. The environment includes the Apache Beam SDK and other dependencies.\cb1 \
\ls30\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Developers execute the pipeline and create a template. The Apache Beam SDK stages files in Cloud Storage, creates a template file (similar to job request), and saves the template file in Cloud Storage.\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls30\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Non-developer users or other workflow tools like Airflow can easily execute jobs with the Google Cloud Console, gcloud command-line tool, or the REST API to submit template file execution requests to the Cloud Dataflow service.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 In this lab, you will practice using one of the many\'a0{\field{\*\fldinst{HYPERLINK "https://cloud.google.com/dataflow/docs/templates/provided-templates"}}{\fldrslt \cf2 \cb3 \strokec2 Google-created Dataflow Templates}}\'a0to accomplish the same task as the pipeline that you built in Part 1.\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 1. Create a JSON schema file\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 Just like before, you must pass the Dataflow Template a JSON file representing the schema in this example.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls31\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Return to the terminal in your IDE. Run the following commands to navigate back to the main directory, then grab the schema from your existing\'a0
\f3\fs30 \cb4 logs.logs
\f1\fs32 \cb3 \'a0table:\cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 cd $BASE_DIR/../..\
bq show --schema --format=prettyjson logs.logs\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls32\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Now, capture this output in a file and upload to GCS. The extra\'a0
\f3\fs30 \cb4 sed
\f1\fs32 \cb3 \'a0commands are to build a full JSON object that Dataflow will expect.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 \
bq show --schema --format=prettyjson logs.logs | sed '1s/^/\{"BigQuery Schema":/' | sed '$s/$/\}/' > schema.json\
cat schema.json\
export PROJECT_ID=$(gcloud config get-value project)\
gsutil cp schema.json gs://$\{PROJECT_ID\}/\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 2. Write a JavaScript user-defined function\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 The Cloud Storage to BigQuery Dataflow Template requires a JavaScript function to convert the raw text into valid JSON. In this case, each line of text is valid JSON, so the function is somewhat trivial.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls33\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To complete this task, create a\'a0
\f0\b New File
\f1\b0 \'a0in the\'a0
\f0\b dataflow_python
\f1\b0 \'a0folder in the file explorer of your IDE.\cb1 \
\ls33\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 To create\'a0
\f0\b New File
\f1\b0 , click on\'a0
\f0\b File >> New >> Text File
\f1\b0 .\cb1 \
\ls33\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Rename the file name as\'a0
\f0\b transform.js
\f1\b0 , to rename the file name right click on it.\cb1 \
\ls33\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Open\'a0
\f0\b transform.js file
\f1\b0 \'a0in the editor panel, click on the file to open it.\cb1 \
\ls33\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Copy the function below to the transform.js file and save it:\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 function transform(line) \{\
  return line;\
\}\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls34\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Then run the following to copy the file to Google Cloud Storage:\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 \
export PROJECT_ID=$(gcloud config get-value project)\
gsutil cp *.js gs://$\{PROJECT_ID\}/\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 3. Run a Dataflow Template\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa360\partightenfactor0
\ls35\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Go to the\'a0{\field{\*\fldinst{HYPERLINK "https://console.cloud.google.com/dataflow"}}{\fldrslt Cloud Dataflow Web UI}}.\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click\'a0
\f0\b CREATE JOB FROM TEMPLATE
\f1\b0 .\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Enter a Job name for your Cloud Dataflow job.\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b Dataflow template
\f1\b0 , select the\'a0
\f0\b Text Files on Cloud Storage to BigQuery
\f1\b0 \'a0template under the\'a0
\f0\b Process Data in Bulk (batch)
\f1\b0 \'a0section, NOT the Streaming section.\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b Cloud Storage Input File
\f1\b0 , enter the path to\'a0
\f3\fs30 \cb4 events.json
\f1\fs32 \cb3 \'a0in the form\'a0
\f3 \cb7 Cloud Storage input path
\f1 \cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b Cloud Storage location of your BigQuery schema file
\f1\b0 , write the path to your\'a0
\f3\fs30 \cb4 schema.json
\f1\fs32 \cb3 \'a0file, in the form\'a0
\f3 \cb7 JSON path
\f1 \cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	7	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b BigQuery output table
\f1\b0 , enter\'a0
\f3 \cb7 BigQuery output table
\f1 \cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	8	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b Temporary BigQuery directory
\f1\b0 , enter a new folder within this same bucket. The job will create it for you.\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	9	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b Temporary location
\f1\b0 , enter a second new folder within this same bucket.\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	10	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Leave\'a0
\f0\b Encryption
\f1\b0 \'a0at\'a0
\f0\b Google-managed encryption key
\f1\b0 .\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	11	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click to open\'a0
\f0\b Optional Prameters
\f1\b0 .\cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	12	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b JavaScript UDF path in Cloud Storage
\f1\b0 , enter in the path to your\'a0
\f3\fs30 \cb4 .js
\f1\fs32 \cb3 , in the form\'a0
\f3 \cb7 JavaScript UDF path
\f1 \cb1 \
\ls35\ilvl0\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	13	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Under\'a0
\f0\b JavaScript UDF name
\f1\b0 , enter\'a0
\f3\fs30 \cb4 transform
\f1\fs32 \cb3 .\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls35\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	14	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Click the\'a0
\f0\b Run job
\f1\b0 \'a0button.\
\pard\tx720\pardeftab720\partightenfactor0
\cf2 \cb1 \strokec2 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 While your job is running, you may inspect it from within the Dataflow Web UI.\
\
\pard\pardeftab720\sa640\partightenfactor0

\f0\b\fs60 \cf2 Task 4. Inspect the Dataflow Template code\
\pard\pardeftab720\sa480\partightenfactor0

\f1\b0\fs32 \cf2 The code for the Dataflow Template you just used is located in this\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/src/main/java/com/google/cloud/teleport/templates/TextIOToBigQuery.java"}}{\fldrslt \cf2 \cb3 \strokec2 TextIOToBigQuery}}\'a0guide.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa480\partightenfactor0
\ls36\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Scroll down to the main method. The code should look familiar to the pipeline you authored!\cb1 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls36\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 It begins with a\'a0
\f3\fs30 \cb4 Pipeline
\f1\fs32 \cb3 \'a0object, created using a\'a0
\f3\fs30 \cb4 PipelineOptions
\f1\fs32 \cb3 \'a0object.\cb1 \
\ls36\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 It consists of a chain of\'a0
\f3\fs30 \cb4 PTransform
\f1\fs32 \cb3 s, beginning with a\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/src/main/java/com/google/cloud/teleport/templates/TextIOToBigQuery.java#L97"}}{\fldrslt TextIO.read()}}\'a0transform.\cb1 \
\ls36\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/src/main/java/com/google/cloud/teleport/templates/TextIOToBigQuery.java#L97"}}{\fldrslt PTransform after the read transform}}\'a0is a bit different; it allows one to use Javascript to transform the input strings if, for example, the source format doesn\'92t align well with the BigQuery table format; for documentation on how to use this feature, see\'a0{\field{\*\fldinst{HYPERLINK "https://cloud.google.com/dataflow/docs/guides/templates/provided-batch#gcstexttobigquery"}}{\fldrslt this}}\'a0page.\cb1 \
\ls36\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/src/main/java/com/google/cloud/teleport/templates/TextIOToBigQuery.java#L103"}}{\fldrslt PTransform after the Javascript UDF}}\'a0uses a library function to convert the Json into a tablerow; you can inspect that code\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/src/main/java/com/google/cloud/teleport/templates/common/BigQueryConverters.java#L324"}}{\fldrslt here}}.\cb1 \
\ls36\ilvl1\cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u9702 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/v1/src/main/java/com/google/cloud/teleport/templates/TextIOToBigQuery.java#L106"}}{\fldrslt write PTransform}}\'a0looks a bit different because instead of making use of a schema that is known at graph compile-time, the code is intended to accept parameters that will only be known at run-time. The\'a0{\field{\*\fldinst{HYPERLINK "https://beam.apache.org/releases/javadoc/2.22.0/org/apache/beam/sdk/options/ValueProvider.html"}}{\fldrslt NestedValueProvider}}\'a0class is what makes this possible.\cb1 \
\pard\pardeftab720\sa480\partightenfactor0
\cf2 \cb3 \strokec2 Make sure to check out the next lab, which will cover making pipelines that are not simply chains of\'a0
\f3\fs30 \cf2 \cb4 \strokec2 PTransform
\f1\fs32 \cf2 \cb3 \strokec2 s, and how you can adapt a pipeline you\'92ve built to be a custom Dataflow Template.\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf2 \cb1 \
}