Y = f(X) + e  (equation 2.1)

What does the above equation represent? the relationship between the response y and the predictor(s). Where
X = (X₁, X₂, X₃...Xₚ) represents different predictors that we assume have a relationship with Y. Y could be 
sales and X could represent advertising budgets of different mediums.

Predictors are also known as independent variables or features.
Response is also known as the dependent variable.

f is some fixed but unknown function of X₁...Xₚ and e is a random error term which is independent of X and 
has mean zero (what does this mean? read description in figure22.png to understand)

It is all about finding the function that connects the predictors such that a response can be
determined. This function is usually estimated. Once it is estimated the vertical lines from the 
observed values to the estimated function represent the error associated with each observation.
If an observation lies above the curve, the error is positive; if it lies below, the error is 
negative. Overall the errors have a mean of zero.

In general a function f may involve more than one input variable see figure23.png Here, income
is plotted as a function of years of education and seniority. f is 2d surface that must be estimated
based on the observed data.

In essence statistical learning refers to a set of approaches for estimating f.

Why estimate f?

For two main reasons: prediction and inference.

Prediction: in many situations a set of inputs X are readily available, but the output Y cannot
be easily obtained. Since the error term averages to zero, we can predict Y using:

ŷ = fˆ(X)   2.2

where f hat represents the estimate of f and y hat represents the resulting prediction for y.
In this setting, f hat is often treated as a black box in the sense that one is not typically
concerned with the exact form of f hat provided that it yields accurate predictions of y.

The accuracy of ŷ as a prediction for y depends on two quantities; the reducible error and
the irreducible error. In general, fˆ will not be a perfect estimate for f and this inaccuracy 
will introduce some error. This error is reducible because we can potentially improve the 
accuracy of fˆ by using the most appropriate statistical learning technique to estimate f.

Even if it were possible to form a perfect estimate of f, so that our estimated response took
the form of ŷ = f(x), our prediction would still have some error in it. Why? because Y is also
a function of e which by definition cannot be predicted using x (i don't understand this last
bit??).

Variability associated with e also affects the accuracy of our predictions. This is known as
the irreducible error, because no matter how well we estimate f, we cannot reduce the error 
introduced by e.

Why is the irreducible error larger than zero? The quantity e may contain unmeasured variables
that are useful in predicting y. Since they are not measured, f cannot use them for its 
prediction. The quantity e may also contain unmeasured variation.

Consider a given estimate fˆ and a set of predictors X, which yields the prediction 
Ŷ = fˆ(X). Assume for a moment that both fˆ and X are fixed,
so that the only variability comes from e. Then, it is easy to show that

E(Y - Ŷ )² = E[f(X) + e - fˆ(X)]² 
           = [f(X) - fˆ(X)]² + Var(e)     equation 2.3
              reducible        irreducible

where the term on the left represents the average, or expected value, of the squared difference
between the predicted and actual value of Y, and var(e) represents the variance associated 
with the error term e.

The focus of this book is on techniques for estimating f with the aim of minimizing the 
reducible error. It is important to keep in mind that the irreducible error will always 
provide an upper bound on the accuracy of our prediction for Y. This bound is almost always 
unknown in practice.

inference: we are often interested in understanding the relationship between Y and X₁...Xₚ. In
this situation we wish to estimate f, but our goal is not to necessarily make predictions for Y.
Now fˆ cannot be treated as a black box because we need to know its exact form.

Depending on whether my ultimate goal is inference or prediction, or a combination of the two,
different methods for estimating f may be appropriate.

Broadly speaking, most statistical learning methods (linear and non linear) can be characterised as either
parametric or non parametric.

Parametric methods: involve a two step model based approach:
1. first make an assumption about the functional form or shape of f. For example, one very simple assumption 
is that f is linear in X

f(X) = β₀ + β₁X₁ + β₂X₂ + ···+ βₚXₚ.     (2.4)

This is a linear model. Once we have assumed that f is linear, the problem of estimating f is greatly simplifed. 

2. After a model has been selected, we need a procedure that uses training data to fit or train the model. In the 
case of the linear model (2.4), we need to estimate the parameters β₀, β₁... βₚ. That is, we want to find values
of these parameters such that 

Y  ≈ β₀ + β₁X₁ + β₂X₂ + ···+ βₚXₚ.

The most common approach to fitting the model (see figure24.png) is referred to as (ordinary) least squares. 
However, least squares is one of the many possible ways to fit the linear model.

The model based approach described above is referred to as parametric; it reduces the problem of estimating f
down to one of estimating a set of parameters. The potential disadvantage of a parametric approach is that the 
model we choose will usually not match the true unknown form of f.

If the chosen model is too far from the true f, then our estimate will be poor. We can try to address this problem
by choosing flexible models that can fit many different possible functional forms for f. But in general, fitting a
more flexible model requires estimating a greater number of parameters. These more complex models can lead to a 
phenomenon known as overfitting the data which essentially means that they follow the errors or noise too closely.

Figure 2.4 (see figure24.png) shows an example of the parametric approach applied to the income data from figure23.png
we have fit a linear model of the form:

income ≈ β₀ + β₁ × education + β₂ × seniority.

since we have assumed a linear relationship between the response and the two predictors, the entire fitting problem
reduces to estimating β₀, β₁ and β₂ which we do using least squares linear regression.

Comparing Figure 2.3 to Figure 2.4, we can see that the linear fit given in Figure 2.4 is not quite right: the true 
f has some curvature that is not captured in the linear fit. However, the linear fit still appears to do a reasonable 
job of capturing the positive relationship between years of education and income, as well as the slightly less 
positive relationship between seniority and income.

Non parametric methods: 

Non-parametric methods do not make explicit assumptions about the functional form of f. Instead 
they seek an estimate of f that gets as close to the data points as possible without being too 
rough or wiggly. Such approaches can have a major advantage over parametric approaches: by 
avoiding the assumption of a particular functional form for f, they have the potential
to accurately fit a wider range of possible shapes for f.

Any parametric approach brings with it the possibility that the functional form used to
estimate f is very different from the true f, in which case the resulting model will not 
fit the data well. In contrast, non-parametric approaches completely avoid this danger, since 
essentially no assumption about the form of f is made.

But non-parametric approaches do suffer from a major disadvantage: since they do not reduce 
the problem of estimating f to a small number of parameters, a very large number of 
observations (far more than is typically needed for a parametric approach) is required in 
order to obtain an accurate estimate for f.

An example of a non-parametric approach to fitting the Income data is shown in Figure 2.5. A 
thin-plate spline is used to estimate f. This approach does not impose any pre-specified 
model on f. It instead attempts to produce an estimate for f that is as close as possible to 
the observed data, subject to the fit—that is, the yellow surface in Figure 2.5—being smooth. 
In this case, the non-parametric fit has produced a remarkably accurate estimate of the true f 
shown in Figure 2.3. In order to fit a thin-plate spline, the data analyst must select a level 
of smoothness. 

Figure 2.6 shows the same thin-plate spline fit using a lower level of smoothness, allowing
for a rougher fit. The resulting estimate fits the observed data perfectly! However, the 
spline fit shown in Figure 2.6 is far more variable than the true function f, from Figure 
2.3. This is an example of overfitting the data, which we discussed previously. It is an 
undesirable situation because the fit obtained will not yield accurate estimates of the 
response on new observations that were not part of the original training data set.

The Trade-Off Between Prediction Accuracy and Model Interpretability:

Of the many methods that we examine in this book, some are less flexible, or more restrictive, 
in the sense that they can produce just a relatively small range of shapes to estimate f . For 
example, linear regression is a relatively inflexible approach, because it can only generate 
linear functions such as the lines shown in Figure 2.1 or the plane shown in Figure 2.4. Other 
methods, such as the thin plate splines shown in Figures 2.5 and 2.6, are considerably more 
flexible because they can generate a much wider range of possible shapes to estimate f.

One might reasonably ask the following question: why would we ever choose to use a more 
restrictive method instead of a very flexible approach?

There are several reasons that we might prefer a more restrictive model. If we are mainly 
interested in inference, then restrictive models are much more interpretable. For instance, 
when inference is the goal, the linear model may be a good choice since it will be quite easy 
to understand the relationship between Y and X₁, X₂, X₃...Xₚ. In contrast, very flexible
approaches, such as figure25.png and figure26.png, can lead to such complicated estimates of f 
that it is difficult to understand how any individual predictor is associated with the response.

Figure 2.7 provides an illustration of the trade-off between flexibility and interpretability 
for some of the methods that we cover in this book.

Supervised Versus Unsupervised Learning:

Most statistical learning problems fall into one of two categories: supervised or unsupervised. In supervised 
learning, for each observation of the predictor measurement(s) Xᵢ where i = 1,..., n there is an associated 
response yᵢ.

The wish is to fit a model that relates the response to the predictors, with the aim of accurately predicting the 
response for future observations (prediction) or better understanding the relationship between the response and the 
predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression
well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised 
learning domain. 

By contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation 
i = 1,...,n, we observe a vector of measurements xᵢ but no associated response yi.

It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting,
we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable
that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the 
relationships between the variables or between the observations. One statistical learning tool that we may use
regression in this setting is cluster analysis, or clustering. The goal of cluster analysis  is to ascertain, on 
the basis of X₁, X₂, X₃...Xₙ , whether the observations fall into relatively distinct groups.

semi-supervised learning problem arises when for instance we have a set of n observations. For m of the observations,
where m<n, we have both predictor measurements and a response measurement. For the remaining n − m observations, 
we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be 
measured relatively cheaply but the corresponding responses are much more expensive to collect.

In this setting, we wish to use a statistical learning method that can incorporate the m observations for which 
response measurements are available as well as the n − m observations for which they are not.

Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative 
variables take on numerical values.

In contrast, qualitative variables take on values in one of K diferent classes, or categories. Examples of 
qualitative variables include a person’s marital status (married or not), the brand of product purchased 
(brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous 
Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia).

We tend to refer to problems with a quantitative response as regression problems, while those involving a
qualitative response are often referred to as classification problems. However, the distinction is not always that 
crisp. 

Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used
with a qualitative (two-class, or binary) binary response.

We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; 
i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether 
the predictors are qualitative or quantitative is generally considered less important. 

Measuring the Quality of Fit

In order to evaluate the performance of a statistical learning method on a given data set, we need some way to 
measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which 
the predicted response value for a given observation is close to the true response value for that observation. In 
the regression setting, the most commonly-used measure is the mean squared error (MSE), given by equation25.png

where fˆ(xᵢ) is the prediction that f gives for the ith observation. The MSE will be small if the predicted responses
are very close to the true responses, and will be large if for some of the observations, the predicted and true 
responses difer substantially.

The MSE in (equation 2.5) is computed using the training data that was used to fit the model, and so should more 
accurately be referred to as the training MSE. But in general, we do not really care how well the method works on 
the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our 
method to previously unseen test data. Why is this what we care about? Suppose that we are interested in developing 
an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns
from the past 6 months. But we don’t really care how well our method predicts last week’s stock price. We instead 
care about how well it will predict tomorrow’s price or next month’s price. 

To state it more mathematically, suppose that we fit our statistical learning method on our training observations 
{(x₁,y₁), (x₂,y₂),...,(xₙ,yₙ)}, and we obtain the estimate fˆ. We can then compute fˆ(x₁), fˆ(x₂),...,fˆ(xₙ).

If these are approximately equal to y₁,y₂ ,...,yₙ, then the training MSE given by (equation 2.5) is small. However, 
we are really not interested in whether fˆ(xᵢ) ≈ yᵢ; instead, we want to know whether fˆ(x₀) is approximately equal
to y₀, where (x₀,y₀) is a previously unseen test observation not used to train the statistical learning method. We 
want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. In other words,
if we had a large number of test observations, we could compute see equation26.png

the average squared prediction error for these test observations (x₀,y₀). We’d like to select the model for which 
this quantity is as small as possible.

How can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data
set available that is, we may have access to a set of observations that were not used to train the statistical 
learning method. We can then simply evaluate (equation 2.6) on the test observations, and select the learning method 
for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine 
simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a 
sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a 
fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also
have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate 
coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but 
the test MSE is often much larger.

Figure 2.9 illustrates this phenomenon on a simple example. In the left-hand panel of Figure 2.9, we have generated
observations from (equation 2.1) with the true f given by the black curve. The orange, blue and green curves illus-
trate three possible estimates for f obtained using methods with increasing levels of fexibility. The orange line 
is the linear regression fit, which is relatively infexible. The blue and green curves were produced using smoothing
splines with diferent levels of smoothness. It is clear that as the level of fexibility increases, the curves fit the
observed data more closely. The green curve is the most fexible and matches the data very well; however, we observe 
that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of 
the smoothing spline fit, we can produce many diferent fits to this data.

We now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function
of flexibility, or more formally the degrees of freedom, for a number of smoothing splines. 

The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the left-hand panel.
A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve—note that in Figure 2.9,
linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines 
monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not
flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it 
corresponds to the most flexible of the three curves fit in the left-hand panel.

In this example, we know the true function f, and so we can also compute the test MSE over a very large test set, as
a function of fexibility. (Of course, in general f is unknown, so this will not be possible.) The test MSE is 
displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially
declines as the level of flexibility increases. However, at some point the test MSE levels of and then starts to 
increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test
MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of 
Figure 2.9. The horizontal dashed line indicates Var(e), the irreducible error in (equation 2.3), which corresponds 
to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue 
curve is close to optimal.

In the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a
monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical 
learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. 
As model fexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields 
a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our 
statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some
patterns that are just caused by random chance rather than by true properties of the unknown function f. When we 
overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the
training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, 
we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods 
either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which
a less fexible model would have yielded a smaller test MSE.

Figure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training
MSE decreases monotonically as the model fexibility increases, and that there is a U-shape in the test MSE. However,
because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the 
orange least squares fit is substantially better than the highly fexible green curve. Finally, Figure 2.11 displays 
an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, 
but now there is a rapid decrease in both curves before the test MSE starts to increase slowly.

In practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably
more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility
level corresponding to the model with the minimal test MSE can vary considerably among data sets.

