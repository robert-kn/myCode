Y = f(X) + e  (2.1)

What does the above equation represent? the relationship between the response y and the predictor(s). Where
X = (X₁, X₂, X₃...Xₚ) represents different predictors that we assume have a relationship with Y. Y could be 
sales and X could represent advertising budgets of different mediums.

Predictors are also known as independent variables or features.
Response is also known as the dependent variable.

f is some fixed but unknown function of X₁...Xₚ and e is a random error term which is independent of X and 
has mean zero (what does this mean? read description in figure22.png to understand)

It is all about finding the function that connects the predictors such that a response can be
determined. This function is usually estimated. Once it is estimated the vertical lines from the 
observed values to the estimated function represent the error associated with each observation.
If an observation lies above the curve, the error is positive; if it lies below, the error is 
negative. Overall the errors have a mean of zero.

In general a function f may involve more than one input variable see figure23.png Here, income
is plotted as a function of years of education and seniority. f is 2d surface that must be estimated
based on the observed data.

In essence statistical learning refers to a set of approaches for estimating f.

Why estimate f?

For two main reasons: prediction and inference.

Prediction: in many situations a set of inputs X are readily available, but the output Y cannot
be easily obtained. Since the error term averages to zero, we can predict Y using:

ŷ = fˆ(X)   2.2

where f hat represents the estimate of f and y hat represents the resulting prediction for y.
In this setting, f hat is often treated as a black box in the sense that one is not typically
concerned with the exact form of f hat provided that it yields accurate predictions of y.

The accuracy of ŷ as a prediction for y depends on two quantities; the reducible error and
the irreducible error. In general, fˆ will not be a perfect estimate for f and this inaccuracy 
will introduce some error. This error is reducible because we can potentially improve the 
accuracy of fˆ by using the most appropriate statistical learning technique to estimate f.

Even if it were possible to form a perfect estimate of f, so that our estimated response took
the form of ŷ = f(x), our prediction would still have some error in it. Why? because Y is also
a function of e which by definition cannot be predicted using x (i don't understand this last
bit??).

Variability associated with e also affects the accuracy of our predictions. This is known as
the irreducible error, because no matter how well we estimate f, we cannot reduce the error 
introduced by e.

Why is the irreducible error larger than zero? The quantity e may contain unmeasured variables
that are useful in predicting y. Since they are not measured, f cannot use them for its 
prediction. The quantity e may also contain unmeasured variation.

Consider a given estimate fˆ and a set of predictors X, which yields the prediction 
Ŷ = fˆ(X). Assume for a moment that both fˆ and X are fixed,
so that the only variability comes from e. Then, it is easy to show that

E(Y - Ŷ )² = E[f(X) + e - fˆ(X)]² 
           = [f(X) - fˆ(X)]² + Var(e)     2.3
              reducible        irreducible

where the term on the left represents the average, or expected value, of the squared difference
between the predicted and actual value of Y, and var(e) represents the variance associated 
with the error term e.

The focus of this book is on techniques for estimating f with the aim of minimizing the 
reducible error. It is important to keep in mind that the irreducible error will always 
provide an upper bound on the accuracy of our prediction for Y . This bound is almost always 
unknown in practice.

inference: we are often interested in understanding the relationship between Y and X₁...Xₚ. In
this situation we wish to estimate f, but our goal is not to necessarily make predictions for Y.
Now fˆ cannot be treated as a black box because we need to know its exact form.

Depending on whether my ultimate goal is inference or prediction, or a combination of the two,
different methods for estimating f may be appropriate.

Broadly speaking, most statistical learning methods (linear and non linear) can be characterised as either
parametric or non parametric.

Parametric methods: involve a two step model based approach:
1. first make an assumption about the functional form or shape of f. For example, one very simple assumption 
is that f is linear in X

f(X) = β₀ + β₁X₁ + β₂X₂ + ···+ βₚXₚ.     (2.4)

This is a linear model. Once we have assumed that f is linear, the problem of estimating f is greatly simplifed. 

2. After a model has been selected, we need a procedure that uses training data to fit or train the model. In the 
case of the linear model (2.4), we need to estimate the parameters β₀, β₁... βₚ. That is, we want to find values
of these parameters such that 

Y  ≈ β₀ + β₁X₁ + β₂X₂ + ···+ βₚXₚ.

The most common approach to fitting the model (2.4 see figure24.png) is referred to as (ordinary) least squares. 
However, least squares is one of the many possible ways to fit the linear model.

The model based approach described above is referred to as parametric; it reduces the problem of estimating f
down to one of estimating a set of parameters. The potential disadvantage of a parametric approach is that the 
model we choose will usually not match the true unknown form of f.

If the chosen model is too far from the true f, then our estimate will be poor. We can try to address this problem
by choosing flexible models that can fit many different possible functional forms for f. But in general, fitting a
more flexible model requires estimating a greater number of parameters. These more complex models can lead to a 
phenomenon known as overfitting the data which essentially means that they follow the errors or noise too closely.

Figure 2.4 (see figure24.png) shows an example of the parametric approach applied to the income data from figure23.png
we have fit a linear model of the form:

income ≈ β₀ + β₁ × education + β₂ × seniority.

since we have assumed a linear relationship between the response and the two predictors, the entire fitting problem
reduces to estimating  β₀, β₁ and β₂ which we do using least squares linear regression.

Comparing Figure 2.3 to Figure 2.4, we can see that the linear fit given in Figure 2.4 is not quite right: the true 
f has some curvature that is not captured in the linear ft. However, the linear ft still appears to do a reasonable 
job of capturing the positive relationship between years of education and income, as well as the slightly less 
positive relationship between seniority and income.

Non parametric methods: 

Non-parametric methods do not make explicit assumptions about the functional form of f. Instead 
they seek an estimate of f that gets as close to the data points as possible without being too 
rough or wiggly. Such approaches can have a major advantage over parametric approaches: by 
avoiding the assumption of a particular functional form for f, they have the potential
to accurately fit a wider range of possible shapes for f.

Any parametric approach brings with it the possibility that the functional form used to
estimate f is very different from the true f, in which case the resulting model will not 
fit the data well. In contrast, non-parametric approaches completely avoid this danger, since 
essentially no assumption about the form of f is made.

But non-parametric approaches do suffer from a major disadvantage: since they do not reduce 
the problem of estimating f to a small number of parameters, a very large number of 
observations (far more than is typically needed for a parametric approach) is required in 
order to obtain an accurate estimate for f.

An example of a non-parametric approach to fitting the Income data is shown in Figure 2.5. A 
thin-plate spline is used to estimate f . This approach does not impose any pre-specified 
model on f. It instead attempts to produce an estimate for f that is as close as possible to 
the observed data, subject to the fit—that is, the yellow surface in Figure 2.5—being smooth. 
In this case, the non-parametric fit has produced a remarkably accurate estimate of the true f 
shown in Figure 2.3. In order to fit a thin-plate spline, the data analyst must select a level 
of smoothness. 

Figure 2.6 shows the same thin-plate spline fit using a lower level of smoothness, allowing
for a rougher fit. The resulting estimate fits the observed data perfectly! However, the 
spline fit shown in Figure 2.6 is far more variable than the true function f , from Figure 
2.3. This is an example of overfitting the data, which we discussed previously. It is an 
undesirable situation because the fit obtained will not yield accurate estimates of the 
response on new observations that were not part of the original training data set.

The Trade-Off Between Prediction Accuracy and Model Interpretability:

Of the many methods that we examine in this book, some are less flexible, or more restrictive, 
in the sense that they can produce just a relatively small range of shapes to estimate f . For 
example, linear regression is a relatively inflexible approach, because it can only generate 
linear functions such as the lines shown in Figure 2.1 or the plane shown in Figure 2.4. Other 
methods, such as the thin plate splines shown in Figures 2.5 and 2.6, are considerably more 
flexible because they can generate a much wider range of possible shapes to estimate f.

One might reasonably ask the following question: why would we ever choose to use a more 
restrictive method instead of a very flexible approach?

There are several reasons that we might prefer a more restrictive model. If we are mainly 
interested in inference, then restrictive models are much more interpretable. For instance, 
when inference is the goal, the linear model may be a good choice since it will be quite easy 
to understand the relationship between Y and X₁, X₂, X₃...Xₚ. In contrast, very flexible
approaches, such as figure25.png and figure25.png, can lead to such complicated estimates of f 
that it is difficult to understand how any individual predictor is associated with the response.

Figure 2.7 provides an illustration of the trade-off between flexibility and interpretability 
for some of the methods that we cover in this book.

Supervised Versus Unsupervised Learning:

Most statistical learning problems fall into one of two categories: supervised or unsupervised.

