Y = f(X) + e  (equation 2.1)

What does the above equation represent? the relationship between the response y and the predictor(s). Where
X = (X₁, X₂, X₃...Xₚ) represents different predictors that we assume have a relationship with Y. Y could be 
sales and X could represent advertising budgets of different mediums.

Predictors are also known as independent variables or features.
Response is also known as the dependent variable.

f is some fixed but unknown function of X₁...Xₚ and e is a random error term which is independent of X and 
has mean zero (what does this mean? read description in figure22.png to understand)

It is all about finding the function that connects the predictors such that a response can be
determined. This function is usually estimated. Once it is estimated the vertical lines from the 
observed values to the estimated function represent the error associated with each observation.
If an observation lies above the curve, the error is positive; if it lies below, the error is 
negative. Overall the errors have a mean of zero.

In general a function f may involve more than one input variable see figure23.png Here, income
is plotted as a function of years of education and seniority. f is 2d surface that must be estimated
based on the observed data.

In essence statistical learning refers to a set of approaches for estimating f.

Why estimate f?

For two main reasons: prediction and inference.

Prediction: in many situations a set of inputs X are readily available, but the output Y cannot
be easily obtained. Since the error term averages to zero, we can predict Y using:

ŷ = fˆ(X)   2.2

where f hat represents the estimate of f and y hat represents the resulting prediction for y.
In this setting, f hat is often treated as a black box in the sense that one is not typically
concerned with the exact form of f hat provided that it yields accurate predictions of y.

The accuracy of ŷ as a prediction for y depends on two quantities; the reducible error and
the irreducible error. In general, fˆ will not be a perfect estimate for f and this inaccuracy 
will introduce some error. This error is reducible because we can potentially improve the 
accuracy of fˆ by using the most appropriate statistical learning technique to estimate f.

Even if it were possible to form a perfect estimate of f, so that our estimated response took
the form of ŷ = f(x), our prediction would still have some error in it. Why? because Y is also
a function of e which by definition cannot be predicted using x (i don't understand this last
bit??).

Variability associated with e also affects the accuracy of our predictions. This is known as
the irreducible error, because no matter how well we estimate f, we cannot reduce the error 
introduced by e.

Why is the irreducible error larger than zero? The quantity e may contain unmeasured variables
that are useful in predicting y. Since they are not measured, f cannot use them for its 
prediction. The quantity e may also contain unmeasured variation.

Consider a given estimate fˆ and a set of predictors X, which yields the prediction 
Ŷ = fˆ(X). Assume for a moment that both fˆ and X are fixed,
so that the only variability comes from e. Then, it is easy to show that

E(Y - Ŷ )² = E[f(X) + e - fˆ(X)]² 
           = [f(X) - fˆ(X)]² + Var(e)     equation 2.3
              reducible        irreducible

where the term on the left represents the average, or expected value, of the squared difference
between the predicted and actual value of Y, and var(e) represents the variance associated 
with the error term e.

The focus of this book is on techniques for estimating f with the aim of minimizing the 
reducible error. It is important to keep in mind that the irreducible error will always 
provide an upper bound on the accuracy of our prediction for Y. This bound is almost always 
unknown in practice.

inference: we are often interested in understanding the relationship between Y and X₁...Xₚ. In
this situation we wish to estimate f, but our goal is not to necessarily make predictions for Y.
Now fˆ cannot be treated as a black box because we need to know its exact form.

Depending on whether my ultimate goal is inference or prediction, or a combination of the two,
different methods for estimating f may be appropriate.

Broadly speaking, most statistical learning methods (linear and non linear) can be characterised as either
parametric or non parametric.

Parametric methods: involve a two step model based approach:
1. first make an assumption about the functional form or shape of f. For example, one very simple assumption 
is that f is linear in X

f(X) = β₀ + β₁X₁ + β₂X₂ + ···+ βₚXₚ.     (2.4)

This is a linear model. Once we have assumed that f is linear, the problem of estimating f is greatly simplifed. 

2. After a model has been selected, we need a procedure that uses training data to fit or train the model. In the 
case of the linear model (2.4), we need to estimate the parameters β₀, β₁... βₚ. That is, we want to find values
of these parameters such that 

Y  ≈ β₀ + β₁X₁ + β₂X₂ + ···+ βₚXₚ.

The most common approach to fitting the model (see figure24.png) is referred to as (ordinary) least squares. 
However, least squares is one of the many possible ways to fit the linear model.

The model based approach described above is referred to as parametric; it reduces the problem of estimating f
down to one of estimating a set of parameters. The potential disadvantage of a parametric approach is that the 
model we choose will usually not match the true unknown form of f.

If the chosen model is too far from the true f, then our estimate will be poor. We can try to address this problem
by choosing flexible models that can fit many different possible functional forms for f. But in general, fitting a
more flexible model requires estimating a greater number of parameters. These more complex models can lead to a 
phenomenon known as overfitting the data which essentially means that they follow the errors or noise too closely.

Figure 2.4 (see figure24.png) shows an example of the parametric approach applied to the income data from figure23.png
we have fit a linear model of the form:

income ≈ β₀ + β₁ × education + β₂ × seniority.

since we have assumed a linear relationship between the response and the two predictors, the entire fitting problem
reduces to estimating β₀, β₁ and β₂ which we do using least squares linear regression.

Comparing Figure 2.3 to Figure 2.4, we can see that the linear fit given in Figure 2.4 is not quite right: the true 
f has some curvature that is not captured in the linear fit. However, the linear fit still appears to do a reasonable 
job of capturing the positive relationship between years of education and income, as well as the slightly less 
positive relationship between seniority and income.

Non parametric methods: 

Non-parametric methods do not make explicit assumptions about the functional form of f. Instead 
they seek an estimate of f that gets as close to the data points as possible without being too 
rough or wiggly. Such approaches can have a major advantage over parametric approaches: by 
avoiding the assumption of a particular functional form for f, they have the potential
to accurately fit a wider range of possible shapes for f.

Any parametric approach brings with it the possibility that the functional form used to
estimate f is very different from the true f, in which case the resulting model will not 
fit the data well. In contrast, non-parametric approaches completely avoid this danger, since 
essentially no assumption about the form of f is made.

But non-parametric approaches do suffer from a major disadvantage: since they do not reduce 
the problem of estimating f to a small number of parameters, a very large number of 
observations (far more than is typically needed for a parametric approach) is required in 
order to obtain an accurate estimate for f.

An example of a non-parametric approach to fitting the Income data is shown in Figure 2.5. A 
thin-plate spline is used to estimate f. This approach does not impose any pre-specified 
model on f. It instead attempts to produce an estimate for f that is as close as possible to 
the observed data, subject to the fit—that is, the yellow surface in Figure 2.5—being smooth. 
In this case, the non-parametric fit has produced a remarkably accurate estimate of the true f 
shown in Figure 2.3. In order to fit a thin-plate spline, the data analyst must select a level 
of smoothness. 

Figure 2.6 shows the same thin-plate spline fit using a lower level of smoothness, allowing
for a rougher fit. The resulting estimate fits the observed data perfectly! However, the 
spline fit shown in Figure 2.6 is far more variable than the true function f, from Figure 
2.3. This is an example of overfitting the data, which we discussed previously. It is an 
undesirable situation because the fit obtained will not yield accurate estimates of the 
response on new observations that were not part of the original training data set.

The Trade-Off Between Prediction Accuracy and Model Interpretability:

Of the many methods that we examine in this book, some are less flexible, or more restrictive, 
in the sense that they can produce just a relatively small range of shapes to estimate f . For 
example, linear regression is a relatively inflexible approach, because it can only generate 
linear functions such as the lines shown in Figure 2.1 or the plane shown in Figure 2.4. Other 
methods, such as the thin plate splines shown in Figures 2.5 and 2.6, are considerably more 
flexible because they can generate a much wider range of possible shapes to estimate f.

One might reasonably ask the following question: why would we ever choose to use a more 
restrictive method instead of a very flexible approach?

There are several reasons that we might prefer a more restrictive model. If we are mainly 
interested in inference, then restrictive models are much more interpretable. For instance, 
when inference is the goal, the linear model may be a good choice since it will be quite easy 
to understand the relationship between Y and X₁, X₂, X₃...Xₚ. In contrast, very flexible
approaches, such as figure25.png and figure26.png, can lead to such complicated estimates of f 
that it is difficult to understand how any individual predictor is associated with the response.

Figure 2.7 provides an illustration of the trade-off between flexibility and interpretability 
for some of the methods that we cover in this book.

Supervised Versus Unsupervised Learning:

Most statistical learning problems fall into one of two categories: supervised or unsupervised. In supervised 
learning, for each observation of the predictor measurement(s) Xᵢ where i = 1,..., n there is an associated 
response yᵢ.

The wish is to fit a model that relates the response to the predictors, with the aim of accurately predicting the 
response for future observations (prediction) or better understanding the relationship between the response and the 
predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression
as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised 
learning domain. 

By contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation 
i = 1,...,n, we observe a vector of measurements xᵢ but no associated response yi.

It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting,
we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable
that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the 
relationships between the variables or between the observations. One statistical learning tool that we may use
regression in this setting is cluster analysis, or clustering. The goal of cluster analysis  is to ascertain, on 
the basis of X₁, X₂, X₃...Xₙ , whether the observations fall into relatively distinct groups.

semi-supervised learning problem arises when for instance we have a set of n observations. For m of the observations,
where m<n, we have both predictor measurements and a response measurement. For the remaining n − m observations, 
we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be 
measured relatively cheaply but the corresponding responses are much more expensive to collect.

In this setting, we wish to use a statistical learning method that can incorporate the m observations for which 
response measurements are available as well as the n − m observations for which they are not.

Variables can be characterized as either quantitative or qualitative (also known as categorical). Quantitative 
variables take on numerical values.

In contrast, qualitative variables take on values in one of K diferent classes, or categories. Examples of 
qualitative variables include a person’s marital status (married or not), the brand of product purchased 
(brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous 
Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia).

We tend to refer to problems with a quantitative response as regression problems, while those involving a
qualitative response are often referred to as classification problems. However, the distinction is not always that 
crisp. 

Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used
with a qualitative (two-class, or binary) binary response.

We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; 
i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether 
the predictors are qualitative or quantitative is generally considered less important. 

Measuring the Quality of Fit

In order to evaluate the performance of a statistical learning method on a given data set, we need some way to 
measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which 
the predicted response value for a given observation is close to the true response value for that observation. In 
the regression setting, the most commonly used measure is the mean squared error (MSE), given by equation25.png

where fˆ(xᵢ) is the prediction that f gives for the ith observation. The MSE will be small if the predicted responses
are very close to the true responses, and will be large if for some of the observations, the predicted and true 
responses difer substantially.

The MSE in (equation 2.5) is computed using the training data that was used to fit the model, and so should more 
accurately be referred to as the training MSE. But in general, we do not really care how well the method works on 
the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our 
method to previously unseen test data. Why is this what we care about? Suppose that we are interested in developing 
an algorithm to predict a stock’s price based on previous stock returns. We can train the method using stock returns
from the past 6 months. But we don’t really care how well our method predicts last week’s stock price. We instead 
care about how well it will predict tomorrow’s price or next month’s price. 

To state it more mathematically, suppose that we fit our statistical learning method on our training observations 
{(x₁,y₁), (x₂,y₂),...,(xₙ,yₙ)}, and we obtain the estimate fˆ. We can then compute fˆ(x₁), fˆ(x₂),...,fˆ(xₙ).

If these are approximately equal to y₁,y₂ ,...,yₙ, then the training MSE given by (equation 2.5) is small. However, 
we are really not interested in whether fˆ(xᵢ) ≈ yᵢ; instead, we want to know whether fˆ(x₀) is approximately equal
to y₀, where (x₀,y₀) is a previously unseen test observation not used to train the statistical learning method. We 
want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. In other words,
if we had a large number of test observations, we could compute see equation26.png

the average squared prediction error for these test observations (x₀,y₀). We’d like to select the model for which 
this quantity is as small as possible.

How can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data
set available that is, we may have access to a set of observations that were not used to train the statistical 
learning method. We can then simply evaluate (equation 2.6) on the test observations, and select the learning method 
for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine 
simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a 
sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a 
fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also
have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate 
coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but 
the test MSE is often much larger.

Figure 2.9 illustrates this phenomenon on a simple example. In the left-hand panel of Figure 2.9, we have generated
observations from (equation 2.1) with the true f given by the black curve. The orange, blue and green curves illus-
trate three possible estimates for f obtained using methods with increasing levels of fexibility. The orange line 
is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing
splines with diferent levels of smoothness. It is clear that as the level of flexibility increases, the curves fit the
observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe 
that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of 
the smoothing spline fit, we can produce many diferent fits to this data.

We now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function
of flexibility, or more formally the degrees of freedom, for a number of smoothing splines. 

The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the left-hand panel.
A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve—note that in Figure 2.9,
linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines 
monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not
flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it 
corresponds to the most flexible of the three curves fit in the left-hand panel.

In this example, we know the true function f, and so we can also compute the test MSE over a very large test set, as
a function of flexibility (Of course, in general f is unknown, so this will not be possible). The test MSE is 
displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially
declines as the level of flexibility increases. However, at some point the test MSE levels of and then starts to 
increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test
MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of 
Figure 2.9. The horizontal dashed line indicates Var(e), the irreducible error in (equation 2.3), which corresponds 
to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue 
curve is close to optimal.

In the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a
monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical 
learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. 
As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields 
a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our 
statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some
patterns that are just caused by random chance rather than by true properties of the unknown function f. When we 
overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the
training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, 
we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods 
either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which
a less flexible model would have yielded a smaller test MSE.

Figure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training
MSE decreases monotonically as the model fexibility increases, and that there is a U-shape in the test MSE. However,
because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the 
orange least squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays 
an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, 
but now there is a rapid decrease in both curves before the test MSE starts to increase slowly.

In practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably
more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility
level corresponding to the model with the minimal test MSE can vary considerably among data sets.

The Bias-Variance Trade-Of

The U-shape observed in the test MSE curves (Figures 2.9–2.11) turns out to be the result of two competing properties
of statistical learning methods.

Though the mathematical proof is beyond the scope of this book, it is possible to show that the expected test MSE, 
for a given value x₀, can always be decomposed into the sum of three fundamental quantities: the variance of fˆ(x₀),
the squared bias of fˆ(x₀) and the variance of the error e. That is, see equation27.png

where the term on the left side in equation27 defines the expected test MSE at x₀, and refers to the average test MSE
that we would obtain if we repeatedly estimated f using a large number of training sets, and tested each at x₀. The
overall expected test MSE can be computed by averaging the quantity on the left side of equation27 over all possible
values of x₀ in the test set.


Equation 2.7 tells us that in order to minimize the expected test error, we need to select a statistical learning 
method that simultaneously achieves low variance and low bias. Note that variance is inherently a nonnegative 
quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below Var(e),
the irreducible error from (2.3).

What do we mean by the variance and bias of a statistical learning method? Variance refers to the amount by which fˆ 
would change if we estimated it using a different training data set. Since the training data are used to fit the 
statistical learning method, diferent training data sets will result in a different f. But ideally the estimate for 
f should not vary too much between training sets. However, if a method has high variance then small changes in the 
training data can result in large changes in fˆ. In general, more flexible statistical methods have higher variance. 
Consider the green and orange curves in Figure 2.9. The flexible green curve is following the observations very 
closely. It has high variance because changing any one of these data points may cause the estimate f to change 
considerably.

In contrast, the orange least squares line is relatively inflexible and has low variance, because moving any single 
observation will likely cause only a small shift in the position of the line.

On the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be 
extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear 
relationship between Y and X₁,X₂,...,Xₚ. It is unlikely that any real-life problem truly has such a simple linear 
relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f.

In Figure 2.11, the true f is substantially non-linear, so no matter how many training observations we are given, it
will not be possible to produce an accurate estimate using linear regression. In other words, linear regression 
results in high bias in this example. However, in Figure 2.10 the true f is very close to linear, and so given 
enough data, it should be possible for linear regression to produce an accurate estimate. Generally, more flexible 
methods result in less bias.

As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The 
relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we 
increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance 
increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little 
impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. 
Note that we observed this pattern of decreasing test MSE followed by increasing test MSE in the right-hand panels 
of Figures 2.9–2.11.

The three plots in Figure 2.12 illustrate Equation 2.7 for the examples in Figures 2.9–2.11. In each case the blue 
solid curve represents the squared bias, for diferent levels of flexibility, while the orange curve corresponds to 
the variance. The horizontal dashed line represents Var(e), the irreducible error. Finally, the red curve, 
corresponding to the test set MSE, is the sum of these three quantities. In all three cases, the variance increases 
and the bias decreases as the method’s flexibility increases. However, the flexibility level corresponding to the 
optimal test MSE differs considerably among the three data sets, because the squared bias and variance change at 
different rates in each of the data sets.

In the left-hand panel of Figure 2.12, the bias initially decreases rapidly, resulting in an initial sharp decrease 
in the expected test MSE. On the other hand, in the center panel of Figure 2.12 the true f is close to linear, so 
there is only a small decrease in bias as flexibility increases, and the test MSE only declines slightly before 
increasing rapidly as the variance increases. Finally, in the right-hand panel of Figure 2.12, as flexibility 
increases, there is a dramatic decline in bias because the true f is very non-linear. There is also very little 
increase in variance as flexibility increases. Consequently, the test MSE declines substantially before experiencing 
a small increase as model flexibility increases.

The relationship between bias, variance, and test set MSE given in Equation 2.7 and displayed in Figure 2.12 is 
referred to as the bias-variance trade-off. Good test set performance of a statistical learning method requires low 
variance as well as low squared bias. This is referred to as a trade-off because it is easy to obtain a method with 
extremely low bias but high variance (for instance, by drawing a curve that passes through every single training 
observation) or a method with very low variance but high bias (by fitting a horizontal line to the data). The 
challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of 
the most important recurring themes in this book.

In a real-life situation in which f is unobserved, it is generally not possible to explicitly compute the test MSE, 
bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off
in mind. 

The Classification Setting:

Thus far, our discussion of model accuracy has been focused on the regression setting. But many of the concepts 
that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only 
some modifications due to the fact that yᵢ is no longer quantitative. Suppose that we seek to estimate f on the 
basis of training observations {(x₁,y₁),...,(xₙ,yₙ)}, where now y₁,...,yₙ are qualitative. The most common approach 
for quantifying the accuracy of our estimate f^ is the training error rate, the proportion of mistakes that are made 
if we apply our estimate f^ to the training observations: see equation28.png 

Here yˆᵢ is the predicted class label for the ith observation using f^. And I(yᵢ != yᵢˆ) is an indicator variable 
that equals 1 if yᵢ != yᵢˆ and zero if yᵢ = yᵢˆ. If I(yᵢ != yᵢˆ) == 0 then the ith observation was classifed correctly
by our classifcation method; otherwise it was misclassifed. Hence Equation 2.8 computes the fraction of incorrect 
classifications.

Equation 2.8 is referred to as the training error rate because it is computed based on the data that was used to 
train our classifer. As in the regression setting, we are most interested in the error rates that result from 
applying our classifer to test observations that were not used in training. The test error rate associated with a 
set of test observations of the form (x₀,y₀) is given by see equation29 where y₀ˆ is the predicted class label that 
results from applying the classifer to the test observation with predictor x₀. A good classifier is one for which the
test error (2.9) is smallest.

The Bayes Classifer

It is possible to show that the test error rate given in (2.9) is minimized, on average, by a very simple classifier 
that assigns each observation to the most likely class, given its predictor values. In other words, we should simply
assign a test observation with predictor vector x₀ to the class j for which see equation210.png is largest. Note that
(2.10) is a conditional probability: it is the probability that Y = j, given the observed predictor vector x₀. This
very simple classifier is called the Bayes classifer. In a two-class problem where there are only two possible 
response values, say class 1 or class 2, the Bayes classifer corresponds to predicting class one if Pr(Y = 1|X = x₀) 
> 0.5, and class two otherwise.

Figure 2.13 provides an example using a simulated data set in a two-dimensional space consisting of predictors X1 and
X2. The orange and blue circles correspond to training observations that belong to two diferent classes. For each 
value of X1 and X2, there is a diferent probability of the response being orange or blue. Since this is simulated 
data, we know how the data were generated and we can calculate the conditional probabilities for each value of X1 and
X2. The orange shaded region reflects the set of points for which Pr(Y = orange|X) is greater than 50%, while the 
blue shaded region indicates the set of points for which the probability is below 50%. The purple dashed line 
represents the points where the probability is exactly 50%. This is called the Bayes  decision  boundary. The Bayes
classifer’s prediction is determined by the Bayes decision boundary; an observation that falls on the orange side of
the boundary will be assigned to the orange class, and similarly an observation on the blue side of the boundary will
be assigned to the blue class.

The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. Since the Bayes 
classifier will always choose the class for which (2.10) is largest, the error rate will be 1 − maxⱼ Pr(Y = j|X = x₀)
at X = x₀. In general, the overall Bayes error rate is given by see equation211.png where the expectation averages 
the probability over all possible values of X. For our simulated data, the Bayes error rate is 0.133. It is greater 
than zero, because the classes overlap in the true population, which implies that maxⱼ Pr(Y = j|X = x₀) < 1 for some
values of x₀. The Bayes error rate is analogous to the irreducible error, discussed earlier.

K -Nearest Neighbors

In theory we would always like to predict qualitative responses using the Bayes classifer. But for real data, we do 
not know the conditional distribution of Y given X , and so computing the Bayes classifer is impossible. Therefore,
the Bayes classifier serves as an unattainable gold standard against which to compare other methods. Many approaches 
attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class 
with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifer. Given a positive 
integer K and a test observation x₀, the KNN classifer first identifes the K points in the training data that are 
closest to x₀, represented by N₀. It then estimates the conditional probability for class j as the fraction of points
in N₀ whose response values equal j see equation212

Finally, KNN classifies the test observation x₀ to the class with the largest probability from (2.12).

Figure 2.14 provides an illustrative example of the KNN approach. In the left-hand panel, we have plotted a small 
training data set consisting of six blue and six orange observations. Our goal is to make a prediction for the point
labeled by the black cross. Suppose that we choose K=3. Then KNN will first identify the three observations that are
closest to the cross. This neighborhood is shown as a circle. It consists of two blue points and one orange point, 
resulting in estimated probabilities of 2/3 for the blue class and 1/3 for the orange class. Hence KNN will predict 
that the black cross belongs to the blue class. In the right-hand panel of Figure 2.14 we have applied the KNN 
approach with K =3 at all of the possible values for X₁ and X₂, and have drawn in the corresponding KNN decision 
boundary.

Despite the fact that it is a very simple approach, KNN can often produce classifers that are surprisingly close 
to the optimal Bayes classifer. Figure 2.15 displays the KNN decision boundary, using K = 10, when applied to the 
larger simulated data set from Figure 2.13. Notice that even though the true distribution is not known by the KNN 
classifer, the KNN decision boundary is very close to that of the Bayes classifer. The test error rate using KNN is 
0.1363, which is close to the Bayes error rate of 0.1304.

The choice of K has a drastic efect on the KNN classifer obtained. Figure 2.16 displays two KNN fits to the simulated
data from Figure 2.13, using K = 1 and K = 100. When K = 1, the decision boundary is overly flexible and finds patterns
in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifer that has low bias 
but very high variance. As K grows, the method becomes less flexible and produces a decision boundary that is close 
to linear. This corresponds to a low-variance but high-bias classifer. On this simulated data set, neither K = 1 nor
K = 100 give good predictions: they have test error rates of 0.1695 and 0.1925, respectively.

Just as in the regression setting, there is not a strong relationship between the training error rate and the test 
error rate. With K = 1, the KNN training error rate is 0, but the test error rate may be quite high. In general, as 
we use more flexible classification methods, the training error rate will decline but the test error rate may not. In 
Figure 2.17, we have plotted the KNN test and training errors as a function of 1/K. As 1/K increases, the method 
becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility 
increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at 
approximately K = 10) before increasing again when the method becomes excessively flexible and overfits.

In both the regression and classification settings, choosing the correct level of flexibility is critical to the 
success of any statistical learning method. The bias-variance tradeoff, and the resulting U-shape in the test error, 
can make this a difficult task.

In order to run the code in the book visit https://www.statlearning.com/resources-python and follow instructions.
Essentially, you will need to do the following:
- install conda which is a command line tool for package and environment management that runs on linux, windows and
macos 

- once conda is installed, create a conda environment to isolate code from other dependencies i.e. conda create
--name islp python=3.11

- conda activate islp

- Once in the env, install the ISLP package using the command pip install ISLP

- The ISLP package will include datasets and custom built functions provide to you by the teaching staff. It also
downloads all other packages needed by the labs you will tackle.

- to run a lab run the command jupyter lab <*.ipynb>

- the islp labs use torch and various related packages for the lab on deep learning. You can find the name of all the
packages installed in the requirements.txt file in this repo https://github.com/intro-stat-learning/ISLP_labs/blob/v2.2/requirements.txt

- for more information on the datasets used in the book, where they were sourced from etc see https://intro-stat-learning.github.io/ISLP/data.html

